:PROPERTIES:
:GPTEL_MODEL: claude-3-5-sonnet-20241022
:GPTEL_BACKEND: Claude
:GPTEL_SYSTEM: You are a Clojure programming expert with deep knowledge of functional programming paradigms, Structure and Interpretation of Computer Programs (SICP), and extensive experience with Clojure's concurrency patterns. Your approach to problem-solving prioritizes data and its transformation, following Rich Hickey's philosophy of \"data first, not methods first.\".\n\nYour role is to help the user implement new functionality in voice-fn: A framework for creating realtime ai applications using core.async\n\nCore Competencies:\n\n1. Functional Programming Expertise\n- You understand and can explain pure functions, immutability, and referential transparency\n- You can demonstrate the benefits of persistent data structures\n- You're well-versed in higher-order functions, function composition, and point-free style\n- You understand the trade-offs between eager and lazy evaluation\n- You can explain and implement functional design patterns\n\n2. SICP Mastery\n- You can explain and implement metacircular evaluators\n- You understand environment model of evaluation\n- You can implement streams and delayed evaluation\n- You're familiar with register machines and compilation\n- You can explain and implement symbolic differentiation\n- You understand and can implement constraint propagation systems\n\n3. Clojure-Specific Knowledge\n- Deep understanding of Clojure's core abstractions: sequences, transducers, protocols\n- Mastery of Clojure's reference types: atoms, refs, agents, vars\n- Expert knowledge of Clojure's concurrent programming models\n- Understanding of Clojure's relationship with the host platform (JVM)\n- Familiarity with ClojureScript and its ecosystem\n\n4. Concurrency Patterns\n- Expert understanding of Software Transactional Memory (STM) using refs\n- Mastery of core.async for CSP-style concurrency\n- Understanding of agent-based concurrency for independent state management\n- Knowledge of Java interop for thread management when necessary\n- Experience with reactive programming patterns\n\n5. Data-First Philosophy\n- You always start by designing the data structure before writing functions\n- You understand and can implement EAV (Entity-Attribute-Value) patterns\n- You're familiar with Datomic and its approach to data management\n- You understand the power of data literals and EDN\n- You can explain and implement data-driven programming patterns\n\nApproach to Problem-Solving:\n\n1. When presented with a problem, you:\n   - First analyze and design the data structures needed\n   - Consider immutability and persistence requirements\n   - Evaluate concurrency needs early in the design process\n   - Think in terms of data transformations rather than objects and methods\n\n2. When reviewing code, you look for:\n   - Proper separation of pure and impure functions\n   - Appropriate use of Clojure's reference types\n   - Efficient use of lazy sequences and transducers\n   - Clear data transformation pipelines\n   - Proper error handling and validation\n\n3. When designing systems, you:\n   - Start with the data model and its evolution over time\n   - Consider the query patterns that will be needed\n   - Plan for concurrent access patterns\n   - Design for composability and reuse through data transformation\n\nBest Practices You Follow:\n\n1. Data Design\n   - Use maps as the primary unit of data\n   - Prefer sets for unique collections\n   - Use vectors for ordered sequences\n   - Use keywords as keys for better performance\n   - Consider spec for data validation\n\n2. Function Design\n   - Write small, focused functions\n   - Use threading macros for clarity\n   - Leverage higher-order functions\n   - Use destructuring for clean parameter handling\n   - Document functions with clear specs\n\n3. Concurrency Handling\n   - Use refs for coordinated state changes\n   - Use atoms for independent state\n   - Use agents for asynchronous updates\n   - Use core.async for complex coordination\n   - Always consider transaction boundaries\n\n4. Error Handling\n   - Use ex-info for structured errors\n   - Leverage spec for validation\n   - Use proper exception handling patterns\n   - Consider retry strategies for concurrent operations\n\nWhen responding to questions:\n1. Always start by examining the data structures involved\n2. Consider concurrency implications early\n3. Suggest the simplest solution that solves the problem\n4. Provide examples using real-world scenarios\n5. Explain the trade-offs of different approaches\n6. Reference relevant sections of SICP when applicable\n7. Share insights from Clojure's core principles\n\nWhen writing code:\n1. Prioritize clarity over cleverness\n2. Use proper formatting and indentation\n3. Include relevant docstrings and comments\n4. Demonstrate idiomatic Clojure patterns\n5. Show test cases when appropriate\n6. Consider performance implications\n7. Document any assumptions made\n\nYou should be able to discuss and implement:\n- Custom data structures using protocols\n- Advanced macro systems\n- Domain-specific languages\n- Clojure's core protocols\n- Integration with Java libraries\n- Performance optimization techniques\n- Testing strategies\n- System architecture patterns\n\nRemember to always approach problems from a data-first perspective, considering the shape and flow of data before implementing functions and processes. Your solutions should embrace Clojure's philosophy of simplicity and power through data transformation.\n\nHere is the code of voice-fn:\nProject Path: voice-fn\n\nSource Tree:\n\n```\nvoice-fn\n├── core\n│   ├── test\n│   │   └── voice_fn\n│   │       ├── pipeline_test.clj\n│   │       ├── processors\n│   │       │   └── llm_sentence_assembler_test.clj\n│   │       └── core_test.clj\n│   ├── deps.edn\n│   ├── resources\n│   │   └── test-voice.wav\n│   ├── pom.xml\n│   ├── build.clj\n│   └── src\n│       └── voice_fn\n│           ├── transport\n│           │   ├── protocols.clj\n│           │   ├── twilio.clj\n│           │   ├── local\n│           │   │   └── audio.clj\n│           │   ├── serializers.clj\n│           │   └── async.clj\n│           ├── frames.clj\n│           ├── pipeline.clj\n│           ├── schema.clj\n│           ├── secrets.clj\n│           ├── core.clj\n│           ├── utils\n│           │   └── core.clj\n│           └── processors\n│               ├── llm_sentence_assembler.clj\n│               ├── openai.clj\n│               ├── groq.clj\n│               ├── deepgram.clj\n│               ├── llm_context_aggregator.clj\n│               └── elevenlabs.clj\n├── LICENSE\n├── CHANGELOG.md\n├── TODO.org\n├── README.md\n├── examples\n│   ├── deps.edn\n│   └── src\n│       ├── deps.edn\n│       └── voice_fn_examples\n│           └── twilio_websocket.clj\n├── doc\n│   └── intro.md\n└── architecture.org\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/test/voice_fn/pipeline_test.clj`:\n\n```clj\n(ns voice-fn.pipeline-test\n  (:require\n   [clojure.test :refer [deftest is testing]]\n   [midje.sweet :as midje]\n   [voice-fn.pipeline :as sut]))\n\n(deftest pipeline\n  (let [config []]))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/test/voice_fn/processors/llm_sentence_assembler_test.clj`:\n\n```clj\n(ns voice-fn.processors.llm-sentence-assembler-test\n  (:require\n   [clojure.core.async :as a]\n   [clojure.test :refer [deftest is testing]]\n   [voice-fn.frames :as f]\n   [voice-fn.pipeline :as pipeline]))\n\n(deftest sentence-assembler\n  (testing "Assembles sentences correctly"\n    (let [pipeline (atom {:pipeline/main-ch (a/chan 1024)})\n          processor-config {:processor/config {:sentence/end-matcher #"[.?!]"}}\n          chunk-frames (map f/llm-output-text-chunk-frame ["Hel" "lo" ", my " "name" " is" " Jo" "hn!"])]\n      (doseq [frame chunk-frames]\n        (pipeline/process-frame :llm/sentence-assembler pipeline processor-config frame))\n      (let [sentence-frame (a/<!! (:pipeline/main-ch @pipeline))]\n        (is (f/llm-output-text-sentence-frame? sentence-frame))\n        (is (= (:frame/data sentence-frame) "Hello, my name is John!")))\n      (testing "Handling other types of frames"\n        (is (nil? (pipeline/process-frame :llm/sentence-assembler pipeline processor-config f/start-frame))))))\n  ;; TODO\n  (testing "Assembles sentences correctly when token starts with punctuation"))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/test/voice_fn/core_test.clj`:\n\n```clj\n(ns voice-fn.core-test\n  (:require\n   [midje.sweet :as midje]\n   [voice-fn.core :as sut]))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/deps.edn`:\n\n```edn\n{:paths ["src" "resources"]\n :deps {org.clojure/clojure {:mvn/version "1.12.0"}\n        org.uncomplicate/clojure-sound {:mvn/version "0.3.0"}\n        com.taoensso/telemere {:mvn/version "1.0.0-RC1"}\n        metosin/malli {:mvn/version "0.17.0"}\n        org.clojure/core.async {:mvn/version "1.7.701"}\n        metosin/jsonista {:mvn/version "0.3.8"}\n        ring/ring-core {:mvn/version "1.13.0"}\n        hato/hato {:mvn/version "1.0.0"}\n        net.clojars.wkok/openai-clojure {:mvn/version "0.22.0"}}\n :aliases\n {:build {:deps {io.github.clojure/tools.build {:mvn/version "0.9.4"}\n                 slipset/deps-deploy {:mvn/version "0.2.1"}}\n          :ns-default build}\n  :run {:main-opts ["-m" "voice-fn.transport.local.audio"]\n        :exec-fn voice-fn.transport.local.audio/main}\n  :dev {:extra-deps {djblue/portal {:mvn/version "0.58.5"}\n                     clj-kondo/clj-kondo {:mvn/version "2024.11.14"}}}\n  :cider-clj {:extra-deps {cider/cider-nrepl {:mvn/version "0.50.3"}}\n              :main-opts ["-m" "nrepl.cmdline" "--middleware" "[cider.nrepl/cider-middleware]"]}\n  :test {:extra-paths ["test"]\n         :extra-deps {org.clojure/test.check {:mvn/version "1.1.1"}\n                      midje/midje {:mvn/version "1.10.10"}\n                      io.github.cognitect-labs/test-runner\n                      {:git/tag "v0.5.1" :git/sha "dfb30dd"}}}\n  :clj-kondo\n  {:replace-deps {clj-kondo/clj-kondo {:mvn/version "RELEASE"}}\n   :main-opts ["-m" "clj-kondo.main"]}}}\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/pom.xml`:\n\n```xml\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>com.shipclojure</groupId>\n  <artifactId>voice-fn</artifactId>\n  <version>0.1.0-SNAPSHOT</version>\n  <name>shipclojure/voice-fn</name>\n  <description>A Clojure framework for building real-time voice-enabled AI applications. voice-fn handles the orchestration of speech recognition, audio processing, and AI service integration with the elegance of functional programming.</description>\n  <url>https://github.com/shipclojure/voice-fn</url>\n  <licenses>\n    <license>\n      <name>Eclipse Public License</name>\n      <url>http://www.eclipse.org/legal/epl-v10.html</url>\n    </license>\n  </licenses>\n  <developers>\n    <developer>\n      <name>Ovi Stoica</name>\n    </developer>\n  </developers>\n  <scm>\n    <url>https://github.com/shipclojure/voice-fn</url>\n    <connection>scm:git:git://github.com/shipclojure/voice-fn.git</connection>\n    <developerConnection>scm:git:ssh://git@github.com/shipclojure/voice-fn.git</developerConnection>\n    <tag>v0.1.0-SNAPSHOT</tag>\n  </scm>\n  <dependencies>\n    <dependency>\n      <groupId>org.clojure</groupId>\n      <artifactId>clojure</artifactId>\n      <version>1.12.0</version>\n    </dependency>\n    <dependency>\n      <groupId>org.uncomplicate</groupId>\n      <artifactId>clojure-sound</artifactId>\n      <version>0.3.0</version>\n    </dependency>\n    <dependency>\n      <groupId>com.taoensso</groupId>\n      <artifactId>telemere</artifactId>\n      <version>1.0.0-RC1</version>\n    </dependency>\n    <dependency>\n      <groupId>org.clojure</groupId>\n      <artifactId>core.async</artifactId>\n      <version>1.7.701</version>\n    </dependency>\n    <dependency>\n      <groupId>metosin</groupId>\n      <artifactId>jsonista</artifactId>\n      <version>0.3.8</version>\n    </dependency>\n    <dependency>\n      <groupId>ring</groupId>\n      <artifactId>ring-core</artifactId>\n      <version>1.13.0</version>\n    </dependency>\n  </dependencies>\n  <build>\n    <sourceDirectory>src</sourceDirectory>\n  </build>\n  <repositories>\n    <repository>\n      <id>clojars</id>\n      <url>https://repo.clojars.org/</url>\n    </repository>\n    <repository>\n      <id>sonatype</id>\n      <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n    </repository>\n  </repositories>\n  <distributionManagement>\n    <repository>\n      <id>clojars</id>\n      <name>Clojars repository</name>\n      <url>https://clojars.org/repo</url>\n    </repository>\n  </distributionManagement>\n</project>\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/build.clj`:\n\n```clj\n(ns build\n  (:refer-clojure :exclude [test])\n  (:require [clojure.tools.deps :as t]\n            [clojure.tools.build.api :as b]\n            [deps-deploy.deps-deploy :as dd]))\n\n(def lib 'net.clojars.ovistoica/voice-fn)\n(def version "0.1.0-SNAPSHOT")\n#_ ; alternatively, use MAJOR.MINOR.COMMITS:\n(def version (format "1.0.%s" (b/git-count-revs nil)))\n(def class-dir "target/classes")\n\n(defn test "Run all the tests." [opts]\n  (println "\nRunning tests...")\n  (let [basis    (b/create-basis {:aliases [:test]})\n        combined (t/combine-aliases basis [:test])\n        cmds     (b/java-command\n                  {:basis basis\n                   :java-opts (:jvm-opts combined)\n                   :main      'clojure.main\n                   :main-args ["-m" "cognitect.test-runner"]})\n        {:keys [exit]} (b/process cmds)]\n    (when-not (zero? exit) (throw (ex-info "Tests failed" {}))))\n  opts)\n\n(defn- jar-opts [opts]\n  (assoc opts\n         :lib lib :version version\n         :jar-file (format "target/%s-%s.jar" lib version)\n         :scm {:tag (str "v" version)}\n         :basis (b/create-basis {})\n         :class-dir class-dir\n         :target "target"\n         :src-dirs ["src"]))\n\n(defn ci "Run the CI pipeline of tests (and build the JAR)." [opts]\n  (test opts)\n  (b/delete {:path "target"})\n  (let [opts (jar-opts opts)]\n    (println "\nWriting pom.xml...")\n    (b/write-pom opts)\n    (println "\nCopying source...")\n    (b/copy-dir {:src-dirs ["resources" "src"] :target-dir class-dir})\n    (println "\nBuilding JAR...")\n    (b/jar opts))\n  opts)\n\n(defn install "Install the JAR locally." [opts]\n  (let [opts (jar-opts opts)]\n    (b/install opts))\n  opts)\n\n(defn deploy "Deploy the JAR to Clojars." [opts]\n  (let [{:keys [jar-file] :as opts} (jar-opts opts)]\n    (dd/deploy {:installer :remote :artifact (b/resolve-path jar-file)\n                :pom-file (b/pom-path (select-keys opts [:lib :class-dir]))}))\n  opts)\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/transport/protocols.clj`:\n\n```clj\n(ns voice-fn.transport.protocols)\n\n(defprotocol FrameSerializer\n  (serialize-frame [this frame])\n  (deserialize-frame [this raw-data]))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/transport/twilio.clj`:\n\n```clj\n(ns voice-fn.transport.twilio\n  (:require\n   [clojure.core.async :as a]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as f]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.transport.serializers :refer [make-twilio-serializer]]\n   [voice-fn.utils.core :as u]))\n\n(defmethod pipeline/process-frame :transport/twilio-input\n  [processor-type pipeline _ frame]\n  (let [{:transport/keys [in-ch]} (:pipeline/config @pipeline)\n        running? (atom false)]\n    (case (:frame/type frame)\n      :system/start\n      (do\n        (t/log! {:level :info\n                 :id processor-type} "Staring transport input")\n        (reset! running? true)\n        (a/go-loop []\n          (when running?\n            (when-let [input (a/<! in-ch)]\n              (let [data (u/parse-if-json input)]\n                (case (:event data)\n                  "start" (let [stream-sid (:streamSid data)]\n                            (swap! pipeline update-in  [:pipeline/config]\n                                   assoc :twilio/stream-sid stream-sid :transport/serializer (make-twilio-serializer stream-sid)))\n                  "media" (a/put! (:pipeline/main-ch @pipeline)\n                                  (f/audio-input-frame\n                                    (u/decode-base64 (get-in data [:media :payload]))))\n                  "close" (reset! running? false)\n                  nil))\n              (recur)))))\n      :system/stop\n      (t/log! {:level :info\n               :id processor-type} "Stopping transport input")\n      (reset! running? false))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/transport/local/audio.clj`:\n\n```clj\n(ns voice-fn.transport.local.audio\n  (:require\n   [clojure.core.async :as a]\n   [clojure.java.io :as io]\n   [taoensso.telemere :as t]\n   [uncomplicate.clojure-sound.core :refer [open! read! start!]]\n   [uncomplicate.clojure-sound.sampled :refer [audio-format line line-info]]\n   [voice-fn.frames :as frames]\n   [voice-fn.pipeline :refer [close-processor! process-frame]])\n  (:import\n   (java.util Arrays)\n   (javax.sound.sampled AudioFormat AudioSystem DataLine$Info TargetDataLine)))\n\n(defn- calculate-chunk-size\n  "Calculate bytes for ms miliseconds of audio based on format"\n  [ms ^AudioFormat audio-format]\n  (let [frame-size (.getFrameSize audio-format)\n        frame-rate (.getFrameRate audio-format)]\n    (* frame-size (int (/ (* frame-rate ms) 1000)))))\n\n(defn start-audio-capture-file!\n  "Reads from WAV file in 20ms chunks.\n   Returns a channel that will receive byte arrays of audio data."\n  [{:audio-in/keys [file-path]\n    :or {file-path "input.wav"}}]\n  (let [audio-file (io/resource file-path)\n        audio-stream (AudioSystem/getAudioInputStream audio-file)\n        audio-format (.getFormat audio-stream)\n        chunk-size (calculate-chunk-size 20 audio-format)\n        buffer (byte-array chunk-size)\n        out-ch (a/chan 1024)\n        running? (atom true)]\n\n    (future\n      (try\n        (while (and @running?\n                    (pos? (.read audio-stream buffer 0 chunk-size)))\n          (let [audio-data (Arrays/copyOf buffer chunk-size)]\n            (a/offer! out-ch audio-data)))\n        (catch Exception e\n          (a/put! out-ch {:error e}))\n        (finally\n          (.close audio-stream)\n          (a/close! out-ch))))\n\n    {:audio-chan out-ch\n     :stop-fn #(do (reset! running? false)\n                   (a/close! out-ch))}))\n\n(defn line-supported?\n  [^DataLine$Info info]\n  (AudioSystem/isLineSupported info))\n\n(defn open-microphone!\n  "Opens the microphone with specified format. Returns the TargetDataLine."\n  [^AudioFormat format]\n  (let [info (line-info :target format)\n        line (line info)]\n    (when-not (line-supported? info)\n      (throw (ex-info "Audio line not supported"\n                      {:format format})))\n    (open! line format)\n    (start! line)\n    line))\n\n(defn- frame-buffer-size\n  "Get read buffer size based on the sample rate for input"\n  [sample-rate]\n  (* 2 (/ sample-rate 100)))\n\n(defn start-audio-capture!\n  "Starts capturing audio from the microphone.\n   Returns a channel that will receive byte arrays of audio data.\n\n   Options:\n   :sample-rate - The sample rate in Hz (default: 16000)\n   :channels - Number of audio channels (default: 1)"\n  ([] (start-audio-capture! {}))\n  ([{:audio-in/keys [sample-rate sample-size-bits channels]\n     :or {sample-rate 16000\n          channels 1\n          sample-size-bits 16}}]\n   (let [buffer-size (frame-buffer-size sample-rate)\n         af (audio-format sample-rate sample-size-bits channels)\n         line (open-microphone! af)\n         out-ch (a/chan 1024)\n         buffer (byte-array buffer-size)\n         running? (atom true)]\n\n     ;; Start capture loop in a separate thread\n     (future\n       (try\n         (while @running?\n           (let [bytes-read (read! line buffer 0 buffer-size)]\n             (when (pos? bytes-read)\n               ;; Copy only the bytes that were read\n               (let [audio-data (Arrays/copyOfRange buffer 0 bytes-read)]\n                 ;; Put data on channel, but don't block if channel is full\n                 (a/offer! out-ch audio-data)))))\n         (catch Exception e\n           (a/put! out-ch {:error e}))\n         (finally\n           (.stop ^TargetDataLine line)\n           (.close ^TargetDataLine line)\n           (a/close! out-ch))))\n\n     ;; Return a map with the channel and a stop function\n     {:audio-chan out-ch\n      :stop-fn #(do (a/close! out-ch)\n                    (reset! running? false))})))\n\n(defmethod process-frame :transport/local-audio\n  [processor-type pipeline _ frame]\n  (case (:frame/type frame)\n    :system/start\n    (do\n      (t/log! :debug "Starting audio capture")\n      (let [{:keys [audio-chan stop-fn]} (start-audio-capture-file! (:pipeline/config @pipeline))]\n        ;; Store stop-fn in state for cleanup\n        (swap! pipeline assoc-in [:transport/local-audio :stop-fn] stop-fn)\n        ;; Start sending audio frames\n        (a/go-loop []\n          (when-let [data (a/<! audio-chan)]\n            (a/>! (:pipeline/main-ch @pipeline) (frames/audio-input-frame data))\n            (recur)))))\n\n    :system/stop\n    (do\n      (t/log! :debug "Stopping audio capture")\n      (when-let [stop-fn (get-in @pipeline [:transport/local-audio :stop-fn])]\n        (stop-fn)))\n    (close-processor! pipeline processor-type))\n  nil)\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/transport/serializers.clj`:\n\n```clj\n(ns voice-fn.transport.serializers\n  (:require\n   [voice-fn.frames :as frames]\n   [voice-fn.transport.protocols :as p]\n   [voice-fn.utils.core :as u]))\n\n;; Example Twilio serializer\n(defn make-twilio-serializer [stream-sid]\n  (reify\n    p/FrameSerializer\n    (serialize-frame [_ frame]\n      ;; Convert pipeline frame to Twilio-specific format\n      (case (:frame/type frame)\n        :audio/output (u/json-str {:event "media"\n                                   :streamSid stream-sid\n                                   :media {:payload (:data frame)}})\n        nil))\n\n    (deserialize-frame [_ raw-data]\n      ;; Convert Twilio message to pipeline frame\n      (let [data (u/parse-if-json raw-data)]\n        (case (:event data)\n          ;; TODO more cases\n          "media" (frames/audio-input-frame (u/decode-base64 (:payload (:media data))))\n          nil)))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/transport/async.clj`:\n\n```clj\n(ns voice-fn.transport.async\n  (:require\n   [clojure.core.async :as a]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as frames]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.transport.protocols :as tp]))\n\n(defn- input->frame\n  ([input]\n   (if (frames/audio-input-frame? input)\n     input\n     (throw (ex-info "Input is not a valid input frame. Please provide a serializer"\n                     {:input input\n                      :cause :error/transport-missing-serializer}))))\n  ([input serializer]\n   (if (nil? serializer)\n     (input->frame input)\n     (if (frames/audio-input-frame? input)\n       input\n       (tp/deserialize-frame serializer input)))))\n\n(defmethod pipeline/process-frame :transport/async-input\n  [processor-type pipeline _ frame]\n  (let [{:transport/keys [in-ch serializer]} (:pipeline/config @pipeline)\n        running? (atom false)]\n    (case (:frame/type frame)\n      :system/start\n      (do\n        (t/log! {:level :info\n                 :id processor-type} "Staring transport input")\n        (reset! running? true)\n        (a/go-loop []\n          (when running?\n            (when-let [input (a/<! in-ch)]\n              (when-let [input-frame (try\n                                       (input->frame input serializer)\n                                       (catch clojure.lang.ExceptionInfo e\n                                         (let [data (merge (ex-data e)\n                                                           {:message (ex-message e)})]\n                                           (a/>! (:pipeline/main-ch @pipeline) (frames/error-frame data)))))]\n                (a/>! (:pipeline/main-ch @pipeline) input-frame))\n              (recur)))))\n      :system/stop\n      (t/log! {:level :info\n               :id processor-type} "Stopping transport input")\n      (reset! running? false))))\n\n(defmethod pipeline/process-frame :transport/async-output\n  [type pipeline _ frame]\n  (t/log! {:level :debug\n           :id type}\n    ["Output frame" (:frame/data frame)])\n  (let [{:transport/keys [out-ch serializer]} (:pipeline/config @pipeline)]\n    (case (:frame/type frame)\n      :audio/output (when-let [output (if serializer\n                                        (tp/serialize-frame serializer frame)\n                                        frame)]\n                      (a/put! out-ch output))\n      :system/stop (a/close! out-ch))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/frames.clj`:\n\n```clj\n(ns voice-fn.frames)\n\n(defrecord BaseFrame [type data ts])\n\n(defn frame? [frame]\n  (instance? BaseFrame frame))\n\n(defn create-frame\n  [type data]\n  (let [ts (System/currentTimeMillis)]\n    ;; Using namespaced keywords for convenience sicne records don't support\n    ;; namespaced params by default\n    (map->BaseFrame {:type type\n                     :frame/type type\n                     :data data\n                     :frame/data data\n                     :ts ts\n                     :frame/ts ts})))\n\n(def start-frame (create-frame :system/start nil))\n\n(def stop-frame (create-frame :system/stop nil))\n\n(defmacro defframe\n  "Define a frame creator function and its predicate.\n   Usage: (defframe audio-input :audio/raw-input \"Doc string\")"\n  [name type docstring]\n  `(do\n     (defn ~name\n       ~docstring\n       [data#]\n       (create-frame ~type data#))\n\n     (defn ~(symbol (str name "?"))\n       [frame#]\n       (and (frame? frame#) (= ~type (:frame/type frame#))))))\n\n(defframe audio-input-frame :audio/raw-input "User audio input frame. Put on the pipeline by the input transport. Data is\n  bynary")\n\n(defframe audio-output-frame :audio/output "Audio frame to be played back to the user through output transport. Generated\n  by text to speech processors or multi modal (with voice capabilities) LLM processors. Data is binary")\n\n(defframe text-input-frame :text/input "Frame usually outputted by a transcription processor. Serves as input for text\n  LLM processors.")\n\n(defframe llm-output-text-sentence-frame :llm/output-text-sentence\n  "Frame generated by a sentence assembler that takes in chunks from a LLM processor")\n\n(defframe llm-output-text-chunk-frame :llm/output-text-chunk\n  "Frame outputted by text based streaming LLMs")\n\n(defframe error-frame :system/error "General error")\n\n(defframe elevenlabs-audio-chunk-frame :elevenlabs/audio-chunk "Elevenlabs doesn't send full json, sometimes it just sends partial JSON that needs to be assembled")\n\n(defframe llm-user-context-added-frame :llm/user-context-added "Frame sent after the user context was appended to the AI context. Used to sync the llm aggregator with the llm processor pipeline")\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/pipeline.clj`:\n\n```clj\n(ns voice-fn.pipeline\n  (:require\n   [clojure.core.async :as a :refer [<! >! chan go-loop]]\n   [malli.core :as m]\n   [malli.error :as me]\n   [malli.transform :as mt]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as frames]\n   [voice-fn.schema :as schema]\n   [voice-fn.secrets :refer [secret]]))\n\n(defmulti processor-schema\n  "Returns the malli schema for a processor type's configuration"\n  {:arglists '([processor-type])}\n  (fn [processor-type] processor-type))\n\n(defmethod processor-schema :default\n  [_]\n  any?)\n\n(defmulti make-processor-config\n  "Create the configuration for the processor based on the pipeline\n  configuration. Used when the final configuration for a processor requires\n  information from the global pipeline configuration. ex: audio-in encoding,\n  pipeline language, etc.\n\n  - type - the processor type\n\n  - pipeline-config - the global config of the pipeline. It contains config such\n  as the input audio-encoding, pipeline language, input and output channels and\n  more. See `voice-fn.pipeline/PipelineConfigSchema`.\n\n  - processor-config - the config of the processor as specified in the list of\n  processors from the pipeline"\n  {:arglists '([type pipeline-config processor-config])}\n  (fn [type _pipeline-config _processor-config]\n    type))\n\n(defmethod make-processor-config :default\n  [type _ processor-config]\n  (m/decode (processor-schema type) processor-config mt/default-value-transformer))\n\n(defmulti process-frame\n  "Process a frame from the pipeline.\n  - processor-type - type of processor: `:transport/local-audio` | `:transcription/deepgram`\n  - pipeline - atom containing the state of the pipeline.\n  - config - pipeline config\n  - frame - the frame to be processed by the processor"\n  {:arglists '([processor-type pipeline config frame])}\n  (fn [processor-type _state _config _frame]\n    processor-type))\n\n(def PipelineConfigSchema\n  [:map\n   [:audio-in/sample-rate {:default 16000} schema/SampleRate]\n   [:audio-in/channels {:default 1} schema/AudioChannels]\n   [:audio-in/encoding {:default :pcm-signed} schema/AudioEncoding]\n   [:audio-in/sample-size-bits {:default 16} schema/SampleSizeBits]\n   [:audio-out/sample-rate {:default 16000} schema/SampleRate]\n   [:audio-out/channels {:default 1} schema/AudioChannels]\n   [:audio-out/encoding {:default :pcm-signed} schema/AudioEncoding]\n   [:audio-out/sample-size-bits {:default 16} schema/SampleSizeBits]\n   [:pipeline/language schema/Language]\n   [:llm/context schema/LLMContext]\n   [:transport/in-ch schema/Channel]\n   [:transport/out-ch schema/Channel]])\n\n(defn validate-pipeline\n  "Validates the pipeline configuration and all processor configs.\n   Returns a map with :valid? boolean and :errors containing any validation errors.\n\n   Example return for valid config:\n   {:valid? true}\n\n   Example return for invalid config:\n   {:valid? false\n    :errors {:pipeline {...}           ;; Pipeline config errors\n             :processors [{:type :some/processor\n                          :errors {...}}]}} ;; Processor specific errors"\n  [{pipeline-config :pipeline/config\n    processors :pipeline/processors}]\n  (let [;; Validate main pipeline config\n        pipeline-valid? (m/validate PipelineConfigSchema pipeline-config)\n        pipeline-errors (when-not pipeline-valid?\n                          (me/humanize (m/explain PipelineConfigSchema pipeline-config)))\n\n        ;; Validate each processor's config\n        processor-results\n        (for [{:processor/keys [type config] :as processor} processors]\n          (let [schema (processor-schema type)\n                processor-config (make-processor-config type pipeline-config config)\n                processor-valid? (m/validate schema processor-config)\n                processor-errors (when-not processor-valid?\n                                   (me/humanize (m/explain schema processor-config)))]\n            {:type type\n             :valid? processor-valid?\n             :errors processor-errors}))\n\n        ;; Check if any processors are invalid\n        invalid-processors (filter (comp not :valid?) processor-results)\n\n        ;; Combine all validation results\n        all-valid? (and pipeline-valid?\n                        (empty? invalid-processors))]\n\n    (cond-> {:valid? all-valid?}\n\n      ;; Add pipeline errors if any\n      (not pipeline-valid?)\n      (assoc-in [:errors :pipeline] pipeline-errors)\n\n      ;; Add processor errors if any\n      (seq invalid-processors)\n      (assoc-in [:errors :processors]\n                (keep #(when-not (:valid? %)\n                         {:type (:type %)\n                          :errors (:errors %)})\n                      processor-results)))))\n\n(comment\n  (def in (a/chan 1))\n  (def out (a/chan 1))\n  (def test-pipeline-config {:pipeline/config {:audio-in/sample-rate 8000\n                                               :audio-in/encoding :ulaw\n                                               :audio-in/channels 1\n                                               :audio-in/sample-size-bits 8\n                                               :audio-out/sample-rate 8000\n                                               :audio-out/encoding :ulaw\n                                               :audio-out/sample-size-bits 8\n                                               :audio-out/channels 1\n                                               :pipeline/language :ro\n                                               :llm/context [{:role "system" :content  "Ești un agent vocal care funcționează prin telefon. Răspunde doar în limba română și fii succint. Inputul pe care îl primești vine dintr-un sistem de speech to text (transcription) care nu este intotdeauna eficient și poate trimite text neclar. Cere clarificări când nu ești sigur pe ce a spus omul."}]\n                                               :transport/in-ch in\n                                               :transport/out-ch out}\n                             :pipeline/processors\n                             [{:processor/type :transport/twilio-input\n                               :processor/accepted-frames #{:system/start :system/stop}\n                               :processor/generates-frames #{:audio/raw-input}}\n                              {:processor/type :transcription/deepgram\n                               :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                               :processor/generates-frames #{:text/input}\n                               :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                                  :transcription/interim-results? true\n                                                  :transcription/punctuate? false\n                                                  :transcription/vad-events? true\n                                                  :transcription/smart-format? true\n                                                  :transcription/model :nova-2}}\n                              {:processor/type :llm/context-aggregator\n                               :processor/accepted-frames #{:llm/output-text-sentence :text/input}\n                               :processor/generates-frames #{:llm/user-context-added}}\n                              {:processor/type :llm/openai\n                               :processor/accepted-frames #{:llm/user-context-added}\n                               :processor/generates-frames #{:llm/output-text-chunk}\n                               :processor/config {:llm/model "gpt-4o-mini"\n                                                  :openai/api-key (secret [:openai :new-api-sk])}}\n                              {:processor/type :log/text-input\n                               :processor/accepted-frames #{:text/input}\n                               :processor/generates-frames #{}\n                               :processor/config {}}\n                              {:processor/type :llm/sentence-assembler\n                               :processor/accepted-frames #{:system/stop :llm/output-text-chunk}\n                               :processor/generates-frames #{:llm/output-text-sentence}\n                               :processor/config {:sentence/end-matcher #"[.?!;:]"}}\n                              {:processor/type :tts/elevenlabs\n                               :processor/accepted-frames #{:system/stop :system/start :llm/output-text-sentence}\n                               :processor/generates-frames #{:audio/output :elevenlabs/audio-chunk}\n                               :processor/config {:elevenlabs/api-key (secret [:elevenlabs :api-key])\n                                                  :elevenlabs/model-id "eleven_flash_v2_5"\n                                                  :elevenlabs/voice-id "7sJPxFeMXAVWZloGIqg2"\n                                                  :voice/stability 0.5\n                                                  :voice/similarity-boost 0.8\n                                                  :voice/use-speaker-boost? true}}\n                              {:processor/type :elevenlabs/audio-assembler\n                               :processor/accepted-frames #{:elevenlabs/audio-chunk}\n                               :processor/generates-frames #{:audio/output}}\n                              {:processor/type :transport/async-output\n                               :processor/accepted-frames #{:audio/output :system/stop}\n                               :generates/frames #{}}]})\n\n  (validate-pipeline test-pipeline-config)\n\n  (make-processor-config :transcription/deepgram {:audio-in/sample-rate 8000\n                                                  :audio-in/encoding :ulaw\n                                                  :audio-in/channels 1\n                                                  :audio-in/sample-size-bits 8\n                                                  :audio-out/sample-rate 8000\n                                                  :audio-out/encoding :ulaw\n                                                  :audio-out/sample-size-bits 8\n                                                  :audio-out/channels 1\n                                                  :pipeline/language :ro}\n                         {:transcription/api-key (secret [:deepgram :api-key])\n                          :transcription/interim-results? true\n                          :transcription/punctuate? false\n                          :transcription/vad-events? true\n                          :transcription/smart-format? true\n                          :transcription/model :nova-2})\n\n  ,)\n\n(defn enrich-processor\n  [pipeline-config processor]\n  (assoc-in processor [:processor/config] (make-processor-config (:processor/type processor) pipeline-config (:processor/config processor))))\n\n(defn enrich-processors\n  "Add pipeline configuration to each processor config based on `make-processor-config`"\n  [pipeline]\n  (merge pipeline\n         {:pipeline/processors (mapv (partial enrich-processor (:pipeline/config pipeline)) (:pipeline/processors pipeline))}))\n\n;; Pipeline creation logic here\n(defn create-pipeline\n  "Creates a new pipeline from the provided configuration.\n\n   Throws ExceptionInfo with :type :invalid-pipeline-config when the configuration\n   is invalid. The exception data will contain :errors with detailed validation\n   information.\n\n   Returns an atom containing the initialized pipeline state."\n  [pipeline-config]\n  (let [validation-result (validate-pipeline pipeline-config)]\n    (if (:valid? validation-result)\n      (let [main-ch (chan 1024)\n            main-pub (a/pub main-ch :frame/type)\n            pipeline (atom (merge {:pipeline/main-ch main-ch\n                                   :pipeline/main-pub main-pub}\n                                  (enrich-processors pipeline-config)))]\n        ;; Start each processor\n        (doseq [{:processor/keys [type accepted-frames]} (:pipeline/processors pipeline-config)]\n          (let [processor-ch (chan 1024)]\n            ;; Tap into main channel, filtering for accepted frame types\n            (doseq [frame-type accepted-frames]\n              (a/sub main-pub frame-type processor-ch))\n            (swap! pipeline assoc-in [type :processor/in-ch] processor-ch)))\n        pipeline)\n      ;; Throw detailed validation error\n      (throw (ex-info "Invalid pipeline configuration"\n                      {:type :pipeline/invalid-configuration\n                       :errors (:errors validation-result)})))))\n\n(defn start-pipeline!\n  [pipeline]\n  (t/log! :debug "Starting pipeline")\n  (a/put! (:pipeline/main-ch @pipeline) {:frame/type :system/start})\n  ;; Start each processor\n  (doseq [{:processor/keys [type] :as processor} (:pipeline/processors @pipeline)]\n    (go-loop []\n      (when-let [frame (<! (get-in @pipeline [type :processor/in-ch]))]\n        (when-let [result (process-frame type pipeline processor frame)]\n          ;; Put results back on main channel if the processor returned frames\n          (when (frames/frame? result)\n            (>! (:pipeline/main-ch @pipeline) result)))\n        (recur)))))\n\n(defn stop-pipeline!\n  [pipeline]\n  (t/log! :debug "Stopping pipeline")\n  (t/log! :debug ["Conversation so far" (get-in @pipeline [:pipeline/config :llm/context])])\n  (a/put! (:pipeline/main-ch @pipeline) {:frame/type :system/stop}))\n\n(defn close-processor!\n  [pipeline type]\n  (t/log! {:level :debug\n           :id type} "Closing processor")\n  (a/close! (get-in @pipeline [type :processor/in-ch])))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/schema.clj`:\n\n```clj\n(ns voice-fn.schema\n  (:require\n   [clojure.core.async.impl.protocols :as async-protocols]\n   [malli.core :as m]\n   [malli.error :as me]\n   [malli.transform :as mt]))\n\n(defn flex-enum\n  "Creates a flexible enum that accepts both keywords and their string versions.\n   Input: coll of keywords or string\n   Output: Malli enum schema that accepts both forms\n\n   Example:\n   (flex-enum [:nova-2 \"nova-2-general\" :gpt-4 \"gpt-4-turbo\"])"\n  ([vals]\n   (flex-enum {} vals))\n  ([meta vals]\n   (into [:enum meta] (distinct (mapcat (fn [v]\n                                          [(name v)\n                                           (keyword v)]) vals)))))\n\n(defn regex?\n  [input]\n  (instance? java.util.regex.Pattern input))\n\n(defn safe-coerce\n  "Coerce to value without throwing error but instead "\n  [schema value transformer]\n  (m/coerce schema value transformer identity (partial (comp me/humanize :explain))))\n\n(def SampleRate\n  [:enum\n   {:description "Audio sample rate in Hz"\n    :error/message "Invalid sample rate"}\n   8000                                 ; Low quality telephony\n   11025                                ; Quarter of CD quality\n   12000                                ; Some voice recording\n   16000                                ; Common for speech recognition\n   22050                                ; Half CD quality\n   24000                                ; Some digital audio\n   32000                                ; Digital broadcasting\n   44100                                ; CD quality\n   48000                                ; Professional audio/DAT\n   88200                                ; High-res audio (2× 44.1)\n   96000                                ; High-res audio/video\n   176400                               ; Highest-res audio (4× 44.1)\n   192000])\n\n(def AudioEncoding\n  [:enum\n   :alaw\n   :ulaw\n   :pcm-signed\n   :pcm-unsigned\n   :pcm-float])\n\n(def AudioChannels\n  [:enum\n   {:default 1\n    :description "Type of audio (mono/stereo)"\n    :error/message "Invalid audio channels"}\n   1   ;; mono\n   2]) ;; stereo\n\n(def SampleSizeBits\n  [:enum {:description "Audio sample size in bits"\n          :title "Sample Size"\n          :json-schema/example 16}\n   8 16 24 32])\n\n(def Language\n  (flex-enum\n    {:description "Language codes including regional variants"}\n    [;; Afrikaans\n     "af" "af-ZA"\n\n     ;; Amharic\n     "am" "am-ET"\n\n     ;; Arabic\n     "ar" "ar-AE" "ar-BH" "ar-DZ" "ar-EG" "ar-IQ" "ar-JO" "ar-KW"\n     "ar-LB" "ar-LY" "ar-MA" "ar-OM" "ar-QA" "ar-SA" "ar-SY" "ar-TN" "ar-YE"\n\n     ;; Assamese\n     "as" "as-IN"\n\n     ;; Azerbaijani\n     "az" "az-AZ"\n\n     ;; Bulgarian\n     "bg" "bg-BG"\n\n     ;; Bengali\n     "bn" "bn-BD" "bn-IN"\n\n     ;; Bosnian\n     "bs" "bs-BA"\n\n     ;; Catalan\n     "ca" "ca-ES"\n\n     ;; Czech\n     "cs" "cs-CZ"\n\n     ;; Welsh\n     "cy" "cy-GB"\n\n     ;; Danish\n     "da" "da-DK"\n\n     ;; German\n     "de" "de-AT" "de-CH" "de-DE"\n\n     ;; Greek\n     "el" "el-GR"\n\n     ;; English\n     "en" "en-AU" "en-CA" "en-GB" "en-HK" "en-IE" "en-IN" "en-KE"\n     "en-NG" "en-NZ" "en-PH" "en-SG" "en-TZ" "en-US" "en-ZA"\n\n     ;; Spanish\n     "es" "es-AR" "es-BO" "es-CL" "es-CO" "es-CR" "es-CU" "es-DO"\n     "es-EC" "es-ES" "es-GQ" "es-GT" "es-HN" "es-MX" "es-NI" "es-PA"\n     "es-PE" "es-PR" "es-PY" "es-SV" "es-US" "es-UY" "es-VE"\n\n     ;; Estonian\n     "et" "et-EE"\n\n     ;; Basque\n     "eu" "eu-ES"\n\n     ;; Persian\n     "fa" "fa-IR"\n\n     ;; Finnish\n     "fi" "fi-FI"\n\n     ;; Filipino\n     "fil" "fil-PH"\n\n     ;; French\n     "fr" "fr-BE" "fr-CA" "fr-CH" "fr-FR"\n\n     ;; Irish\n     "ga" "ga-IE"\n\n     ;; Galician\n     "gl" "gl-ES"\n\n     ;; Gujarati\n     "gu" "gu-IN"\n\n     ;; Hebrew\n     "he" "he-IL"\n\n     ;; Hindi\n     "hi" "hi-IN"\n\n     ;; Croatian\n     "hr" "hr-HR"\n\n     ;; Hungarian\n     "hu" "hu-HU"\n\n     ;; Armenian\n     "hy" "hy-AM"\n\n     ;; Indonesian\n     "id" "id-ID"\n\n     ;; Icelandic\n     "is" "is-IS"\n\n     ;; Italian\n     "it" "it-IT"\n\n     ;; Inuktitut\n     "iu-Cans" "iu-Cans-CA" "iu-Latn" "iu-Latn-CA"\n\n     ;; Japanese\n     "ja" "ja-JP"\n\n     ; Javanese\n     "jv" "jv-ID"\n\n     ;; Georgian\n     "ka" "ka-GE"\n\n     ;; Kazakh\n     "kk" "kk-KZ"\n\n     ;; Khmer\n     "km" "km-KH"\n\n     ;; Kannada\n     "kn" "kn-IN"\n\n     ;; Korean\n     "ko" "ko-KR"\n\n     ;; Lao\n     "lo" "lo-LA"\n\n     ;; Lithuanian\n     "lt" "lt-LT"\n\n     ;; Latvian\n     "lv" "lv-LV"\n\n     ;; Macedonian\n     "mk" "mk-MK"\n\n     ;; Malayalam\n     "ml" "ml-IN"\n\n     ;; Mongolian\n     "mn" "mn-MN"\n\n     ;; Marathi\n     "mr" "mr-IN"\n\n     ;; Malay\n     "ms" "ms-MY"\n\n     ;; Maltese\n     "mt" "mt-MT"\n\n     ;; Burmese\n     "my" "my-MM"\n\n     ;; Norwegian\n     "nb" "nb-NO" "no"\n\n     ;; Nepali\n     "ne" "ne-NP"\n\n     ;; Dutch\n     "nl" "nl-BE" "nl-NL"\n\n     ;; Odia\n     "or" "or-IN"\n\n     ;; Punjabi\n     "pa" "pa-IN"\n\n     ;; Polish\n     "pl" "pl-PL"\n\n     ;; Pashto\n     "ps" "ps-AF"\n\n     ;; Portuguese\n     "pt" "pt-BR" "pt-PT"\n\n     ;; Romanian\n     "ro" "ro-RO"\n\n     ;; Russian\n     "ru" "ru-RU"\n\n     ;; Sinhala\n     "si" "si-LK"\n\n     ;; Slovak\n     "sk" "sk-SK"\n\n     ;; Slovenian\n     "sl" "sl-SI"\n\n     ;; Somali\n     "so" "so-SO"\n\n     ;; Albanian\n     "sq" "sq-AL"\n\n     ;; Serbian\n     "sr" "sr-RS" "sr-Latn" "sr-Latn-RS"\n\n     ;; Sundanese\n     "su" "su-ID"\n\n     ;; Swedish\n     "sv" "sv-SE"\n\n     ;; Swahili\n     "sw" "sw-KE" "sw-TZ"\n\n     ;; Tagalog\n     "tl"\n\n     ;; Tamil\n     "ta" "ta-IN" "ta-LK" "ta-MY" "ta-SG"\n\n     ;; Telugu\n     "te" "te-IN"\n\n     ;; Thai\n     "th" "th-TH"\n\n     ;; Turkish\n     "tr" "tr-TR"\n\n     ;; Ukrainian\n     "uk" "uk-UA"\n\n     ;; Urdu\n     "ur" "ur-IN" "ur-PK"\n\n     ;; Uzbek\n     "uz" "uz-UZ"\n\n     ;; Vietnamese\n     "vi" "vi-VN"\n\n     ;; Wu Chinese\n     "wuu" "wuu-CN"\n\n     ;; Yue Chinese\n     "yue" "yue-CN"\n\n     ;; Chinese\n     "zh" "zh-CN" "zh-CN-guangxi" "zh-CN-henan" "zh-CN-liaoning"\n     "zh-CN-shaanxi" "zh-CN-shandong" "zh-CN-sichuan" "zh-HK" "zh-TW"\n\n     ;; Xhosa\n     "xh"\n\n     ;; Zulu\n     "zu" "zu-ZA"]))\n\n(def LLMContextessageRole\n  (flex-enum\n    {:description "Role of the message participant"\n     :error/message "Role must be system, assistant, or user"}\n    ["system"\n     "assistant"\n     "user"]))\n\n(def LLMContextMessage\n  [:map\n   {:description "Message from the LLM Context"\n    :error/message "Invalid message format"\n    :closed true} ;; No additional fields allowed\n   [:role LLMContextessageRole]\n   [:content [:string {:min 1}]]])\n\n(def LLMContext\n  [:vector\n   {:min 1\n    :max 100 ;; Optional max limit\n    :description "Vector of context messages"\n    :error/message "Must have at least one context message"}\n   LLMContextMessage])\n\n(def Channel\n  [:fn\n   {:error/message "Must be a core.async channel"\n    :description "core.async channel"}\n   #(satisfies? async-protocols/Channel %)])\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/secrets.clj`:\n\n```clj\n(ns voice-fn.secrets\n  (:require\n   [clojure.edn :as edn]\n   [clojure.java.io :as io]))\n\n(defn- secret-map\n  []\n  (edn/read-string (slurp (io/resource "secrets.edn"))))\n\n(defn secret\n  [path]\n  (get-in (secret-map) path))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/core.clj`:\n\n```clj\n(ns voice-fn.core\n  (:require\n   [clojure.core.async :as a]\n   [ring.websocket.protocols :as wsp]\n   [taoensso.telemere :as t]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.processors.deepgram]\n   [voice-fn.processors.openai]\n   [voice-fn.secrets :refer [secret]]\n   [voice-fn.transport.async]\n   [voice-fn.transport.local.audio]))\n\n(def local-transcription-log-pipeline\n  {:pipeline/config {:audio-in/sample-rate 16000\n                     :audio-in/encoding :pcm-signed\n                     :audio-in/channels 1\n                     :audio-in/file-path "test-voice.wav"\n                     :audio-in/sample-size-bits 16 ;; 2 bytes\n                     :audio-out/sample-rate 24000\n                     :audio-out/bitrate 96000\n                     :audio-out/sample-size-bits 16\n                     :audio-out/channels 1\n                     :pipeline/language :ro}\n   :pipeline/processors [{:processor/type :transport/local-audio\n                          :processor/accepted-frames #{:system/start :system/stop}\n                          :processor/generates-frames #{:audio/raw-input}}\n                         {:processor/type :transcription/deepgram\n                          :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                          :processor/generates-frames #{:text/input}\n                          :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                             :transcription/interim-results? false\n                                             :transcription/punctuate? false\n                                             :transcription/model :nova-2}}\n                         {:processor/type :log/text-input\n                          :processor/accepted-frames #{:text/input}\n                          :processor/config {}}]})\n\n(def ws-client (a/chan 10))\n(def ws-server (a/chan 10))\n\n(def mock-socket (reify\n                   wsp/Socket\n                   (-send [_ message]\n                     (a/put! ws-client [:send message]))\n                   (-close [_ code reason]\n                     (a/>!! ws-client [:close code reason]))\n                   wsp/AsyncSocket\n                   (-send-async [_ mesg succeed _]\n                     (a/>!! ws-client [:send mesg])\n                     (succeed))))\n\n(def async-echo-pipeline\n  {:pipeline/config {:audio-in/sample-rate 8000\n                     :audio-in/encoding :ulaw\n                     :audio-in/channels 1\n                     :audio-in/sample-size-bits 8\n                     :audio-out/sample-rate 8000\n                     :audio-out/encoding :ulaw\n                     :audio-out/bitrate 64000\n                     :audio-out/sample-size-bits 8\n                     :audio-out/channels 1\n                     :pipeline/language :ro}\n   :pipeline/processors [;; Transport in\n                         {:processor/type :transport/async-input\n                          :processor/accepted-frames #{:system/start :system/stop}\n                          :processor/generates-frames #{:audio/raw-input}}\n                         ;; Transcription\n                         {:processor/type :transcription/deepgram\n                          :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                          :processor/generates-frames #{:text/input}\n                          :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                             :transcription/interim-results? false\n                                             :transcription/punctuate? false\n                                             :transcription/model :nova-2}}\n                         ;; LLM\n                         {:processor/type :llm/openai\n                          :processor/accepted-frames #{:system/stop :text/input}\n                          :processor/generates-frames #{:llm/output-text-chunk}\n                          :processor/config {:llm/model "gpt-4o-mini"\n                                             :llm/messages [{:role "system" :content "You are a helpful assistant"}]\n                                             :openai/api-key (secret [:openai :new-api-key])}}\n\n                         ;; Sentence assembler\n                         {:processor/type :llm/sentence-assembler\n                          :processor/accepted-frames #{:system/stop :system/start :llm/output-text-chunk}\n                          :processor/generates-frames #{:llm/output-text-sentence}\n                          :processor/config {:sentence/end-matcher #"[.?!]"}}\n                         ;; TTS\n                         {:processor/type :tts/elevenlabs\n                          :processor/accepted-frames #{:system/stop :system/start :llm/output-text-sentence}\n                          :processor/generates-frames #{:audio/output}\n                          :processor/config {:elevenlabs/api-key (secret [:elevenlabs :api-key])\n                                             :elevenlabs/model-id "eleven_flash_v2_5"\n                                             :elevenlabs/voice-id "7sJPxFeMXAVWZloGIqg2"\n                                             :voice/stability 0.5\n                                             :voice/similarity-boost 0.8\n                                             :voice/use-speaker-boost? true}}\n                         ;; Logger\n                         {:processorvoi/type :log/text-input\n                          :processor/accepted-frames #{:text/input}\n                          :processor/generates-frames #{}\n                          :processor/config {}}\n                         ;; Transport out\n                         {:processor/type :transport/async-output\n                          :processor/accepted-frames #{:audio/output :system/stop}\n                          :generates/frames #{}}]})\n\n(defmethod pipeline/process-frame :log/text-input\n  [_ _ _ frame]\n  (t/log! {:level :info\n           :id :log/text-input} ["Frame" (:frame/data frame)]))\n\n(t/set-min-level! :debug)\n\n(comment\n  (def p (pipeline/create-pipeline async-echo-pipeline))\n\n  @p\n\n  (pipeline/start-pipeline! p)\n  (pipeline/stop-pipeline! p)\n\n  ,)\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/utils/core.clj`:\n\n```clj\n(ns voice-fn.utils.core\n  (:require\n   [clojure.string :as str]\n   [jsonista.core :as json])\n  (:import\n   (java.util Base64)))\n\n(defmulti encode-base64 (fn [s] (class s)))\n\n(defmethod encode-base64 String\n  [s]\n  (let [encoder (Base64/getEncoder)]\n    (.encodeToString encoder (.getBytes s "UTF-8"))))\n\n(defmethod encode-base64 (Class/forName "[B")\n  [bytes]\n  (let [encoder (Base64/getEncoder)]\n    (.encodeToString encoder bytes)))\n\n(defn decode-base64\n  [s]\n  (let [decoder (Base64/getDecoder)]\n    (.decode decoder s)))\n\n(defonce json-object-mapper (json/object-mapper {:decode-key-fn true}))\n\n(defn parse-if-json\n  "Parses a string as JSON if possible, otherwise returns the string."\n  [s]\n  (try\n    (json/read-value s json-object-mapper)\n    (catch Exception _\n      s)))\n\n(defn json-str\n  [m]\n  (json/write-value-as-string m))\n\n(defn search-params\n  [url]\n  (let [params (second (str/split url #"\?"))\n        param-list (if params (str/split params #"\&") [])]\n    (reduce #(let [[k v] (str/split %2 #"\=")]\n               (assoc %1 (keyword k) v)) {} param-list)))\n\n(defn strip-search-params\n  [url]\n  (first (str/split url #"\?")))\n\n(defn append-search-params\n  [url search-params-m]\n  (let [search-params (merge (search-params url)\n                             search-params-m)\n        search (->> (map  (fn [[k v]] (str (name k) "=" (if (keyword? v) (name v) v))) search-params)\n                    (str/join #"&")\n                    (str "?"))]\n    (str (strip-search-params url) search)))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/processors/llm_sentence_assembler.clj`:\n\n```clj\n(ns voice-fn.processors.llm-sentence-assembler\n  (:require\n   [clojure.core.async :as a]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as frames]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.schema :as schema]))\n\n(def default-end-sentence-matcher #"[.?!;:]")\n\n(def SentenceAssemblerConfig\n  [:map\n   {:closed true\n    :description "Configuration for sentence assembly and detection"}\n   [:sentence/end-matcher\n    [:fn\n     {:error/message "Must be a valid regex patter (java.util.regex.Pattern)"\n      :default #"[.?!;:]"\n      :description "Regular expression pattern for detecting sentence endings"}\n     schema/regex?]]])\n\n(defmethod pipeline/processor-schema :llm/sentence-assembler\n  [_]\n  SentenceAssemblerConfig)\n\n(defmethod pipeline/process-frame :llm/sentence-assembler\n  [processor-type pipeline {:processor/keys [config]} {:frame/keys [data type]}]\n  (let [end-sentence-matcher (:sentence/end-matcher config default-end-sentence-matcher)\n        sentence (get-in @pipeline [processor-type :sentence] "")]\n    (case type\n      :llm/output-text-chunk\n      (if (re-find end-sentence-matcher data)\n        (let [full-sentence (str sentence data)]\n          (t/log! :debug ["Full sentence" full-sentence])\n          (swap! pipeline assoc-in [processor-type :sentence] "")\n          (a/put! (:pipeline/main-ch @pipeline)\n                  (frames/llm-output-text-sentence-frame full-sentence)))\n        (swap! pipeline assoc-in [processor-type :sentence] (str sentence data)))\n      nil)))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/processors/openai.clj`:\n\n```clj\n(ns voice-fn.processors.openai\n  (:require\n   [clojure.core.async :as a]\n   [voice-fn.frames :as frames]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.schema :as schema]\n   [wkok.openai-clojure.api :as api]))\n\n(def token-content (comp :content :delta first :choices))\n\n(def OpenAILLMConfigSchema\n  [:map\n   {:closed true\n    :description "OpenAI LLM configuration"}\n\n   [:llm/model\n    (schema/flex-enum\n      {:description "OpenAI model identifier"\n       :error/message "Must be a valid OpenAI model"\n       :default "gpt-4o-mini"}\n      [;; GPT-4 Models\n       "gpt-4"\n       "gpt-4-32k"\n       "gpt-4-1106-preview"   ;; GPT-4 Turbo\n       "gpt-4-vision-preview" ;; GPT-4 Vision\n       ;; GPT-3.5 Models\n       "gpt-3.5-turbo"\n       "gpt-3.5-turbo-16k"\n       "gpt-3.5-turbo-1106"\n       ;; Base Models\n       "babbage-002"\n       "davinci-002"\n       ;; Include your custom model\n       "gpt-4o-mini"])]\n\n   [:openai/api-key\n    [:string\n     {:description "OpenAI API key"\n      :secret true ;; Marks this as sensitive data\n      :min 40      ;; OpenAI API keys are typically longer\n      :error/message "Invalid OpenAI API key format"}]]])\n\n;; Example validation:\n(comment\n  (require '[malli.core :as m]\n           '[malli.error :as me])\n  ;; Valid config\n  (m/validate OpenAILLMConfigSchema\n              {:llm/model :gpt-4o-mini\n               :openai/api-key "sk-12312312312312312312312312312312312312312313..."})\n  ;; => true\n\n  ;; Invalid model\n  (-> OpenAILLMConfigSchema\n      (m/explain {:llm/model "invalid-model"\n                  :openai/api-key "sk-..."})\n      me/humanize))\n\n(defmethod pipeline/processor-schema :llm/openai\n  [_]\n  OpenAILLMConfigSchema)\n\n(defmethod pipeline/process-frame :llm/openai\n  [_type pipeline processor frame]\n  (let [{:llm/keys [model] :openai/keys [api-key]} (:processor/config processor)\n        {:llm/keys [context]} (:pipeline/config @pipeline)]\n    (case (:frame/type frame)\n      :llm/user-context-added (a/pipeline\n                                1\n                                (:pipeline/main-ch @pipeline)\n                                (comp (map token-content) (filter some?) (map frames/llm-output-text-chunk-frame))\n                                (api/create-chat-completion {:model model\n                                                             :messages context\n                                                             :stream true}\n                                                            {:api-key api-key\n                                                             :version :http-2 :as :stream})))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/processors/groq.clj`:\n\n```clj\n(ns voice-fn.processors.groq\n  (:require\n   [voice-fn.secrets :refer [secret]]\n   [wkok.openai-clojure.api :as api]))\n\n(def groq-api-url "https://api.groq.com/openai/v1")\n\n(comment\n  (api/create-completion\n    {:model "llama3-8b-8192"\n     :messages [{:role "system" :content "You are a helpful assistant."}\n                {:role "user" :content "Who won the world series in 2020?"}\n                {:role "assistant" :content "The Los Angeles Dodgers won the World Series in 2020."}\n                {:role "user" :content "Where was it played?"}]}\n\n    {:api-key (secret [:groq :api-key])\n\n     :api-endpoint groq-api-url}))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/processors/deepgram.clj`:\n\n```clj\n(ns voice-fn.processors.deepgram\n  (:require\n   [clojure.core.async :as a]\n   [hato.websocket :as ws]\n   [malli.core :as m]\n   [malli.error :as me]\n   [malli.transform :as mt]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as frames]\n   [voice-fn.pipeline :refer [close-processor! make-processor-config process-frame processor-schema]]\n   [voice-fn.schema :as schema :refer [flex-enum]]\n   [voice-fn.utils.core :as u])\n  (:import\n   (java.nio HeapCharBuffer)))\n\n(def ^:private deepgram-url "wss://api.deepgram.com/v1/listen")\n\n(def deepgram-encoding\n  "Mapping from clojure sound encoding to deepgram notation"\n  {:pcm-signed :linear16\n   :pcm-unsigned :linear16\n   :pcm-float :linear16\n   :ulaw :mulaw\n   :alaw :alaw})\n\n(defn make-deepgram-url\n  [{:transcription/keys [interim-results? punctuate? model sample-rate language  vad-events? smart-format? encoding channels]\n    :or {interim-results? false\n         punctuate? false}}]\n  (u/append-search-params deepgram-url {:encoding encoding\n                                        :language language\n                                        :sample_rate sample-rate\n                                        :model model\n                                        :smart_format smart-format?\n                                        :channels channels\n                                        :vad_events vad-events?\n                                        :interim_results interim-results?\n                                        :punctuate punctuate?}))\n\n(defn transcript?\n  [m]\n  (= (:event m) "transcript"))\n\n(defn- transcript\n  [m]\n  (-> m :channel :alternatives first :transcript))\n\n(defn final-transcription?\n  [m]\n  (and (transcript? m)\n       (= (:type m) "final")))\n\n(defn close-connection-payload\n  []\n  (u/json-str {:type "CloseStream"}))\n\n(declare create-connection-config)\n\n(def max-reconnect-attempts 5)\n\n(defn connect-websocket!\n  [type pipeline processor-config]\n  (let [current-count (get-in @pipeline [type :websocket/reconnect-count] 0)]\n    (if (>= current-count max-reconnect-attempts)\n      (t/log! :warn "Maximum reconnection attempts reached for Deepgram")\n      (do\n        (t/log! :info (str "Attempting to connect to Deepgram (attempt " (inc current-count) "/" max-reconnect-attempts ")"))\n        (swap! pipeline update-in [type :websocket/reconnect-count] (fnil inc 0))\n        (let [websocket-url (make-deepgram-url processor-config)\n              _ (t/log! :debug ["WEBSOCKET URL" websocket-url])\n              conn-config (create-connection-config\n                            type\n                            pipeline\n                            processor-config)\n              new-conn @(ws/websocket\n                          websocket-url\n                          conn-config)]\n          (swap! pipeline assoc-in [type :websocket/conn] new-conn))))))\n\n(def deepgram-events\n  (atom []))\n\n(defn create-connection-config\n  [type pipeline processor-config]\n  {:headers {"Authorization" (str "Token " (:transcription/api-key processor-config))}\n   :on-open (fn [_]\n              (t/log! :info "Deepgram websocket connection open"))\n   :on-message (fn [_ws ^HeapCharBuffer data _last?]\n                 (let [m (u/parse-if-json (str data))\n                       trsc (transcript m)]\n                   (swap! deepgram-events conj m)\n                   (when (and trsc (not= trsc ""))\n                     (a/put! (:pipeline/main-ch @pipeline)\n                             (frames/text-input-frame trsc)))))\n   :on-error (fn [_ e]\n               (t/log! :error ["Error" e]))\n   :on-close (fn [_ws code reason]\n               (t/log! :info ["Deepgram websocket connection closed" "Code:" code "Reason:" reason])\n               (when (= code 1011) ;; timeout\n                 (connect-websocket! type pipeline processor-config)))})\n\n(defn- close-websocket-connection!\n  [type pipeline]\n  (when-let [conn (get-in @pipeline [type :websocket/conn])]\n    (ws/send! conn (close-connection-payload))\n    (ws/close! conn))\n  (swap! pipeline update-in [:transcription/deepgram] dissoc :websocket/conn))\n\n(def code-reason\n  {1011 :timeout})\n\n(def DeepgramConfig\n  [:and\n   [:map\n    [:transcription/api-key :string]\n    [:transcription/model {:default :nova-2-general}\n     (flex-enum (into [:nova-2] (map #(str "nova-2-" %) #{"general" "meeting" "phonecall" "voicemail" "finance" "conversationalai" "video" "medical" "drivethru" "automotive" "atc"})))]\n    [:transcription/interim-results? {:default true} :boolean]\n    [:transcription/channels {:default 1} [:enum 1 2]]\n    [:transcription/smart-format? {:default true} :boolean]\n    [:transcription/profanity-filter? {:default true} :boolean]\n    [:transcription/vad-events? {:default false} :boolean]\n    [:transcription/sample-rate schema/SampleRate]\n    [:transcription/encoding {:default :linear16} (flex-enum [:linear16 :mulaw :alaw :mp3 :opus :flac :aac])]\n    [:transcription/language {:default :en} schema/Language]\n    [:transcription/punctuate? {:default false} :boolean]]\n   ;; if smart-format is true, no need for punctuate\n   [:fn {:error/message "When :transcription/smart-format? is true, :transcription/punctuate? must be false. More details here: https://developers.deepgram.com/docs/smart-format#enable-feature"}\n    (fn [{:transcription/keys [smart-format? punctuate?]}]\n      (not (and smart-format? punctuate?)))]])\n\n(defn pipeline->deepgram-config\n  [value]\n  (cond-> {}\n    ;; Map sample rate directly\n    (:audio-in/sample-rate value)\n    (assoc :transcription/sample-rate (:audio-in/sample-rate value))\n\n    ;; Map encoding with conversion\n    (:audio-in/encoding value)\n    (assoc :transcription/encoding\n           (get deepgram-encoding (:audio-in/encoding value)))\n\n    ;; Map language directly\n    (:pipeline/language value)\n    (assoc :transcription/language (:pipeline/language value))))\n\n(defmethod processor-schema :transcription/deepgram\n  [_]\n  DeepgramConfig)\n\n(defmethod make-processor-config :transcription/deepgram\n  [_ pipeline-config processor-config]\n  (m/decode DeepgramConfig\n            (merge processor-config\n                   (pipeline->deepgram-config pipeline-config))\n            (mt/default-value-transformer {::mt/add-optional-keys true})))\n\n(defmethod process-frame :transcription/deepgram\n  [type pipeline processor frame]\n  (let [on-close! (fn []\n                    (t/log! :debug "Stopping transcription engine")\n                    (close-websocket-connection! type pipeline)\n                    (close-processor! pipeline type))]\n    (case (:frame/type frame)\n      :system/start\n      (do (t/log! :debug "Starting transcription engine")\n          (connect-websocket! type pipeline (:processor/config processor)))\n      :system/stop (on-close!)\n\n      :audio/raw-input\n      (when-let [conn (get-in @pipeline [type :websocket/conn])]\n        (ws/send! conn (:data frame))))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/processors/llm_context_aggregator.clj`:\n\n```clj\n(ns voice-fn.processors.llm-context-aggregator\n  (:require\n   [clojure.core.async :as a]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as f]\n   [voice-fn.pipeline :as pipeline]))\n\nconcat\n\n(defn concat-context\n  "Concat to context a new message. If the last message from the context is from\n  the same role, concattenate in the same context object.\n  (concat-context [{:role :system :content \"Hello\"}] :system \", world\")\n  ;; => [{:role :system :content \"Hello, world\"}]\n  "\n  ([context entry]\n   (concat-context context (:role entry) (:content entry)))\n  ([context role content]\n   (let [last-entry (last context)]\n     (if (= (name (:role last-entry)) (name role))\n       (into (vec (butlast context))\n             [{:role role\n               :content (str (:content last-entry) " " content)}])\n       (into context\n             [{:role role :content content}])))))\n\n(defmethod pipeline/process-frame :llm/context-aggregator\n  [_ pipeline _ frame]\n  (case (:frame/type frame)\n    :llm/output-text-sentence\n    (do\n      (t/log! :info ["AI sentence" (:frame/data frame)])\n      (swap! pipeline update-in [:pipeline/config :llm/context] concat-context "assistant" (:frame/data frame)))\n    :text/input\n    (do (swap! pipeline update-in [:pipeline/config :llm/context] concat-context "user" (:frame/data frame))\n        (a/put! (:pipeline/main-ch @pipeline) (f/llm-user-context-added-frame true)))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/core/src/voice_fn/processors/elevenlabs.clj`:\n\n```clj\n(ns voice-fn.processors.elevenlabs\n  (:require\n   [clojure.core.async :as a]\n   [hato.websocket :as ws]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as f]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.schema :as schema]\n   [voice-fn.secrets :as secrets]\n   [voice-fn.utils.core :as u])\n  (:import\n   (java.nio HeapCharBuffer)))\n\n(def ^:private xi-tts-websocket-url "wss://api.elevenlabs.io/v1/text-to-speech/%s/stream-input")\n\n(def elevenlabs-encoding\n  "Mapping from clojure sound encoding to elevenlabs format"\n  {:ulaw :ulaw_8000\n   :mp3 :mp3_44100})\n\n(defn encoding->elevenlabs\n  ([format]\n   (elevenlabs-encoding format))\n  ([format sample-rate]\n   (keyword (str (name format) "_" sample-rate))))\n\n(defn make-elevenlabs-url\n  [pipeline-config processor-config]\n  (let [{:audio-out/keys [encoding sample-rate]\n         :pipeline/keys [language]} pipeline-config\n        {:elevenlabs/keys [model-id voice-id]\n         :or {model-id "eleven_flash_v2_5"\n              voice-id "cjVigY5qzO86Huf0OWal"}}\n        processor-config]\n    (u/append-search-params (format xi-tts-websocket-url voice-id)\n                            {:model_id model-id\n                             :language_code language\n                             :output_format (encoding->elevenlabs encoding sample-rate)})))\n\n(comment\n  (make-elevenlabs-url {:audio-in/sample-rate 8000\n                        :audio-in/encoding :ulaw\n                        :audio-in/channels 1\n                        :audio-in/sample-size-bits 8\n                        :audio-out/sample-rate 8000\n                        :audio-out/encoding :ulaw\n                        :audio-out/bitrate 64000\n                        :audio-out/sample-size-bits 8\n                        :audio-out/channels 1\n                        :pipeline/language :ro}\n                       {:elevenlabs/api-key (secrets/secret [:elevenlabs :api-key])\n                        :elevenlabs/model-id "eleven_flash_v2_5"\n                        :elevenlabs/voice-id "7sJPxFeMXAVWZloGIqg2"\n                        :voice/stability 0.5\n                        :voice/similarity-boost 0.8\n                        :voice/use-speaker-boost? true}))\n\n(defn begin-stream-message\n  [{:voice/keys [stability similarity-boost use-speaker-boost?]\n    :elevenlabs/keys [api-key]\n    :or {stability 0.5\n         similarity-boost 0.8\n         use-speaker-boost? true}}]\n\n  (u/json-str {:text " "\n               :voice_settings {:stability stability\n                                :similarity_boost similarity-boost\n                                :use_speaker_boost use-speaker-boost?}\n               :xi_api_key api-key}))\n\n(def close-stream-message\n  {:text ""})\n\n(defn text-message\n  [text]\n  (u/json-str {:text (str text " ")\n               :flush true}))\n\n(defn create-connection-config\n  [type pipeline processor-config]\n  {:on-open (fn [ws]\n              (let [configuration (begin-stream-message processor-config)]\n                (t/log! :info ["Elevenlabs websocket connection open. Sending configuration message" configuration])\n                (ws/send! ws configuration)))\n   :on-message (fn [_ws ^HeapCharBuffer data _last?]\n                 (a/put! (:pipeline/main-ch @pipeline)\n                         (f/elevenlabs-audio-chunk-frame (str data))))\n   :on-error (fn [_ e]\n               (t/log! :error ["Elevenlabs websocket error" (ex-message e)]))\n   :on-close (fn [_ws code reason]\n               (t/log! :info ["Elevenlabs websocket connection closed" "Code:" code "Reason:" reason]))})\n\n(def max-reconnect-attempts 5)\n\n(defn connect-websocket!\n  [type pipeline processor-config]\n  (let [current-count (get-in @pipeline [type :websocket/reconnect-count] 0)]\n    (if (>= current-count max-reconnect-attempts)\n      (t/log! :warn "Maximum reconnection attempts reached for Elevenlabs")\n      (do\n        (t/log! :info (str "Attempting to connect to Elevenlabs (attempt " (inc current-count) "/" max-reconnect-attempts ")"))\n        (swap! pipeline update-in [type :websocket/reconnect-count] (fnil inc 0))\n        (let [conn-config (create-connection-config\n                            type\n                            pipeline\n                            processor-config)\n              new-conn @(ws/websocket (make-elevenlabs-url (:pipeline/config @pipeline) processor-config)\n                                      conn-config)]\n          (swap! pipeline assoc-in [type :websocket/conn] new-conn)\n          (t/log! :debug "Elevenlabs connection ready"))))))\n\n(defn- close-websocket-connection!\n  [type pipeline]\n  (t/log! :info "Closing elevenlabs websocket connection")\n  (when-let  [conn (get-in @pipeline [type :websocket/conn])]\n    (ws/send! conn (u/json-str close-stream-message))\n    (ws/close! conn))\n\n  (swap! pipeline update-in [:tts/elevenlabs] dissoc :websocket/conn))\n\n(def ElevenLabsTTSConfig\n  "Configuration for Elevenlabs TextToSpeech service"\n  [:map\n   {:closed true ;; No additional fields allowed\n    :description "ElevenLabs TTS configuration"}\n   [:elevenlabs/api-key\n    [:string\n     {:min 32      ;; ElevenLabs API keys are typically long\n      :secret true ;; Marks this as sensitive data\n      :description "ElevenLabs API key"}]]\n   [:elevenlabs/model-id\n    (schema/flex-enum\n      {:default "eleven_flash_v2_5"\n       :description "ElevenLabs model identifier"}\n      ["eleven_multilingual_v2" "eleven_turbo_v2_5" "eleven_turbo_v2" "eleven_monolingual_v1" "eleven_multilingual_v1" "eleven_multilingual_sts_v2" "eleven_flash_v2" "eleven_flash_v2_5" "eleven_english_sts_v2"])]\n\n   [:elevenlabs/voice-id\n    [:string\n     {:min 20 ;; ElevenLabs voice IDs are fixed length\n      :max 20\n      :description "ElevenLabs voice identifier"}]]\n   [:voice/stability\n    [:double\n     {:min 0.0\n      :max 1.0\n      :default 0.5\n      :description "Voice stability factor (0.0 to 1.0)"}]]\n   [:voice/similarity-boost\n    [:double\n     {:min 0.0\n      :max 1.0\n      :default 0.8\n      :description "Voice similarity boost factor (0.0 to 1.0)"}]]\n   [:voice/use-speaker-boost?\n    [:boolean\n     {:default true\n      :description "Whether to enable speaker boost enhancement"}]]])\n\n(defmethod pipeline/processor-schema :tts/elevenlabs\n  [_]\n  ElevenLabsTTSConfig)\n\n(defmethod pipeline/process-frame :tts/elevenlabs\n  [type pipeline processor frame]\n  (case (:frame/type frame)\n    :system/start\n    (do (t/log! {:level :debug\n                 :id type} "Starting text to speech engine")\n        (connect-websocket! type pipeline (:processor/config processor)))\n    :system/stop (close-websocket-connection! type pipeline)\n\n    :llm/output-text-sentence\n    (let [conn (get-in @pipeline [type :websocket/conn])\n          xi-message (text-message (:frame/data frame))]\n      (t/log! {:level :debug\n               :id type} ["Sending websocket payload" xi-message])\n      (ws/send! conn xi-message))))\n\n(defmethod pipeline/process-frame :elevenlabs/audio-assembler\n  [type pipeline _ frame]\n  (let [acc (get-in @pipeline [type :audio-accumulator] "")]\n    (case (:frame/type frame)\n      :elevenlabs/audio-chunk\n      (let [attempt (u/parse-if-json (str acc (:frame/data frame)))]\n        (if (map? attempt)\n          (when-let [audio (:audio attempt)]\n            (swap! pipeline assoc-in [type :audio-accumulator] "")\n            (a/put! (:pipeline/main-ch @pipeline)\n                    (f/audio-output-frame audio)))\n          (do\n            (t/log! {:level :debug\n                     :id type} ["Accumulating audio chunk" attempt])\n            (swap! pipeline assoc-in [type :audio-accumulator] attempt))))\n      :system/stop\n      (t/log! {:level :debug\n               :id type} ["Accumulator at the end" acc]))))\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/LICENSE`:\n\n```\nTHE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC\nLICENSE ("AGREEMENT"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM\nCONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n"Contribution" means:\n\na) in the case of the initial Contributor, the initial code and\ndocumentation distributed under this Agreement, and\n\nb) in the case of each subsequent Contributor:\n\ni) changes to the Program, and\n\nii) additions to the Program;\n\nwhere such changes and/or additions to the Program originate from and are\ndistributed by that particular Contributor. A Contribution 'originates' from\na Contributor if it was added to the Program by such Contributor itself or\nanyone acting on such Contributor's behalf. Contributions do not include\nadditions to the Program which: (i) are separate modules of software\ndistributed in conjunction with the Program under their own license\nagreement, and (ii) are not derivative works of the Program.\n\n"Contributor" means any person or entity that distributes the Program.\n\n"Licensed Patents" mean patent claims licensable by a Contributor which are\nnecessarily infringed by the use or sale of its Contribution alone or when\ncombined with the Program.\n\n"Program" means the Contributions distributed in accordance with this\nAgreement.\n\n"Recipient" means anyone who receives the Program under this Agreement,\nincluding all Contributors.\n\n2. GRANT OF RIGHTS\n\na) Subject to the terms of this Agreement, each Contributor hereby grants\nRecipient a non-exclusive, worldwide, royalty-free copyright license to\nreproduce, prepare derivative works of, publicly display, publicly perform,\ndistribute and sublicense the Contribution of such Contributor, if any, and\nsuch derivative works, in source code and object code form.\n\nb) Subject to the terms of this Agreement, each Contributor hereby grants\nRecipient a non-exclusive, worldwide, royalty-free patent license under\nLicensed Patents to make, use, sell, offer to sell, import and otherwise\ntransfer the Contribution of such Contributor, if any, in source code and\nobject code form.  This patent license shall apply to the combination of the\nContribution and the Program if, at the time the Contribution is added by the\nContributor, such addition of the Contribution causes such combination to be\ncovered by the Licensed Patents. The patent license shall not apply to any\nother combinations which include the Contribution. No hardware per se is\nlicensed hereunder.\n\nc) Recipient understands that although each Contributor grants the licenses\nto its Contributions set forth herein, no assurances are provided by any\nContributor that the Program does not infringe the patent or other\nintellectual property rights of any other entity. Each Contributor disclaims\nany liability to Recipient for claims brought by any other entity based on\ninfringement of intellectual property rights or otherwise. As a condition to\nexercising the rights and licenses granted hereunder, each Recipient hereby\nassumes sole responsibility to secure any other intellectual property rights\nneeded, if any. For example, if a third party patent license is required to\nallow Recipient to distribute the Program, it is Recipient's responsibility\nto acquire that license before distributing the Program.\n\nd) Each Contributor represents that to its knowledge it has sufficient\ncopyright rights in its Contribution, if any, to grant the copyright license\nset forth in this Agreement.\n\n3. REQUIREMENTS\n\nA Contributor may choose to distribute the Program in object code form under\nits own license agreement, provided that:\n\na) it complies with the terms and conditions of this Agreement; and\n\nb) its license agreement:\n\ni) effectively disclaims on behalf of all Contributors all warranties and\nconditions, express and implied, including warranties or conditions of title\nand non-infringement, and implied warranties or conditions of merchantability\nand fitness for a particular purpose;\n\nii) effectively excludes on behalf of all Contributors all liability for\ndamages, including direct, indirect, special, incidental and consequential\ndamages, such as lost profits;\n\niii) states that any provisions which differ from this Agreement are offered\nby that Contributor alone and not by any other party; and\n\niv) states that source code for the Program is available from such\nContributor, and informs licensees how to obtain it in a reasonable manner on\nor through a medium customarily used for software exchange.\n\nWhen the Program is made available in source code form:\n\na) it must be made available under this Agreement; and\n\nb) a copy of this Agreement must be included with each copy of the Program.\n\nContributors may not remove or alter any copyright notices contained within\nthe Program.\n\nEach Contributor must identify itself as the originator of its Contribution,\nif any, in a manner that reasonably allows subsequent Recipients to identify\nthe originator of the Contribution.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities with\nrespect to end users, business partners and the like. While this license is\nintended to facilitate the commercial use of the Program, the Contributor who\nincludes the Program in a commercial product offering should do so in a\nmanner which does not create potential liability for other Contributors.\nTherefore, if a Contributor includes the Program in a commercial product\noffering, such Contributor ("Commercial Contributor") hereby agrees to defend\nand indemnify every other Contributor ("Indemnified Contributor") against any\nlosses, damages and costs (collectively "Losses") arising from claims,\nlawsuits and other legal actions brought by a third party against the\nIndemnified Contributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program in\na commercial product offering.  The obligations in this section do not apply\nto any claims or Losses relating to any actual or alleged intellectual\nproperty infringement. In order to qualify, an Indemnified Contributor must:\na) promptly notify the Commercial Contributor in writing of such claim, and\nb) allow the Commercial Contributor to control, and cooperate with the\nCommercial Contributor in, the defense and any related settlement\nnegotiations. The Indemnified Contributor may participate in any such claim\nat its own expense.\n\nFor example, a Contributor might include the Program in a commercial product\noffering, Product X. That Contributor is then a Commercial Contributor. If\nthat Commercial Contributor then makes performance claims, or offers\nwarranties related to Product X, those performance claims and warranties are\nsuch Commercial Contributor's responsibility alone. Under this section, the\nCommercial Contributor would have to defend claims against the other\nContributors related to those performance claims and warranties, and if a\ncourt requires any other Contributor to pay any damages as a result, the\nCommercial Contributor must pay those damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, THE PROGRAM IS PROVIDED ON\nAN "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER\nEXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR\nCONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A\nPARTICULAR PURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all risks\nassociated with its exercise of rights under this Agreement , including but\nnot limited to the risks and costs of program errors, compliance with\napplicable laws, damage to or loss of data, programs or equipment, and\nunavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, NEITHER RECIPIENT NOR ANY\nCONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION\nLOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY\nOF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of the\nremainder of the terms of this Agreement, and without further action by the\nparties hereto, such provision shall be reformed to the minimum extent\nnecessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Program itself\n(excluding combinations of the Program with other software or hardware)\ninfringes such Recipient's patent(s), then such Recipient's rights granted\nunder Section 2(b) shall terminate as of the date such litigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it fails to\ncomply with any of the material terms or conditions of this Agreement and\ndoes not cure such failure in a reasonable period of time after becoming\naware of such noncompliance. If all Recipient's rights under this Agreement\nterminate, Recipient agrees to cease use and distribution of the Program as\nsoon as reasonably practicable. However, Recipient's obligations under this\nAgreement and any licenses granted by Recipient relating to the Program shall\ncontinue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement, but in\norder to avoid inconsistency the Agreement is copyrighted and may only be\nmodified in the following manner. The Agreement Steward reserves the right to\npublish new versions (including revisions) of this Agreement from time to\ntime. No one other than the Agreement Steward has the right to modify this\nAgreement. The Eclipse Foundation is the initial Agreement Steward. The\nEclipse Foundation may assign the responsibility to serve as the Agreement\nSteward to a suitable separate entity. Each new version of the Agreement will\nbe given a distinguishing version number. The Program (including\nContributions) may always be distributed subject to the version of the\nAgreement under which it was received. In addition, after a new version of\nthe Agreement is published, Contributor may elect to distribute the Program\n(including its Contributions) under the new version. Except as expressly\nstated in Sections 2(a) and 2(b) above, Recipient receives no rights or\nlicenses to the intellectual property of any Contributor under this\nAgreement, whether expressly, by implication, estoppel or otherwise. All\nrights in the Program not expressly granted under this Agreement are\nreserved.\n\nThis Agreement is governed by the laws of the State of New York and the\nintellectual property laws of the United States of America. No party to this\nAgreement will bring a legal action under this Agreement more than one year\nafter the cause of action arose. Each party waives its rights to a jury trial\nin any resulting litigation.\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/CHANGELOG.md`:\n\n```md\n# Change Log\nAll notable changes to this project will be documented in this file. This change log follows the conventions of [keepachangelog.com](http://keepachangelog.com/).\n\n## [Unreleased]\n### Changed\n- Add a new arity to `make-widget-async` to provide a different widget shape.\n\n## [0.1.1] - 2024-12-24\n### Changed\n- Documentation on how to make the widgets.\n\n### Removed\n- `make-widget-sync` - we're all async, all the time.\n\n### Fixed\n- Fixed widget maker to keep working when daylight savings switches over.\n\n## 0.1.0 - 2024-12-24\n### Added\n- Files from the new template.\n- Widget maker public API - `make-widget-sync`.\n\n[Unreleased]: https://github.com/ovistoica/voice-fn/compare/0.1.1...HEAD\n[0.1.1]: https://github.com/ovistoica/voice-fn/compare/0.1.0...0.1.1\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/TODO.org`:\n\n```org\n#+TITLE: Tasks to do for voice-fn\n\n* DONE Add schema validation with defaults\nCLOSED: [2025-01-03 Fri 11:01]\n:LOGBOOK:\nCLOCK: [2025-01-03 Fri 07:51]--[2025-01-03 Fri 08:16] =>  0:25\nCLOCK: [2025-01-03 Fri 07:06]--[2025-01-03 Fri 07:31] =>  0:25\nCLOCK: [2025-01-03 Fri 06:35]--[2025-01-03 Fri 07:00] =>  0:25\n:END:\n\n* TODO Add VAD events from deepgram\n:LOGBOOK:\nCLOCK: [2025-01-03 Fri 11:01]\n:END:\n\n* TODO Add support for first message\n\n* TODO Add support for [[https://github.com/fixie-ai/ultravox][ultravox]]\n\n* TODO Add support for google gemini\n\n* TODO Add support for telnyx transport\n\n* TODO Add support for xAI text LLM\n\n* TODO Add support for openai realtime API\n\n* TODO Research webrtc support\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/README.md`:\n\n```md\n# voice-fn - WIP\n\nA Clojure library for building real-time voice-enabled AI applications. voice-fn handles the orchestration of speech recognition, audio processing, and AI service integration with the elegance of functional programming.\n\n## Features\n\n- Real-time speech recognition with configurable backends\n- Clean functional interface for audio stream processing\n- Seamless integration with popular AI models and services\n- Built-in support for conversation state management\n- Efficient audio processing with core.async channels\n- Hot-reloadable pipeline components\n- Extensible architecture with composable transforms\n\n## Why voice-fn?\n\nvoice-fn brings Clojure's functional elegance to voice AI applications. Rather than wrestling with complex state management and imperative audio processing, voice-fn lets you create AI pipelines in a declarative way.\n\n```clojure\n(ns example\n  (:require\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.secrets :refer [secret]]))\n\n(def async-echo-pipeline\n  {:pipeline/config {:audio-in/sample-rate 8000\n                     :audio-in/encoding :ulaw\n                     :audio-in/channels 1\n                     :audio-in/sample-size-bits 8\n                     :audio-out/sample-rate 8000\n                     :audio-out/bitrate 64000\n                     :audio-out/sample-size-bits 8\n                     :audio-out/channels 1\n                     :pipeline/language :ro}\n   :pipeline/processors [;;transport in\n                         {:processor/type :transport/async-input\n                          :processor/accepted-frames #{:system/start :system/stop}\n                          :processor/generates-frames #{:audio/raw-input}}\n                         ;; transcription\n                         {:processor/type :transcription/deepgram\n                          :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                          :processor/generates-frames #{:text/input}\n                          :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                             :transcription/interim-results? false\n                                             :transcription/punctuate? false\n                                             :transcription/model :nova-2}}\n                        ;; text to speech\n                        {:processor/type :tts/openai\n                         :processor/accepted-frames #{:text/input}\n                         :processor/generates-frames #{:audio/output}\n                         :processor/config {:openai/api-key (secret [:openai :api-key])}}\n                        ;; transport out\n                         {:processor/type :transport/async-output\n                          :processor/accepted-frames #{:audio/output :system/stop}\n                          :generates/frames #{}}]})\n\n(def p (pipeline/create-pipeline async-echo-pipeline))\n(pipeline/start-pipeline! p) ;;starting pipeline\n(pipeline/stop-pipeline! p) ;;stopping\n```\n\nTo add a new processor simply tell voice-fn how to run `process-frame` for that new processor. Example of a frame logger for text-input frames\n```clojure\n(ns my-logger-processor\n  (:require\n   [voice-fn.pipeline :as pipeline]))\n\n(defmethod pipeline/process-frame :log/text-input\n  [_ _ _ frame]\n  (t/log! {:level :info\n           :id :log/text-input} ["Frame" (:frame/data frame)]))\n\n\n;; and now you can use it in the pipeline\n(def async-echo-pipeline\n  {:pipeline/config {...}\n   :pipeline/processors [...\n                         {:processor/type :transcription/deepgram\n                          :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                          :processor/generates-frames #{:text/input}\n                          :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                             :transcription/interim-results? false\n                                             :transcription/punctuate? false\n                                             :transcription/model :nova-2}}\n                         {:processor/type :log/text-input\n                          :processor/accepted-frames #{:text/input}\n                          :processor/generates-frames #{}\n                          :processor/config {}}\n                          ...]})\n```\n\n## Status\n\nWIP - Early stage. See examples for what can be achieved for now\n\n## Documentation\n\nWIP - TODO\n\nSee [docs/](docs/) for full documentation, including:\n- Getting Started Guide\n- Architecture Overview\n- API Reference\n- Examples\n- Contributing Guidelines\n\n## License\n\nMIT\n\n---\n\nvoice-fn - Functional voice AI, powered by Clojure.\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/examples/deps.edn`:\n\n```edn\n{:paths ["src"]\n :deps {org.clojure/clojure {:mvn/version "1.12.0"}\n        org.clojure/data.xml {:mvn/version "0.0.8"}\n        ring/ring-jetty-adapter {:mvn/version "1.13.0"}\n        com.shipclojure/voice-fn {:local/root "../core"}\n        metosin/reitit {:mvn/version "0.7.2"}\n        hato/hato {:mvn/version "1.0.0"}}\n :aliases {:cider-clj {:extra-deps {cider/cider-nrepl {:mvn/version "0.50.3"}}\n                       :main-opts ["-m" "nrepl.cmdline" "--middleware" "[cider.nrepl/cider-middleware]"]}}}\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/examples/src/deps.edn`:\n\n```edn\n{:paths ["src" "resources"]\n :deps {org.clojure/clojure {:mvn/version "1.12.0"}\n        com.taoensso/telemere {:mvn/version "1.0.0-RC1"}\n        org.clojure/core.async {:mvn/version "1.7.701"}\n        metosin/jsonista {:mvn/version "0.3.8"}\n        ring/ring-core {:mvn/version "1.13.0"}\n        hato/hato {:mvn/version "1.0.0"}}\n :aliases\n {:build {:deps {io.github.clojure/tools.build {:mvn/version "0.9.4"}\n                 slipset/deps-deploy {:mvn/version "0.2.1"}}\n          :ns-default build}\n  :dev {:extra-deps {djblue/portal {:mvn/version "0.58.5"}}}\n  :cider-clj {:extra-deps {cider/cider-nrepl {:mvn/version "0.50.3"}}\n              :main-opts ["-m" "nrepl.cmdline" "--middleware" "[cider.nrepl/cider-middleware]"]}\n  :test {:extra-paths ["test"]\n         :extra-deps {org.clojure/test.check {:mvn/version "1.1.1"}\n                      midje/midje {:mvn/version "1.10.10"}\n                      io.github.cognitect-labs/test-runner\n                      {:git/tag "v0.5.1" :git/sha "dfb30dd"}}}}}\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/examples/src/voice_fn_examples/twilio_websocket.clj`:\n\n```clj\n(ns voice-fn-examples.twilio-websocket\n  (:require\n   [clojure.core.async :as a]\n   [clojure.data.xml :as xml]\n   [muuntaja.core :as m]\n   [reitit.core]\n   [reitit.dev.pretty :as pretty]\n   [reitit.ring :as ring]\n   [reitit.ring.coercion :as coercion]\n   [reitit.ring.middleware.exception :as exception]\n   [reitit.ring.middleware.muuntaja :as muuntaja]\n   [reitit.ring.middleware.parameters :as parameters]\n   [ring.adapter.jetty :as jetty]\n   [ring.util.response :as r]\n   [ring.websocket :as ws]\n   [taoensso.telemere :as t]\n   [voice-fn.core]\n   [voice-fn.pipeline :as vpipe]\n   [voice-fn.processors.elevenlabs]\n   [voice-fn.processors.llm-context-aggregator]\n   [voice-fn.processors.llm-sentence-assembler]\n   [voice-fn.secrets :refer [secret]]\n   [voice-fn.transport.twilio]))\n\n(defn emit-xml-str\n  [xml-data]\n  (-> xml-data\n      (xml/sexp-as-element)\n      (xml/emit-str)))\n\n(defn header\n  [req header-name]\n  (get (:headers req) header-name))\n\n(defn host\n  [req]\n  (header req "host"))\n\n(defn xml-response\n  [body]\n  (-> (r/response body)\n      (r/content-type "text/xml")))\n\n(defn twilio-inbound-handler\n  "Handler to direct the incoming call to stream audio to our websocket handler"\n  [req]\n  (let [h (host req)\n        ws-url (str "wss://" h "/ws")]\n    ;; https://www.twilio.com/docs/voice/twiml/connect\n    (xml-response\n      (emit-xml-str [:Response\n                     [:Connect\n                      [:Stream {:url ws-url}]]]))))\n\n(defn create-twilio-ai-pipeline\n  [in out]\n  {:pipeline/config {:audio-in/sample-rate 8000\n                     :audio-in/encoding :ulaw\n                     :audio-in/channels 1\n                     :audio-in/sample-size-bits 8\n                     :audio-out/sample-rate 8000\n                     :audio-out/encoding :ulaw\n                     :audio-out/sample-size-bits 8\n                     :audio-out/channels 1\n                     :pipeline/language :ro\n                     :llm/context [{:role "system" :content  "Ești un agent vocal care funcționează prin telefon. Răspunde doar în limba română și fii succint. Inputul pe care îl primești vine dintr-un sistem de speech to text (transcription) care nu este intotdeauna eficient și poate trimite text neclar. Cere clarificări când nu ești sigur pe ce a spus omul."}]\n                     :transport/in-ch in\n                     :transport/out-ch out}\n   :pipeline/processors [{:processor/type :transport/twilio-input\n                          :processor/accepted-frames #{:system/start :system/stop}\n                          :processor/generates-frames #{:audio/raw-input}}\n                         {:processor/type :transcription/deepgram\n                          :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                          :processor/generates-frames #{:text/input}\n                          :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                             :transcription/interim-results? true\n                                             :transcription/punctuate? false\n                                             :transcription/vad-events? true\n                                             :transcription/smart-format? true\n                                             :transcription/model :nova-2}}\n                         {:processor/type :llm/context-aggregator\n                          :processor/accepted-frames #{:llm/output-text-sentence :text/input}\n                          :processor/generates-frames #{:llm/user-context-added}}\n                         {:processor/type :llm/openai\n                          :processor/accepted-frames #{:llm/user-context-added}\n                          :processor/generates-frames #{:llm/output-text-chunk}\n                          :processor/config {:llm/model "gpt-4o-mini"\n                                             :openai/api-key (secret [:openai :new-api-sk])}}\n                         {:processor/type :log/text-input\n                          :processor/accepted-frames #{:text/input}\n                          :processor/generates-frames #{}\n                          :processor/config {}}\n                         {:processor/type :llm/sentence-assembler\n                          :processor/accepted-frames #{:system/stop :llm/output-text-chunk}\n                          :processor/generates-frames #{:llm/output-text-sentence}\n                          :processor/config {:sentence/end-matcher #"[.?!;:]"}}\n                         {:processor/type :tts/elevenlabs\n                          :processor/accepted-frames #{:system/stop :system/start :llm/output-text-sentence}\n                          :processor/generates-frames #{:audio/output :elevenlabs/audio-chunk}\n                          :processor/config {:elevenlabs/api-key (secret [:elevenlabs :api-key])\n                                             :elevenlabs/model-id "eleven_flash_v2_5"\n                                             :elevenlabs/voice-id "7sJPxFeMXAVWZloGIqg2"\n                                             :voice/stability 0.5\n                                             :voice/similarity-boost 0.8\n                                             :voice/use-speaker-boost? true}}\n                         {:processor/type :elevenlabs/audio-assembler\n                          :processor/accepted-frames #{:elevenlabs/audio-chunk}\n                          :processor/generates-frames #{:audio/output}}\n                         {:processor/type :transport/async-output\n                          :processor/accepted-frames #{:audio/output :system/stop}\n                          :generates/frames #{}}]})\n\n;; Using ring websocket protocols to setup a websocket server\n(defn twilio-ws-handler\n  [req]\n  (assert (ws/upgrade-request? req))\n  (let [in (a/chan 1024)\n        out (a/chan 1024)\n        pipeline (vpipe/create-pipeline (create-twilio-ai-pipeline in out))\n        start-pipeline (fn [socket]\n                         ;; listen on the output channel we provided to send\n                         ;; that audio back to twilio\n                         (a/go-loop []\n                           (when-let [output (a/<! out)]\n                             (t/log! {:id :twilio-output\n                                      :level :debug} ["Sending twilio output" output])\n                             (ws/send socket output)\n                             (recur)))\n                         (vpipe/start-pipeline! pipeline))]\n    {::ws/listener\n     {:on-open (fn on-open [socket]\n                 (start-pipeline socket)\n\n                 nil)\n      :on-message (fn on-text [_ws payload]\n                    (a/put! in payload))\n      :on-close (fn on-close [_ws _status-code _reason]\n                  (vpipe/stop-pipeline! pipeline))\n      :on-error (fn on-error [ws error]\n                  (prn error)\n                  (t/log! :debug error))\n      :on-ping (fn on-ping [ws payload]\n                 (ws/send ws payload))}}))\n\n(def routes\n  [["/inbound-call" {:summary "Webhook where a call is made"\n                     :post {:handler twilio-inbound-handler}}]\n   ["/ws" {:summary "Websocket endpoint to receive a twilio call"\n           :get {:handler twilio-ws-handler}}]])\n\n(def app\n  (ring/ring-handler\n    (ring/router\n      routes\n      {:exception pretty/exception\n       :data {:muuntaja m/instance\n              :middleware [;; query-params & form-params\n                           parameters/parameters-middleware\n                           ;; content-negotiation\n                           muuntaja/format-negotiate-middleware\n                           ;; encoding response body\n                           muuntaja/format-response-middleware\n                           ;; exception handling\n                           exception/exception-middleware\n                           ;; decoding request body\n                           muuntaja/format-request-middleware\n                           ;; coercing response bodys\n                           coercion/coerce-response-middleware\n                           ;; coercing request parameters\n                           coercion/coerce-request-middleware]}})\n    (ring/create-default-handler)))\n\n(defn start [& {:keys [port] :or {port 3000}}]\n  (println (str "server running in port " port))\n  (jetty/run-jetty #'app {:port port, :join? false}))\n\n(comment\n\n  (def server (start :port 3000))\n  (.stop server)\n\n  ,)\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/doc/intro.md`:\n\n```md\n# Introduction to voice-fn\n\nTODO: write [great documentation](http://jacobian.org/writing/what-to-write/)\n\n```\n\n`/Users/ovistoica/workspace/voice-fn/architecture.org`:\n\n```org\n:PROPERTIES:\n:GPTEL_MODEL: claude-3-5-sonnet-20241022\n:GPTEL_BACKEND: Claude\n:GPTEL_SYSTEM: You are a Clojure programming expert with deep knowledge of functional programming paradigms, Structure and Interpretation of Computer Programs (SICP), and extensive experience with Clojure's concurrency patterns. Your approach to problem-solving prioritizes data and its transformation, following Rich Hickey's philosophy of "data first, not methods first."\n\nCore Competencies:\n\n1. Functional Programming Expertise\n- You understand and can explain pure functions, immutability, and referential transparency\n- You can demonstrate the benefits of persistent data structures\n- You're well-versed in higher-order functions, function composition, and point-free style\n- You understand the trade-offs between eager and lazy evaluation\n- You can explain and implement functional design patterns\n\n2. SICP Mastery\n- You can explain and implement metacircular evaluators\n- You understand environment model of evaluation\n- You can implement streams and delayed evaluation\n- You're familiar with register machines and compilation\n- You can explain and implement symbolic differentiation\n- You understand and can implement constraint propagation systems\n\n3. Clojure-Specific Knowledge\n- Deep understanding of Clojure's core abstractions: sequences, transducers, protocols\n- Mastery of Clojure's reference types: atoms, refs, agents, vars\n- Expert knowledge of Clojure's concurrent programming models\n- Understanding of Clojure's relationship with the host platform (JVM)\n- Familiarity with ClojureScript and its ecosystem\n\n4. Concurrency Patterns\n- Expert understanding of Software Transactional Memory (STM) using refs\n- Mastery of core.async for CSP-style concurrency\n- Understanding of agent-based concurrency for independent state management\n- Knowledge of Java interop for thread management when necessary\n- Experience with reactive programming patterns\n\n5. Data-First Philosophy\n- You always start by designing the data structure before writing functions\n- You understand and can implement EAV (Entity-Attribute-Value) patterns\n- You're familiar with Datomic and its approach to data management\n- You understand the power of data literals and EDN\n- You can explain and implement data-driven programming patterns\n\nApproach to Problem-Solving:\n\n1. When presented with a problem, you:\n   - First analyze and design the data structures needed\n   - Consider immutability and persistence requirements\n   - Evaluate concurrency needs early in the design process\n   - Think in terms of data transformations rather than objects and methods\n\n2. When reviewing code, you look for:\n   - Proper separation of pure and impure functions\n   - Appropriate use of Clojure's reference types\n   - Efficient use of lazy sequences and transducers\n   - Clear data transformation pipelines\n   - Proper error handling and validation\n\n3. When designing systems, you:\n   - Start with the data model and its evolution over time\n   - Consider the query patterns that will be needed\n   - Plan for concurrent access patterns\n   - Design for composability and reuse through data transformation\n\nBest Practices You Follow:\n\n1. Data Design\n   - Use maps as the primary unit of data\n   - Prefer sets for unique collections\n   - Use vectors for ordered sequences\n   - Use keywords as keys for better performance\n   - Consider spec for data validation\n\n2. Function Design\n   - Write small, focused functions\n   - Use threading macros for clarity\n   - Leverage higher-order functions\n   - Use destructuring for clean parameter handling\n   - Document functions with clear specs\n\n3. Concurrency Handling\n   - Use refs for coordinated state changes\n   - Use atoms for independent state\n   - Use agents for asynchronous updates\n   - Use core.async for complex coordination\n   - Always consider transaction boundaries\n\n4. Error Handling\n   - Use ex-info for structured errors\n   - Leverage spec for validation\n   - Use proper exception handling patterns\n   - Consider retry strategies for concurrent operations\n\nWhen responding to questions:\n1. Always start by examining the data structures involved\n2. Consider concurrency implications early\n3. Suggest the simplest solution that solves the problem\n4. Provide examples using real-world scenarios\n5. Explain the trade-offs of different approaches\n6. Reference relevant sections of SICP when applicable\n7. Share insights from Clojure's core principles\n\nWhen writing code:\n1. Prioritize clarity over cleverness\n2. Use proper formatting and indentation\n3. Include relevant docstrings and comments\n4. Demonstrate idiomatic Clojure patterns\n5. Show test cases when appropriate\n6. Consider performance implications\n7. Document any assumptions made\n\nYou should be able to discuss and implement:\n- Custom data structures using protocols\n- Advanced macro systems\n- Domain-specific languages\n- Clojure's core protocols\n- Integration with Java libraries\n- Performance optimization techniques\n- Testing strategies\n- System architecture patterns\n\nRemember to always approach problems from a data-first perspective, considering the shape and flow of data before implementing functions and processes. Your solutions should embrace Clojure's philosophy of simplicity and power through data transformation.\n:GPTEL_BOUNDS: ((10382 . 11028) (11065 . 11506) (11977 . 12681) (12882 . 13619) (13834 . 14372) (14383 . 14716) (14717 . 14911) (17713 . 18078) (18079 . 19845) (20209 . 21647) (21658 . 24816) (24865 . 25752) (25753 . 28085) (28147 . 28270) (28271 . 31697) (41934 . 45355) (45356 . 45425) (45486 . 48142) (48143 . 48564) (48796 . 49027) (49028 . 50961) (51339 . 51494) (51495 . 54150) (54151 . 54609))\n:END:\n\n*** Help me think of a good architecture for a project similar to pipecat but written in the clojure ecosystem.\n\nA Clojure framework for building real-time voice-enabled AI applications. voice-fn handles the orchestration of speech recognition, audio processing, and AI service integration with the elegance of functional programming.\n\n## Features\n\n- Real-time speech recognition with configurable backends\n- Clean functional interface for audio stream processing\n- Seamless integration with popular AI models and services\n- Built-in support for conversation state management\n- Efficient audio processing with core.async channels\n- Hot-reloadable pipeline components\n- Extensible architecture with composable transforms\n\n\nMy problems:\n- I tried to replicate the same system from pipecat but it doens't map well over clojure way of doing things since it puts things in a very OOP manner.\n- I want to use core.async and I was thinking that all processors have an in and a out. They take a frame from in, call =process-frame= on it and immediately or at a later time, puts its resulting frames onto the out channel.\n- The way pipecat did it with classes for processors and transports works well but it doesn't feel very clojure with data orientation.\n\n  I welcome any ideas you have. Here is some of my ideas in Clojure so far (I am not very happy with them since I tried to port the OOP model to protocols and records and doesn't hold so well since you need maker functions to simulate async work done in a constructor):\n\n  (ns voice-fn.processors.protocols)\n\n(defprotocol FrameProcessor\n  "A frame processor is one of the pieces from the pipeline. Examples:\n  - `TranscriptionProcessor`: processes `RawAudioInputFrames` and outputs `TranscriptionOutputFrames`\n  - `TextLLMProcessor`: processes `TranscriptionOutputFrames` and outputs `LLMOutputTokenFrames`"\n  (process-frame [this frame] "Process a frame of a given type. May output 0, 1 or multiple frames in the pipeline"))\n\n(defprotocol ProcessorLifecycle\n  (start! [this] "Start the processor. Optionally push a start frame further in the pipeline")\n  (stop! [this] "Stop the processor. Optionally push a stop frame further in the pipeline."))\n\n\n  (ns voice-fn.processors.deepgram\n  (:require\n   [hato.websocket :as ws]\n   [taoensso.telemere :as t]\n   [voice-fn.frames :as frames]\n   [voice-fn.processors.protocols :refer [FrameProcessor ProcessorLifecycle stop!]]\n   [voice-fn.utils.core :as u])\n  (:import\n   (java.nio HeapCharBuffer)))\n\n(def ^:private deepgram-url "wss://api.deepgram.com/v1/listen")\n\n(def deepgram-config {:encoding "linear16"\n                      :language "ro"\n                      :sample_rate 16000\n                      :interim_results false\n                      :punctuate true\n                      :model "nova-2"})\n\n(def deepgram-url-encoding\n  (u/append-search-params deepgram-url deepgram-config))\n\n(defn transcript?\n  [m]\n  (= (:event m) "transcript"))\n\n(defn- transcript\n  [m]\n  (-> m :channel :alternatives first :transcript))\n\n(defn final-transcription?\n  [m]\n  (and (transcript? m)\n       (= (:type m) "final")))\n\n(defn- make-websocket-config [api-key]\n  {:headers {"Authorization" (str "Token " api-key)}\n   :on-open (fn [_]\n              (t/log! :info "Deepgram websocket connection open"))\n   :on-message (fn [_ws ^HeapCharBuffer data _last?]\n                 (let [m (u/parse-if-json (str data))\n                       trsc (transcript m)]\n                   (when (and (final-transcription? m)\n                              (not= trsc ""))\n                     (t/log! :info ["Transcript" trsc]))))\n   :on-error (fn [_ e]\n               (t/log! :error ["Error" e]))\n   :on-close (fn [_ code reason]\n               (t/log! :info ["Deepgram websocket connection closed" "Code:" code "Reason" reason]))})\n\n(defn close-connection-payload\n  []\n  (u/json-str {:type "CloseStream"}))\n\n(defrecord DeepgramTranscriptionProcessor [conn]\n  FrameProcessor\n  (process-frame [this frame]\n    (when (frames/raw-audio-input-frame? frame)\n      (ws/send! (:raw frame) false)))\n  ProcessorLifecycle\n  (start! [this]\n    (t/log! :info "Deepgram transcription started"))\n  (stop! [this]\n    (t/log! :info "Closing Deepgram connection")\n    (ws/send! conn (close-connection-payload))\n    (ws/close! conn)))\n\n(defn make-deepgram-transcription-processor\n  [api-key]\n  (let [conn (ws/websocket deepgram-url-encoding (make-websocket-config api-key))]\n    (->DeepgramTranscriptionProcessor conn)))\n\n(comment\n\n  (def processor (make-deepgram-transcription-processor api-key))\n  (stop! processor))\n\n\n\nLet me guide you towards a more Clojure-centric design. Let's start with some high-level hints:\n\n1. Think about what would happen if you modeled your frames as plain data (maps) instead of types/classes. What benefits might this give you?\n\n2. Consider how core.async's transducers could relate to your frame processing pipeline. How might this align better with Clojure's data transformation philosophy?\n\n3. Instead of protocols and records for processors, what if you thought about them as pure functions that transform data? What would that look like?\n\nWould you like to explore any of these directions further? Which aspect interests you most?\n\n*** Let's explore the model first\n\nSure! Let's think about modeling frames as data. Here's a hint to get you started:\n\nIn Clojure, we often use maps with a =:type= key to distinguish between different kinds of data. How might you represent the different frame types you saw in pipecat (AudioRawFrame, TextFrame, etc.) using just maps?\n\nTry sketching out a few example frames as pure data structures. Don't worry about the processing yet - let's nail down the data model first.\n\n*** The initial dummy way to do it is have a map like this:\n{:stt {:provider :elevenlabs\n       :config {...}}\n :llm {:provider :llama\n       :config}\n ...\n }\n\n However this model is too static. I want users to be able to put on the pipeline whatever they want. For example, maybe between the TTS and the STT models I want an extra logger that sends telemetry data with the conversation to storage. The correct model is an array with the processors specified in order\n\nGood thinking! You're moving in the right direction with the array of processors.\n\nHere's a hint to push this further: In Clojure, we often represent "configuration as data" where the configuration itself can describe both structure and behavior.\n\nThink about how you might represent a pipeline like this:\n#+begin_src clojure\n[{:type :stt\n  :provider :deepgram\n  ...}\n {:type :telemetry\n  :store-fn ...}\n {:type :llm\n  :provider :openai\n  ...}]\n#+end_src\n\nWhat if each processor spec in this vector could also include information about:\n- What kinds of frames it can accept\n- What kinds of frames it produces\n- Any side effects it might have\n\nHow might that help with pipeline validation and composition?\n\n*** I don't think we need to specify the type as that doesn't serve any need. Also the pipeline should maintain state. For example, deepgram transcription processor maintains a websocket connection\n\nAh, now we're getting to an interesting design question! Let me offer a hint:\n\nIn Clojure, we often separate the configuration/specification of something from its runtime instance. Consider this pattern:\n\n#+begin_src clojure\n;; The specification (pure data)\n{:provider :deepgram\n :api-key "..."\n :config {...}}\n\n;; The runtime instance (stateful)\n{:chan-in (chan)\n :chan-out (chan)\n :conn (atom nil)  ; websocket connection\n :state (atom {})} ; other state\n#+end_src\n\nThink about:\n1. How might you transform the static specification into a running instance?\n2. Where would the state management code live if not in records/protocols?\n3. How could core.async channels fit into this picture?\n\nWould you like to explore any of these aspects?\n\n*** Let's explore the data oriented specification for a simple pipeline:\n1. Capture local audio from microphone\n2. Send those to a transcription engine (deepgram)\n3. Log all of the transcriptions while they come\n\nGood example to start with! Let's think about how to specify this pipeline in a data-oriented way.\n\nHere's a hint: Consider what minimal information you need to specify each step, keeping in mind that we want to separate the "what" from the "how".\n\nTry writing out a data structure that describes this pipeline. Think about:\n- How to specify the audio source configuration\n- How to connect the components\n- What configuration each component needs\n- How to specify where the logs should go\n\nWant to take a first pass at this specification?\n\n*** Yes\n\nOkay, let's start sketching. Here's a hint for the structure:\n\nThink about each component in terms of:\n1. What it needs to initialize\n2. What resources it needs access to\n3. What configuration parameters it requires\n\nTry writing a data structure that describes your pipeline. You might start with something like:\n\n#+begin_src clojure\n[{:source :microphone\n  ...}\n {:transcribe :deepgram\n  ...}\n {:log ...}]\n#+end_src\n\nFill in what you think each component needs. Don't worry about getting it perfect - we can refine it together.\n\n*** No, it should be something like this:\n#+begin_src clojure\n(defn- make-websocket-config [api-key out-chan]\n  {:headers {"Authorization" (str "Token " api-key)}\n   :on-open (fn [_]\n              (t/log! :info "Deepgram websocket connection open"))\n   :on-message (fn [_ws ^HeapCharBuffer data _last?]\n                 (let [m (u/parse-if-json (str data))\n                       trsc (transcript m)]\n                   (when (and (final-transcription? m)\n                              (not= trsc ""))\n                     (a/go (a/>! out-chan (frames/text-input-frame)))))\n   :on-error (fn [_ e]\n               (t/log! :error ["Error" e]))\n   :on-close (fn [_ code reason]\n               (t/log! :info ["Deepgram websocket connection closed" "Code:" code "Reason" reason]))})\n\n(defn create-deepgram-transcription-websocket [state config]\n  (ws/websocket deepgram-url-encoding (make-websocket-config (:api-key config) (:main-ch @state))))\n\n(defn get-connection! [state config]\n  (if-let [conn (get-in [:transcription/deepgram :conn] @state)]\n    conn\n    (swap! assoc-in [:transcription/deepgram :conn] (create-deepgram-transcription-websocket state config))))\n\n[{:accepted-frames #{:system/start :system/stop :system/config} ;; doesn't need to interrupt on user speech\n  :generated-frames #{:audio/raw-input}\n  :process-frame-fn (fn [state config frame]\n                      (let [running? (atom false)]\n                        (condp type frame\n                          :system/start (start-capture-audio (:audio-config @state))\n                          :system/stop (stop-capture-audio)\n                          )))\n  ;; meta is used to access processor specific state\n  :type :transport/local-audio\n  :direction :input}\n {:accepted-frames #{:system/start :system/stop :system/config :audio/raw-input} ;; also doesn't need to interrupt\n  :generated-frames #{:text/input}\n  :api-key "..."\n  :process-frame-fn\n  (fn [state config frame]\n    (condp type frame\n      :system/start (noop)\n      :system/stop (let [conn (get-in [:transcription/deepgram :conn] @state)]\n                     (close-connection! conn))\n      ;; deepgram websocket times out if we don't send data fast, so we create\n      ;; the connection only when we have some actual data to process\n      :audio/raw-input (let [conn (get-connection! state config)]\n                         (ws/send! conn (:raw frame)))\n\n      )\n    )\n  :type :transcription/deepgram\n  }\n {:accepted-frames #{:system/start :system/stop :system/config :text/input} ;; also doesn't need to interrupt\n  :generated-frames #{}\n  :process-frame-fn\n  (fn [state config frame]\n    (condp type frame\n      :system/start (noop)\n      :system/stop (noop)\n      :text/input (t/log! :info (:text frame))\n\n      )\n    )\n  :type :log/text-input\n  }\n ]\n\n#+end_src\n\n\nAh, now I see where you're going! This is a good start - you're thinking about frames as data flowing through the system and each processor declaring what it can accept and produce. Let me help refine this model.\n\nA few suggestions to make this more idiomatic Clojure:\n\n1. Consider separating the static configuration from the processing logic:\n\n#+begin_src clojure\n\n(def processors\n  {:transport/local-audio\n   {:accepted-frames #{:system/start :system/stop :system/config}\n    :generated-frames #{:audio/raw-input}\n    :direction :input}\n\n   :transcription/deepgram\n   {:accepted-frames #{:system/start :system/stop :system/config :audio/raw-input}\n    :generated-frames #{:text/input}}\n\n   :log/text-input\n   {:accepted-frames #{:system/start :system/stop :system/config :text/input}\n    :generated-frames #{}}})\n\n(defmulti process-frame\n  (fn [processor-type state config frame]\n    processor-type))\n\n(defmethod process-frame :transport/local-audio\n  [_ state config frame]\n  (case (:type frame)\n    :system/start (start-capture-audio (:audio-config @state))\n    :system/stop (stop-capture-audio)\n    ;; ... etc\n    ))\n#+end_src\n\n2. For pipeline construction, you could have:\n\n#+begin_src clojure\n(defn make-pipeline [processors-config]\n  (let [state (atom {})\n        channels (create-channels processors-config)]\n    (doseq [processor processors-config]\n      (start-processor! processor state channels))\n    {:state state\n     :channels channels}))\n\n;; Usage:\n(def pipeline-config\n  [{:type :transport/local-audio\n    :config {...}}\n   {:type :transcription/deepgram\n    :config {:api-key "..."}}\n   {:type :log/text-input}])\n#+end_src\n\n3. For handling the channels:\n\n#+begin_src clojure\n(defn start-processor! [{:keys [type config]} state channels]\n  (let [in-chan (get-in channels [type :in])\n        out-chan (get-in channels [type :out])]\n    (go-loop []\n      (when-let [frame (<! in-chan)]\n        (when-let [result (process-frame type state config frame)]\n          (>! out-chan result))\n        (recur)))))\n#+end_src\n\nWhat do you think about these refinements? Would you like to explore any particular aspect further?\n\n*** It's interesting for sure. I was thinking that it might be better to have one main frame channel where all frames are put and every processor subscribes to the types of frames they care about. Your example in start-processor!, suggests that all processors have an in and an out which doesn't align with this idea. What do you think? We can use mult and tap\n\nAh, that's a great observation! Using a single shared channel with =mult= and =tap= is indeed more elegant and flexible. It better matches the publish-subscribe pattern and eliminates the need to manually wire up channels between processors.\n\nHere's how we might reshape this:\n\n#+begin_src clojure\n(defn create-pipeline [processors-config]\n  (let [main-ch (chan 1024)\n        main-mult (mult main-ch)\n        state (atom {:main-ch main-ch\n                    :main-mult main-mult})]\n\n    ;; Start each processor\n    (doseq [{:keys [type accepted-frames] :as processor} processors-config]\n      (let [processor-ch (chan 1024)\n            ;; Tap into main channel, filtering for accepted frame types\n            _ (tap main-mult processor-ch (comp accepted-frames :type))]\n\n        ;; Start processor loop\n        (go-loop []\n          (when-let [frame (<! processor-ch)]\n            (when-let [result (process-frame type state processor frame)]\n              ;; Put results back on main channel\n              (>! main-ch result))\n            (recur)))))\n\n    state))\n#+end_src\n\nThis approach has several benefits:\n1. Processors don't need to know about each other\n2. New processors can be added dynamically by just tapping into the main channel\n3. Frame routing is handled declaratively via the =accepted-frames= set\n4. We get automatic filtering of irrelevant frames\n\nWould you like to explore how this might work with a concrete example?\n\n*** Yes\n\nLet's implement your original example (microphone → transcription → logging) using this pattern:\n\n#+begin_src clojure\n(ns voice-fn.core\n  (:require [clojure.core.async :as a :refer [chan mult tap go-loop >! <!]]))\n\n;; Frame constructors (pure functions)\n(defn audio-frame [raw-data]\n  {:type :audio/raw-input\n   :data raw-data\n   :ts (System/currentTimeMillis)})\n\n(defn text-frame [text]\n  {:type :text/input\n   :text text\n   :ts (System/currentTimeMillis)})\n\n;; Processor implementations\n(defmethod process-frame :transport/local-audio\n  [_ state {:keys [sample-rate channels] :as config} frame]\n  (case (:type frame)\n    :system/start\n    (let [audio-stream (start-audio-capture! sample-rate channels)\n          stop-fn (fn [] (stop-audio-capture! audio-stream))]\n      ;; Store stop-fn in state for cleanup\n      (swap! state assoc-in [:transport/local-audio :stop-fn] stop-fn)\n      ;; Start sending audio frames\n      (go-loop []\n        (when-let [data (<! audio-stream)]\n          (>! (:main-ch @state) (audio-frame data))\n          (recur))))\n\n    :system/stop\n    (when-let [stop-fn (get-in @state [:transport/local-audio :stop-fn])]\n      (stop-fn))))\n\n(defmethod process-frame :transcription/deepgram\n  [_ state {:keys [api-key] :as config} frame]\n  (case (:type frame)\n    :system/start\n    (let [ws-conn (create-deepgram-websocket!\n                   api-key\n                   {:on-transcription\n                    (fn [text]\n                      (a/put! (:main-ch @state) (text-frame text)))})]\n      (swap! state assoc-in [:transcription/deepgram :conn] ws-conn))\n\n    :system/stop\n    (when-let [conn (get-in @state [:transcription/deepgram :conn])]\n      (close-connection! conn))\n\n    :audio/raw-input\n    (when-let [conn (get-in @state [:transcription/deepgram :conn])]\n      (send-audio! conn (:data frame)))))\n\n(defmethod process-frame :log/text-input\n  [_ state config frame]\n  (case (:type frame)\n    :text/input\n    (println "Transcription:" (:text frame))))\n\n;; Pipeline configuration\n(def pipeline-config\n  [{:type :transport/local-audio\n    :accepted-frames #{:system/start :system/stop}\n    :config {:sample-rate 16000\n            :channels 1}}\n\n   {:type :transcription/deepgram\n    :accepted-frames #{:system/start :system/stop :audio/raw-input}\n    :config {:api-key "your-api-key-here"}}\n\n   {:type :log/text-input\n    :accepted-frames #{:text/input}\n    :config {}}])\n\n;; Usage\n(defn start-pipeline! []\n  (let [pipeline (create-pipeline pipeline-config)\n        main-ch (:main-ch @pipeline)]\n    ;; Send start signal to all processors\n    (a/put! main-ch {:type :system/start})\n    pipeline))\n\n(defn stop-pipeline! [pipeline]\n  (a/put! (:main-ch @pipeline) {:type :system/stop}))\n\n(comment\n  ;; Start the pipeline\n  (def p (start-pipeline!))\n\n  ;; Stop it after some time\n  (stop-pipeline! p))\n#+end_src\n\nKey points about this design:\n\n1. Single shared channel (=main-ch=) that all frames flow through\n2. Each processor declares what frame types it accepts via =accepted-frames=\n3. State is managed in one place but partitioned by processor type\n4. Clean separation between configuration and runtime behavior\n5. Easy to ad\n\n*** How would you split this into namespaces?\n\nHere's a suggested namespace organization that follows Clojure's best practices for separation of concerns:\n\n#+begin_src clojure\nvoice-fn\n├── src\n│   └── voice_fn\n│       ├── core.clj           ;; Main API namespace\n│       ├── frames.clj         ;; Frame definitions and constructors\n│       ├── pipeline.clj       ;; Pipeline creation and management\n│       ├── protocols.clj      ;; Any protocols we might need\n│       ├── specs.clj          ;; Specs for validation\n│       ├── processors\n│       │   ├── audio.clj      ;; Audio capture processor\n│       │   ├── transcription.clj ;; Transcription processors\n│       │   └── logging.clj    ;; Logging processors\n│       └── utils\n│           ├── async.clj      ;; core.async utilities\n│           └── websocket.clj  ;; WebSocket utilities\n#+end_src\n\nHere's how the code might be organized across these namespaces:\n\n#+begin_src clojure\n\n;; src/voice_fn/frames.clj\n(ns voice-fn.frames)\n\n(defn audio-frame [raw-data]\n  {:type :audio/raw-input\n   :data raw-data\n   :ts (System/currentTimeMillis)})\n\n(defn text-frame [text]\n  {:type :text/input\n   :text text\n   :ts (System/currentTimeMillis)})\n\n;; src/voice_fn/pipeline.clj\n(ns voice-fn.pipeline\n  (:require [clojure.core.async :as a]))\n\n(defn create-pipeline [processors-config]\n  ;; Pipeline creation logic here\n  )\n\n(defmulti process-frame\n  (fn [processor-type state config frame]\n    processor-type))\n\n;; src/voice_fn/processors/audio.clj\n(ns voice-fn.processors.audio\n  (:require [voice-fn.pipeline :refer [process-frame]]\n            [voice-fn.frames :as frames]))\n\n(defmethod process-frame :transport/local-audio\n  [_ state config frame]\n  ;; Audio processor implementation\n  )\n\n;; src/voice_fn/processors/transcription.clj\n(ns voice-fn.processors.transcription\n  (:require [voice-fn.pipeline :refer [process-frame]]\n            [voice-fn.frames :as frames]\n            [voice-fn.utils.websocket :as ws]))\n\n(defmethod process-frame :transcription/deepgram\n  [_ state config frame]\n  ;; Transcription processor implementation\n  )\n\n;; src/voice_fn/core.clj\n(ns voice-fn.core\n  (:require [voice-fn.pipeline :as pipeline]\n            [voice-fn.processors.audio]    ;; Load multimethod implementations\n            [voice-fn.processors.transcription]\n            [voice-fn.processors.logging]))\n\n(def default-pipeline-config\n  [{:type :transport/local-audio\n    :accepted-frames #{:system/start :system/stop}\n    :config {:sample-rate 16000\n            :channels 1}}\n   ;; ... other processor configs\n   ])\n\n(defn start!\n  ([] (start! default-pipeline-config))\n  ([config]\n   (let [pipeline (pipeline/create-pipeline config)]\n     (a/put! (:main-ch @pipeline) {:type :system/start})\n     pipeline)))\n\n(defn stop! [pipeline]\n  (a/put! (:main-ch @pipeline) {:type :system/stop}))\n#+end_src\n\nSome key points about this organization:\n\n1. =core.clj= provides the main public API\n2. Each processor type gets its own namespace\n3. Frame definitions are centralized in =frames.clj=\n4. Pipeline logic is isolated in =pipeline.clj=\n5. Utilities are separated into their own namespaces\n6. Processor implementations are loaded by requiring their namespaces\n\nWould you like me to expand on any particular aspect of this organization?\n\n*** Can you also write a function for start-audio-capture!\n\nHere's an implementation of audio capture using the Java Sound API, wrapped in a Clojure-friendly way:\n\n#+begin_src clojure\n\n;; src/voice_fn/processors/audio/capture.clj\n(ns voice-fn.processors.audio.capture\n  (:require [clojure.core.async :as a])\n  (:import [javax.sound.sampled\n            AudioSystem\n            AudioFormat\n            DataLine$Info\n            TargetDataLine]))\n\n(defn create-audio-format\n  "Creates an AudioFormat with given sample rate and channels.\n   Default to 16-bit PCM encoding."\n  [sample-rate channels]\n  (AudioFormat. sample-rate                  ; sample rate\n                16                           ; sample size in bits\n                channels                     ; channels\n                true                         ; signed\n                true))                       ; big endian\n(defn open-microphone!\n  "Opens the microphone with specified format. Returns the TargetDataLine."\n  [^AudioFormat format]\n  (let [info (DataLine$Info. TargetDataLine format)\n        line (AudioSystem/getLine info)]\n    (when-not (.isLineSupported info)\n      (throw (ex-info "Audio line not supported"\n                     {:format format})))\n    (.open ^TargetDataLine line format)\n    (.start ^TargetDataLine line)\n    line))\n\n(defn start-audio-capture!\n  "Starts capturing audio from the microphone.\n   Returns a channel that will receive byte arrays of audio data.\n\n   Options:\n   :sample-rate - The sample rate in Hz (default: 16000)\n   :channels - Number of audio channels (default: 1)\n   :buffer-size - Size of the buffer in bytes (default: 4096)\n   :chan-buf-size - Size of the core.async channel buffer (default: 1024)"\n  ([] (start-audio-capture! {}))\n  ([{:keys [sample-rate channels buffer-size chan-buf-size]\n     :or {sample-rate 16000\n          channels 1\n          buffer-size 4096\n          chan-buf-size 1024}}]\n   (let [format (create-audio-format sample-rate channels)\n         line (open-microphone! format)\n         out-ch (a/chan chan-buf-size)\n         buffer (byte-array buffer-size)\n         running? (atom true)]\n\n     ;; Start capture loop in a separate thread\n     (future\n       (try\n         (while @running?\n           (let [bytes-read (.read ^TargetDataLine line\n                                  buffer\n                                  0\n                                  buffer-size)]\n             (when (pos? bytes-read)\n               ;; Copy only the bytes that were read\n               (let [audio-data (Arrays/copyOfRange buffer 0 bytes-read)]\n                 ;; Put data on channel, but don't block if channel is full\n                 (a/offer! out-ch audio-data)))))\n         (catch Exception e\n           (a/put! out-ch {:error e}))\n         (finally\n           (.stop ^TargetDataLine line)\n           (.close ^TargetDataLine line)\n           (a/close! out-ch))))\n\n     ;; Return a map with the channel and a stop function\n     {:audio-chan out-ch\n      :stop! #(reset! running? false)})))\n\n;; Usage example:\n(comment\n  ;; Start capture\n  (def capture (start-audio-capture! {:sample-rate 16000\n                                     :channels 1}))\n\n  ;; Process audio data\n  (a/go-loop []\n    (when-let [data (a/<! (:audio-chan capture))]\n      (println "Got" (count data) "bytes")\n      (recur)))\n\n  ;; Stop capture\n  ((:stop! capture)))\n#+end_src\n\nAnd here's how to integrate it with your processor:\n\n#+begin_src clojure\n;; src/voice_fn/processors/audio.clj\n(ns voice-fn.processors.audio\n  (:require [voice-fn.pipeline :refer [process-frame]]\n            [voice-fn.frames :as\n\n*** I've implemented my first demo of this and it works quite well. Here's how it is structurd:\n\n(ns voice-fn.core\n  (:require\n   [taoensso.telemere :as t]\n   [voice-fn.pipeline :as pipeline]\n   [voice-fn.secrets :refer [secret]]\n   [voice-fn.transport.local.audio]))\n\n(def pipeline\n  {:pipeline/config {:audio-in/sample-rate 16000\n                     :audio-in/encoding :pcm-signed\n                     :audio-in/channels 1\n                     :audio-in/file-path "test-voice.wav"\n                     :audio-in/sample-size-bits 16 ;; 2 bytes\n                     :audio-out/sample-rate 24000\n                     :audio-out/bitrate 96000\n                     :audio-out/sample-size-bits 16\n                     :audio-out/channels 1\n                     :pipeline/language :ro}\n   :pipeline/processors [{:processor/type :transport/local-audio\n                          :processor/accepted-frames #{:system/start :system/stop}\n                          :processor/generates-frames #{:audio/raw-input}}\n                         {:processor/type :transcription/deepgram\n                          :processor/accepted-frames #{:system/start :system/stop :audio/raw-input}\n                          :processor/generates-frames #{:text/input}\n                          :processor/config {:transcription/api-key (secret [:deepgram :api-key])\n                                             :transcription/interim-results? false\n                                             :transcription/punctuate? false\n                                             :transcription/model :nova-2}}\n                         {:processor/type :log/text-input\n                          :processor/accepted-frames #{:text/input}\n                          :processor/config {}}]})\n\n(defmethod pipeline/process-frame :log/text-input\n  [_ _ _ frame]\n  (t/log! {:level :info\n           :id :log/text-input} ["Frame" (:data frame)]))\n\n(t/set-min-level! :debug)\n\n(comment\n  (def p (pipeline/create-pipeline pipeline))\n\n  (:pipeline/processors @p)\n\n  (pipeline/start-pipeline! p)\n  (pipeline/stop-pipeline! p)\n\n  ,)\n\n\n  (ns voice-fn.transport.local.audio\n  (:require\n   [clojure.core.async :as a]\n   [clojure.java.io :as io]\n   [taoensso.telemere :as t]\n   [uncomplicate.clojure-sound.core :refer [open! read! start!]]\n   [uncomplicate.clojure-sound.sampled :refer [audio-format line line-info]]\n   [voice-fn.frames :as frames]\n   [voice-fn.pipeline :refer [close-processor! process-frame]])\n  (:import\n   (java.util Arrays)\n   (javax.sound.sampled AudioFormat AudioSystem DataLine$Info TargetDataLine)))\n\n(defn- calculate-chunk-size\n  "Calculate bytes for ms miliseconds of audio based on format"\n  [ms ^AudioFormat audio-format]\n  (let [frame-size (.getFrameSize audio-format)\n        frame-rate (.getFrameRate audio-format)]\n    (* frame-size (int (/ (* frame-rate ms) 1000)))))\n\n(defn start-audio-capture-file!\n  "Reads from WAV file in 20ms chunks.\n   Returns a channel that will receive byte arrays of audio data."\n  [{:audio-in/keys [file-path]\n    :or {file-path "input.wav"}}]\n  (let [audio-file (io/resource file-path)\n        audio-stream (AudioSystem/getAudioInputStream audio-file)\n        audio-format (.getFormat audio-stream)\n        chunk-size (calculate-chunk-size 20 audio-format)\n        buffer (byte-array chunk-size)\n        out-ch (a/chan 1024)\n        running? (atom true)]\n\n    (future\n      (try\n        (while (and @running?\n                    (pos? (.read audio-stream buffer 0 chunk-size)))\n          (let [audio-data (Arrays/copyOf buffer chunk-size)]\n            (a/offer! out-ch audio-data)))\n        (catch Exception e\n          (a/put! out-ch {:error e}))\n        (finally\n          (.close audio-stream)\n          (a/close! out-ch))))\n\n    {:audio-chan out-ch\n     :stop-fn #(do (reset! running? false)\n                   (a/close! out-ch))}))\n\n(defn line-supported?\n  [^DataLine$Info info]\n  (AudioSystem/isLineSupported info))\n\n(defn open-microphone!\n  "Opens the microphone with specified format. Returns the TargetDataLine."\n  [^AudioFormat format]\n  (let [info (line-info :target format)\n        line (line info)]\n    (when-not (line-supported? info)\n      (throw (ex-info "Audio line not supported"\n                      {:format format})))\n    (open! line format)\n    (start! line)\n    line))\n\n(defn- frame-buffer-size\n  "Get read buffer size based on the sample rate for input"\n  [sample-rate]\n  (* 2 (/ sample-rate 100)))\n\n(defn start-audio-capture!\n  "Starts capturing audio from the microphone.\n   Returns a channel that will receive byte arrays of audio data.\n\n   Options:\n   :sample-rate - The sample rate in Hz (default: 16000)\n   :channels - Number of audio channels (default: 1)"\n  ([] (start-audio-capture! {}))\n  ([{:audio-in/keys [sample-rate sample-size-bits channels]\n     :or {sample-rate 16000\n          channels 1\n          sample-size-bits 16}}]\n   (let [buffer-size (frame-buffer-size sample-rate)\n         af (audio-format sample-rate sample-size-bits channels)\n         line (open-microphone! af)\n         out-ch (a/chan 1024)\n         buffer (byte-array buffer-size)\n         running? (atom true)]\n\n     ;; Start capture loop in a separate thread\n     (future\n       (try\n         (while @running?\n           (let [bytes-read (read! line buffer 0 buffer-size)]\n             (when (pos? bytes-read)\n               ;; Copy only the bytes that were read\n               (let [audio-data (Arrays/copyOfRange buffer 0 bytes-read)]\n                 ;; Put data on channel, but don't block if channel is full\n                 (a/offer! out-ch audio-data)))))\n         (catch Exception e\n           (a/put! out-ch {:error e}))\n         (finally\n           (.stop ^TargetDataLine line)\n           (.close ^TargetDataLine line)\n           (a/close! out-ch))))\n\n     ;; Return a map with the channel and a stop function\n     {:audio-chan out-ch\n      :stop-fn #(do (a/close! out-ch)\n                    (reset! running? false))})))\n\n(defmethod process-frame :transport/local-audio\n  [processor-type pipeline _ frame]\n  (case (:frame/type frame)\n    :system/start\n    (do\n      (t/log! :debug "Starting audio capture")\n      (let [{:keys [audio-chan stop-fn]} (start-audio-capture-file! (:pipeline/config @pipeline))]\n        ;; Store stop-fn in state for cleanup\n        (swap! pipeline assoc-in [:transport/local-audio :stop-fn] stop-fn)\n        ;; Start sending audio frames\n        (a/go-loop []\n          (when-let [data (a/<! audio-chan)]\n            (a/>! (:pipeline/main-ch @pipeline) (frames/audio-input-frame data))\n            (recur)))))\n\n    :system/stop\n    (do\n      (t/log! :debug "Stopping audio capture")\n      (when-let [stop-fn (get-in @pipeline [:transport/local-audio :stop-fn])]\n        (stop-fn)))\n    (close-processor! pipeline processor-type))\n  nil)\n\n\n  Here's my next issue:\n\n  Other types of transports, like telephony or webrtc, involves opening an API endpoint like a webhook or a websocket to receive the stream of audio for input. How can we set up this during the pipeline? What are your suggestions? Here is how pipecat does it:\n\n  import os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom loguru import logger\n\nfrom pipecat.audio.vad.silero import SileroVADAnalyzer\nfrom pipecat.frames.frames import EndFrame, LLMMessagesFrame\nfrom pipecat.pipeline.pipeline import Pipeline\nfrom pipecat.pipeline.runner import PipelineRunner\nfrom pipecat.pipeline.task import PipelineParams, PipelineTask\nfrom pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\nfrom pipecat.serializers.twilio import TwilioFrameSerializer\nfrom pipecat.services.cartesia import CartesiaTTSService\nfrom pipecat.services.deepgram import DeepgramSTTService\nfrom pipecat.services.openai import OpenAILLMService\nfrom pipecat.transports.network.fastapi_websocket import (\n    FastAPIWebsocketParams,\n    FastAPIWebsocketTransport,\n)\n\nload_dotenv(override=True)\n\nlogger.remove(0)\nlogger.add(sys.stderr, level="DEBUG")\n\n\nasync def run_bot(websocket_client, stream_sid):\n    transport = FastAPIWebsocketTransport(\n        websocket=websocket_client,\n        params=FastAPIWebsocketParams(\n            audio_out_enabled=True,\n            add_wav_header=False,\n            vad_enabled=True,\n            vad_analyzer=SileroVADAnalyzer(),\n            vad_audio_passthrough=True,\n            serializer=TwilioFrameSerializer(stream_sid),\n        ),\n    )\n\n    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"), model="gpt-4o")\n\n    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))\n\n    tts = CartesiaTTSService(\n        api_key=os.getenv("CARTESIA_API_KEY"),\n        voice_id="79a125e8-cd45-4c13-8a67-188112f4dd22",  # British Lady\n    )\n\n    messages = [\n        {\n            "role": "system",\n            "content": "You are a helpful LLM in an audio call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",\n        },\n    ]\n\n    context = OpenAILLMContext(messages)\n    context_aggregator = llm.create_context_aggregator(context)\n\n    pipeline = Pipeline(\n        [\n            transport.input(),  # Websocket input from client\n            stt,  # Speech-To-Text\n            context_aggregator.user(),\n            llm,  # LLM\n            tts,  # Text-To-Speech\n            transport.output(),  # Websocket output to client\n            context_aggregator.assistant(),\n        ]\n    )\n\n    task = PipelineTask(pipeline, params=PipelineParams(allow_interruptions=True))\n\n    @transport.event_handler("on_client_connected")\n    async def on_client_connected(transport, client):\n        # Kick off the conversation.\n        messages.append({"role": "system", "content": "Please introduce yourself to the user."})\n        await task.queue_frames([LLMMessagesFrame(messages)])\n\n    @transport.event_handler("on_client_disconnected")\n    async def on_client_disconnected(transport, client):\n        await task.queue_frames([EndFrame()])\n\n    runner = PipelineRunner(handle_sigint=False)\n\n    await runner.run(task)\n\n\n    So I can see here that they instantiate the websocket connection for input transport beforehand\n\n\n\nFor handling WebSocket/HTTP endpoints in your Clojure pipeline, I'd suggest using Ring/HTTP Kit or Aleph for the server component. Here's how we could structure this:\n\n#+begin_src clojure\n;; src/voice_fn/transport/websocket.clj\n(ns voice-fn.transport.websocket\n  (:require\n    [org.httpkit.server :as http]\n    [compojure.core :refer [GET defroutes]]\n    [voice-fn.pipeline :refer [process-frame]]\n    [clojure.core.async :as a]))\n\n(defn websocket-handler [pipeline req]\n  (http/with-channel req channel\n    (let [client-id (random-uuid)]\n      ;; Store WS connection in pipeline state\n      (swap! pipeline assoc-in [:transport/websocket :connections client-id] channel)\n\n      ;; Handle incoming messages\n      (http/on-receive channel\n        (fn [data]\n          (a/put! (:pipeline/main-ch @pipeline)\n                  (frames/audio-input-frame data))))\n\n      ;; Handle client disconnect\n      (http/on-close channel\n        (fn [status]\n          (swap! pipeline update-in [:transport/websocket :connections] dissoc client-id))))))\n\n(defroutes websocket-routes\n  (GET "/ws" [] #(websocket-handler pipeline %)))\n\n(defmethod process-frame :transport/websocket\n  [processor-type pipeline config frame]\n  (case (:frame/type frame)\n    :system/start\n    (let [port (get-in config [:processor/config :port] 8080)\n          server (http/run-server websocket-routes {:port port})]\n      (swap! pipeline assoc-in [:transport/websocket :server] server))\n\n    :system/stop\n    (when-let [server (get-in @pipeline [:transport/websocket :server])]\n      (server)) ; Stop the server\n\n    :audio/output\n    (doseq [[_ conn] (get-in @pipeline [:transport/websocket :connections])]\n      (http/send! conn (:data frame)))))\n\n;; Usage in pipeline config:\n(def pipeline\n  {:pipeline/config {...}\n   :pipeline/processors\n   [{:processor/type :transport/websocket\n     :processor/accepted-frames #{:system/start :system/stop :audio/output}\n     :processor/generates-frames #{:audio/raw-input}\n     :processor/config {:port 8080}}\n    ...]})\n#+end_src\n\nKey points:\n\n1. The WebSocket transport is started when it receives =:system/start=\n2. It maintains a map of active connections in the pipeline state\n3. Incoming messages are put onto the main channel as audio frames\n4. Outgoing audio can be sent to all connected clients\n5. The server is cleanly shut down on =:system/stop=\n\nFor telephony, you might have something similar:\n\n#+begin_src clojure\n;; src/voice_fn/transport/telephony.clj\n(ns voice-fn.transport.telephony\n  (:require\n    [ring.adapter.jetty :as jetty]\n    [compojure.core :refer [POST defroutes]]\n    [voice-fn.pipeline :refer [process-frame]]))\n\n(defn twilio-webhook-handler [pipeline req]\n  ;; Handle incoming Twilio webhook\n  (let [audio-data (-> req :body :audio)]\n    (a/put! (:pipeline/main-ch @pipeline)\n            (frames/audio-input-frame audio-data))))\n\n(defroutes telephony-routes\n  (POST "/twilio/webhook" [] #(twilio-webhook-handler pipeline %)))\n\n(defmethod process-frame :transport/telephony\n  [processor-type pipeline config frame]\n  (case (:frame/type frame)\n    :system/start\n    (let [port (get-in config [:processor/config :port] 8081)\n          server (jetty/run-jetty telephony-routes {:port port :join? false})]\n      (swap! pipeline assoc-in [:transport/telephony :server] server))\n\n    :system/stop\n    (when-let [server (get-in @pipeline [:transport/telephony :server])]\n      (.stop server))))\n#+end_src\n\nWould you like me to elaborate on any part of this design?\n\nI would like to change to use reitit instead of compojure\n\nHere's how we can restructure the WebSocket transport using reitit:\n\n#+begin_src clojure\n;; src/voice_fn/transport/websocket.clj\n(ns voice-fn.transport.websocket\n  (:require\n   [org.httpkit.server :as http]\n   [reitit.ring :as ring]\n   [reitit.ring.middleware.muuntaja :as muuntaja]\n   [reitit.swagger :as swagger]\n   [reitit.swagger-ui :as swagger-ui]\n   [muuntaja.core :as m]\n   [voice-fn.pipeline :refer [process-frame]]\n   [clojure.core.async :as a]))\n\n(defn websocket-handler [pipeline]\n  (fn [req]\n    (http/with-channel req channel\n      (let [client-id (random-uuid)]\n        ;; Store WS connection in pipeline state\n        (swap! pipeline assoc-in [:transport/websocket :connections client-id] channel)\n\n        ;; Handle incoming messages\n        (http/on-receive channel\n          (fn [data]\n            (a/put! (:pipeline/main-ch @pipeline)\n                    (frames/audio-input-frame data))))\n\n        ;; Handle client disconnect\n        (http/on-close channel\n          (fn [status]\n            (swap! pipeline update-in [:transport/websocket :connections] dissoc client-id)))))))\n\n(defn app-routes [pipeline]\n  ["/api"\n   ["/swagger.json"\n    {:get {:no-doc true\n           :handler (swagger/create-swagger-handler)}}]\n\n   ["/ws"\n    {:get {:handler (websocket-handler pipeline)\n           :summary "WebSocket endpoint for audio streaming"}}]])\n\n(def app-middleware\n  [muuntaja/format-middleware])\n\n(defn create-app [pipeline]\n  (ring/ring-handler\n   (ring/router\n    [(app-routes pipeline)]\n    {:data {:muuntaja m/instance\n            :middleware app-middleware}})\n   (ring/routes\n    (swagger-ui/create-swagger-ui-handler\n     {:path "/api/swagger"})\n    (ring/create-default-handler))))\n\n(defmethod process-frame :transport/websocket\n  [processor-type pipeline config frame]\n  (case (:frame/type frame)\n    :system/start\n    (let [port (get-in config [:processor/config :port] 8080)\n          app (create-app pipeline)\n          server (http/run-server app {:port port})]\n      (swap! pipeline assoc-in [:transport/websocket :server] server))\n\n    :system/stop\n    (when-let [server (get-in @pipeline [:transport/websocket :server])]\n      (server)) ; Stop the server\n\n    :audio/output\n    (doseq [[_ conn] (get-in @pipeline [:transport/websocket :connections])]\n      (http/send! conn (:data frame)))))\n\n;; Usage in pipeline config remains the same:\n(def pipeline\n  {:pipeline/config {...}\n   :pipeline/processors\n   [{:processor/type :transport/websocket\n     :processor/accepted-frames #{:system/start :system/stop :audio/output}\n     :processor/generates-frames #{:audio/raw-input}\n     :processor/config {:port 8080}}\n    ...]})\n#+end_src\n\nKey improvements with reitit:\n1. Better route organization with data-driven routing\n2. Built-in support for OpenAPI/Swagger documentation\n3. Better middleware handling\n4. More extensible routing system\n\nWould you like me to:\n1. Add more endpoints (like health checks)?\n2. Show how to add authentication middleware?\n3. Add proper error handling?\n4. Show how to structure the telephony routes in the same system?\n\n*** This is fine but don't you agree that this adds too much to the model? I think it might be better to just create the websocket connection outside the pipeline config and just give that as configuration to the transport input\n\nAbsolutely! You make a great point. This would better follow the principle of separation of concerns and keep the pipeline focused on processing frames rather than managing servers. Here's the refined approach:\n\n#+begin_src clojure\n\n;; src/voice_fn/server.clj\n(ns voice-fn.server\n  (:require\n   [org.httpkit.server :as http]\n   [reitit.ring :as ring]\n   [voice-fn.pipeline :as pipeline]))\n\n(defn websocket-handler [pipeline-atom]\n  (fn [req]\n    (http/with-channel req channel\n      (let [client-id (random-uuid)]\n        (swap! pipeline-atom assoc-in [:transport/websocket :connections client-id] channel)\n        (http/on-receive channel\n          (fn [data]\n            (pipeline/send-frame! pipeline-atom (frames/audio-input-frame data))))\n        (http/on-close channel\n          (fn [status]\n            (swap! pipeline-atom update-in [:transport/websocket :connections] dissoc client-id)))))))\n\n;; Server setup happens outside pipeline\n(defn start-server! [pipeline-atom port]\n  (let [app (ring/ring-handler\n             (ring/router\n              [["/ws" {:get {:handler (websocket-handler pipeline-atom)}}]]))\n        server (http/run-server app {:port port})]\n    server))\n\n;; The pipeline processor becomes much simpler\n(defmethod pipeline/process-frame :transport/websocket\n  [_ pipeline _ frame]\n  (case (:frame/type frame)\n    :audio/output\n    (doseq [[_ conn] (get-in @pipeline [:transport/websocket :connections])]\n      (http/send! conn (:data frame)))\n    nil))\n\n;; Usage:\n(comment\n  ;; Start server separately\n  (def server (start-server! pipeline-atom 8080))\n\n  ;; Pipeline config focuses only on processing\n  (def pipeline-config\n   {:pipeline/processors\n    [{:processor/type :transport/websocket\n      :processor/accepted-frames #{:audio/output}\n      :processor/generates-frames #{:audio/raw-input}}\n     ...]}))\n#+end_src\n\nThis is much cleaner because:\n1. Server lifecycle is managed separately from the pipeline\n2. Pipeline only handles frame processing\n3. Clearer separation between web concerns and pipeline concerns\n4. More flexible - can have multiple transport inputs feeding the same pipeline\n\nWhat do you think about this approach?\n\n*** Here's an interesting idea from the pipecat project. They have a constant websocket transport that receives a websocket connection as config for the transport and a serializer/deserializer. This is interesting because the websocket transport works the same for different providers, it is just that serializing and deserializing frames for the pipeline will be different.\n\nThat's an excellent pattern! It separates the transport mechanism from the protocol-specific details. Let's implement this in Clojure:\n\n#+begin_src clojure\n\n;; src/voice_fn/transport/websocket/protocols.clj\n(ns voice-fn.transport.websocket.protocols)\n\n(defprotocol FrameSerializer\n  (serialize-frame [this frame])\n  (deserialize-frame [this raw-data]))\n\n;; src/voice_fn/transport/websocket/serializers.clj\n(ns voice-fn.transport.websocket.serializers\n  (:require [voice-fn.transport.websocket.protocols :as p]\n            [voice-fn.frames :as frames]))\n\n;; Example Twilio serializer\n(defrecord TwilioSerializer [stream-sid]\n  p/FrameSerializer\n  (serialize-frame [_ frame]\n    ;; Convert pipeline frame to Twilio-specific format\n    (case (:frame/type frame)\n      :audio/output {:event "media"\n                     :streamSid stream-sid\n                     :media {:payload (:data frame)}}\n      ;; ... other frame types\n      ))\n\n  (deserialize-frame [_ raw-data]\n    ;; Convert Twilio message to pipeline frame\n    (let [data (json/parse-string raw-data true)]\n      (case (:event data)\n        "media" (frames/audio-input-frame (:payload (:media data)))\n        ;; ... other event types\n        ))))\n\n;; src/voice_fn/transport/websocket/transport.clj\n(ns voice-fn.transport.websocket.transport\n  (:require [voice-fn.pipeline :as pipeline]\n            [clojure.core.async :as a]))\n\n(defmethod pipeline/process-frame :transport/websocket\n  [_ pipeline {:keys [websocket serializer]} frame]\n  (case (:frame/type frame)\n    :audio/output\n    (let [serialized (p/serialize-frame serializer frame)]\n      (http/send! websocket serialized))\n\n    :audio/raw-input\n    (when-let [data (:data frame)]\n      (let [deserialized (p/deserialize-frame serializer data)]\n        (a/put! (:pipeline/main-ch @pipeline) deserialized)))\n\n    nil))\n\n;; Usage example:\n(comment\n  ;; In your server setup\n  (defn websocket-handler [pipeline-atom]\n    (fn [req]\n      (http/with-channel req channel\n        (let [stream-sid (get-in req [:params :stream-sid])\n              serializer (->TwilioSerializer stream-sid)]\n          ;; Add websocket connection and its serializer to pipeline config\n          (swap! pipeline-atom update :pipeline/processors conj\n                 {:processor/type :transport/websocket\n                  :processor/config {:websocket channel\n                                   :serializer serializer}\n                  :processor/accepted-frames #{:audio/output}\n                  :processor/generates-frames #{:audio/raw-input}})))))\n\n  ;; Different serializer for different provider\n  (defrecord WebRTCSerializer []\n    p/FrameSerializer\n    (serialize-frame [_ frame]\n      ;; WebRTC-specific serialization\n      )\n    (deserialize-frame [_ raw-data]\n      ;; WebRTC-specific deserialization\n      )))\n\n#+end_src\n\nKey benefits of this approach:\n1. Transport logic is protocol-agnostic\n2. Easy to add new providers by implementing new serializers\n3. Clean separation between transport mechanism and protocol details\n4. Testable - can test serializers independently\n5. Flexible - can swap serializers at runtime\n\nWould you like me to:\n1. Add more example serializers?\n2. Show how to handle different types of frames?\n3. Add error handling for malformed messages?\n\n***\n\n```
:GPTEL_BOUNDS: ((221401 . 221627) (221628 . 222137) (222138 . 222619) (222620 . 223980) (224723 . 227817) (227961 . 231557) (231805 . 232052) (232053 . 234212))
:END:

*** Here's the flow of what happens in the python pipecat project once the user interupts the bot speaking. I want you to propose a code solution that is similar for voice-fn. I care more that we discuss a strategy and architecture, not actual code. We will generate code later.

** Conversation start

Step by step tell me what happens when the user starts speaking over the AI. I am interested in the process of interrupting the AI and what each processor does and what frames get sent. Here is the example:
#
# Copyright (c) 2024, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import os
import sys

import aiohttp
from deepgram import LiveOptions
from dotenv import load_dotenv
from loguru import logger
from runner import configure

from pipecat.frames.frames import (
    BotInterruptionFrame,
    LLMMessagesFrame,
    StopInterruptionFrame,
    UserStartedSpeakingFrame,
    UserStoppedSpeakingFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.deepgram import DeepgramSTTService, DeepgramTTSService
from pipecat.services.openai import OpenAILLMService
from pipecat.transports.services.daily import DailyParams, DailyTransport

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


async def main():
    async with aiohttp.ClientSession() as session:
        (room_url, _) = await configure(session)

        transport = DailyTransport(
            room_url,
            None,
            "Respond bot",
            DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
            ),
        )

        stt = DeepgramSTTService(
            api_key=os.getenv("DEEPGRAM_API_KEY"),
            live_options=LiveOptions(vad_events=True, utterance_end_ms="1000"),
        )

        tts = DeepgramTTSService(api_key=os.getenv("DEEPGRAM_API_KEY"), voice="aura-helios-en")

        llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"), model="gpt-4o")

        messages = [
            {
                "role": "system",
                "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
            },
        ]

        context = OpenAILLMContext(messages)
        context_aggregator = llm.create_context_aggregator(context)

        pipeline = Pipeline(
            [
                transport.input(),  # Transport user input
                stt,  # STT
                context_aggregator.user(),  # User responses
                llm,  # LLM
                tts,  # TTS
                transport.output(),  # Transport bot output
                context_aggregator.assistant(),  # Assistant spoken responses
            ]
        )

        task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))

        @stt.event_handler("on_speech_started")
        async def on_speech_started(stt, *args, **kwargs):
            await task.queue_frames([BotInterruptionFrame(), UserStartedSpeakingFrame()])

        @stt.event_handler("on_utterance_end")
        async def on_utterance_end(stt, *args, **kwargs):
            await task.queue_frames([StopInterruptionFrame(), UserStoppedSpeakingFrame()])

        @transport.event_handler("on_first_participant_joined")
        async def on_first_participant_joined(transport, participant):
            # Kick off the conversation.
            messages.append({"role": "system", "content": "Please introduce yourself to the user."})
            await task.queue_frames([LLMMessagesFrame(messages)])

        runner = PipelineRunner()

        await runner.run(task)


if __name__ == "__main__":
    asyncio.run(main())


Okay, let's break down what happens in this =pipecat= pipeline when a user starts speaking while the AI is responding. I'll focus on the flow of frames and how each processor reacts.

*Initial State*

1.  The =DailyTransport= is active, capturing audio from the user.
2.  The =DeepgramSTTService= is connected to Deepgram's servers, ready to transcribe incoming audio.
3.  The =OpenAILLMService= is initialized with the GPT-4o model.
4.  The =DeepgramTTSService= is ready to convert text into audio using the =aura-helios-en= voice.
5.  The =OpenAILLMContextAggregator= is initialized and starts empty.
6. The pipeline has been created, and is running within a =PipelineTask=.
7.  The bot has already started speaking (or is about to start) because the =on_first_participant_joined= event is triggered. The =LLMMessagesFrame= is sent downstream, triggering the LLM to generate a response, and the TTS to generate audio.

*The User Interrupts*

Here's what happens when the user starts speaking, step by step:

1.  *Audio Input:* The =DailyTransport= captures the user's voice as =InputAudioRawFrame=s. These frames are pushed into the pipeline's input queue.

2.  *VAD in STT Service:* The =DeepgramSTTService= (which has =vad_events=True=) receives these =InputAudioRawFrame=s. Deepgram's VAD detects the start of speech. This is important because the =DeepgramSTTService= is configured to emit =speech_started= and =utterance_end= events.

3.  *=on_speech_started= Event:* The =on_speech_started= event handler in the =DeepgramSTTService= is triggered (because the Deepgram API sent an event through the websocket). This event handler does the following:
    *   It calls =task.queue_frames([BotInterruptionFrame(), UserStartedSpeakingFrame()])=.
    *   A =BotInterruptionFrame= and a =UserStartedSpeakingFrame= are sent to the pipeline. Note that =UserStartedSpeakingFrame= and =BotInterruptionFrame= are =SystemFrame=s and skip all input queues, and are processed immediately by each processor.

4.  *Immediate System Frame Handling:*
    *  *=BaseOutputTransport=*: The =DailyTransport='s output component (which subclasses =BaseOutputTransport=), receives =BotInterruptionFrame=. It immediately stops sending the audio to the output channel. This stops the TTS from playing audio. It will also stop buffering any audio frames (which are stored in the transport) when receiving the next audio chunk from the TTS.
    *   *=FrameProcessor=s*: Every other processor also receives the =BotInterruptionFrame= and =UserStartedSpeakingFrame= immediately (skipping input queues because it's a system frame). They will each execute their =process_frame= method with this frame, and the handling depends on each processor:
        *   =OpenAILLMContextAggregator=: The context aggregator will pause buffering new messages until a =StopInterruptionFrame= is received. This makes sure that only the latest context after the interruption is sent to the LLM.
         *   =OpenAILLMService=: The LLM service will receive the =UserStartedSpeakingFrame= and will cancel any outstanding requests.
        *  =DeepgramTTSService=: The TTS service will stop any pending TTS operations.
         *   =DeepgramSTTService=: The STT service will continue processing audio. The event handler is called /before/ the audio processing.
    *   *VAD state:* The VAD state is updated to indicate the user is now speaking.

5.  *STT Processing:* The =DeepgramSTTService= continues to receive =InputAudioRawFrame=s and transcribes the user's speech, sending =TranscriptionFrame=s (or =InterimTranscriptionFrame=s) down the pipeline.

6.  *Context Aggregation (User):* The =OpenAILLMContextAggregator= (user side) receives =TranscriptionFrame=s. It buffers these transcriptions to update the conversation context, but they are not pushed to the LLM yet because the interruption is in effect.

7.  *LLM is Idle:* The =OpenAILLMService= is now idle, as the previous request was canceled, and it's waiting for new messages.

8.  *TTS is Idle:* The =DeepgramTTSService= is idle and will not produce any audio until a =TTSSpeakFrame= is received.

*User Stops Speaking*

1.  *VAD in STT Service:* The =DeepgramSTTService='s VAD detects the end of the user's speech based on a period of silence.

2.  *=on_utterance_end= Event:* The =on_utterance_end= event handler in the =DeepgramSTTService= is triggered. This handler calls =task.queue_frames([StopInterruptionFrame(), UserStoppedSpeakingFrame()])=. A =StopInterruptionFrame= and =UserStoppedSpeakingFrame= are sent into the pipeline as system frames.

3.  *Immediate System Frame Handling:*
    *   *=FrameProcessor=s*: Each =FrameProcessor= receives =StopInterruptionFrame= and =UserStoppedSpeakingFrame= immediately (skipping input queues).
        * =OpenAILLMContextAggregator=: The gated context aggregator will push the buffered messages to the LLM.
        * =OpenAILLMService=: The LLM service is now able to receive a new request (and will because there are new messages in the context.)
        * =BaseOutputTransport=: The =DailyTransport='s output will start to process audio frames again when they become available (but it will wait for more frames).
       *  =DeepgramSTTService=: The STT service will continue processing audio as normal.

4.  *New LLM Request:* The =OpenAILLMService= receives the updated context from the context aggregator. It generates a new response. The response is received as =TextFrame=s (and optionally =LLMFullResponseStartFrame= and =LLMFullResponseEndFrame=), which are then passed down the pipeline.

5.  *TTS Processing:* The =DeepgramTTSService= converts the LLM's =TextFrame= output into =OutputAudioRawFrame=s.

6.  *Output:* The =DailyTransport='s output component receives the =OutputAudioRawFrame=s and sends them to the output audio channel.

*Key Takeaways*

-   *System Frames:* =StartInterruptionFrame=, =StopInterruptionFrame=, =UserStartedSpeakingFrame=, =UserStoppedSpeakingFrame= and =BotInterruptionFrame= are critical for real-time interruption handling. These frames are processed immediately by every component, bypassing queues. This avoids any delay in the cancellation.
-   *Context Aggregation:* The =OpenAILLMContextAggregator= plays a crucial role in managing the conversation state. It pauses and resumes buffering of user and bot messages around interruptions, making sure that the LLM receives all the necessary context for continuing the conversation.
-   *Event Handling:* The =DeepgramSTTService= uses events (=on_speech_started= and =on_utterance_end=) to trigger interruption and resume processing logic. This allows for a fast, flexible approach to real-time interruption.
-   *Output Transport Control:* The =DailyTransport='s output component immediately stops the audio output when a =BotInterruptionFrame= is received.

Let me know if you have any other questions about this!


*** How is the pipecat pipeline informed when the user has heart the playback from the audio generated by the TTS service?

Here is the daily transport:
#
# Copyright (c) 2024, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import time
import warnings
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Any, Awaitable, Callable, Mapping, Optional

import aiohttp
from daily import (
    CallClient,
    Daily,
    EventHandler,
    VirtualCameraDevice,
    VirtualMicrophoneDevice,
    VirtualSpeakerDevice,
)
from loguru import logger
from pydantic import BaseModel, model_validator

from pipecat.audio.vad.vad_analyzer import VADAnalyzer, VADParams
from pipecat.frames.frames import (
    CancelFrame,
    EndFrame,
    Frame,
    InputAudioRawFrame,
    InterimTranscriptionFrame,
    OutputAudioRawFrame,
    OutputImageRawFrame,
    SpriteFrame,
    StartFrame,
    TranscriptionFrame,
    TransportMessageFrame,
    TransportMessageUrgentFrame,
    UserImageRawFrame,
    UserImageRequestFrame,
)
from pipecat.processors.frame_processor import FrameDirection
from pipecat.transcriptions.language import Language
from pipecat.transports.base_input import BaseInputTransport
from pipecat.transports.base_output import BaseOutputTransport
from pipecat.transports.base_transport import BaseTransport, TransportParams

try:
    from daily import CallClient, Daily, EventHandler
except ModuleNotFoundError as e:
    logger.error(f"Exception: {e}")
    logger.error(
        "In order to use the Daily transport, you need to `pip install pipecat-ai[daily]`."
    )
    raise Exception(f"Missing module: {e}")

VAD_RESET_PERIOD_MS = 2000


@dataclass
class DailyTransportMessageFrame(TransportMessageFrame):
    participant_id: str | None = None


@dataclass
class DailyTransportMessageUrgentFrame(TransportMessageUrgentFrame):
    participant_id: str | None = None


class WebRTCVADAnalyzer(VADAnalyzer):
    def __init__(self, *, sample_rate=16000, num_channels=1, params: VADParams = VADParams()):
        super().__init__(sample_rate=sample_rate, num_channels=num_channels, params=params)

        self._webrtc_vad = Daily.create_native_vad(
            reset_period_ms=VAD_RESET_PERIOD_MS, sample_rate=sample_rate, channels=num_channels
        )
        logger.debug("Loaded native WebRTC VAD")

    def num_frames_required(self) -> int:
        return int(self.sample_rate / 100.0)

    def voice_confidence(self, buffer) -> float:
        confidence = 0
        if len(buffer) > 0:
            confidence = self._webrtc_vad.analyze_frames(buffer)
        return confidence


class DailyDialinSettings(BaseModel):
    call_id: str = ""
    call_domain: str = ""


class DailyTranscriptionSettings(BaseModel):
    language: str = "en"
    tier: Optional[str] = None
    model: str = "nova-2-general"
    profanity_filter: bool = True
    redact: bool = False
    endpointing: bool = True
    punctuate: bool = True
    includeRawResponse: bool = True
    extra: Mapping[str, Any] = {"interim_results": True}

    @model_validator(mode="before")
    def check_deprecated_fields(cls, values):
        with warnings.catch_warnings():
            warnings.simplefilter("always")
            if "tier" in values:
                warnings.warn(
                    "Field 'tier' is deprecated, use 'model' instead.", DeprecationWarning
                )
        return values


class DailyParams(TransportParams):
    api_url: str = "https://api.daily.co/v1"
    api_key: str = ""
    dialin_settings: Optional[DailyDialinSettings] = None
    transcription_enabled: bool = False
    transcription_settings: DailyTranscriptionSettings = DailyTranscriptionSettings()


class DailyCallbacks(BaseModel):
    on_joined: Callable[[Mapping[str, Any]], Awaitable[None]]
    on_left: Callable[[], Awaitable[None]]
    on_error: Callable[[str], Awaitable[None]]
    on_app_message: Callable[[Any, str], Awaitable[None]]
    on_call_state_updated: Callable[[str], Awaitable[None]]
    on_dialin_connected: Callable[[Any], Awaitable[None]]
    on_dialin_ready: Callable[[str], Awaitable[None]]
    on_dialin_stopped: Callable[[Any], Awaitable[None]]
    on_dialin_error: Callable[[Any], Awaitable[None]]
    on_dialin_warning: Callable[[Any], Awaitable[None]]
    on_dialout_answered: Callable[[Any], Awaitable[None]]
    on_dialout_connected: Callable[[Any], Awaitable[None]]
    on_dialout_stopped: Callable[[Any], Awaitable[None]]
    on_dialout_error: Callable[[Any], Awaitable[None]]
    on_dialout_warning: Callable[[Any], Awaitable[None]]
    on_first_participant_joined: Callable[[Mapping[str, Any]], Awaitable[None]]
    on_participant_joined: Callable[[Mapping[str, Any]], Awaitable[None]]
    on_participant_left: Callable[[Mapping[str, Any], str], Awaitable[None]]
    on_participant_updated: Callable[[Mapping[str, Any]], Awaitable[None]]
    on_transcription_message: Callable[[Mapping[str, Any]], Awaitable[None]]
    on_recording_started: Callable[[Mapping[str, Any]], Awaitable[None]]
    on_recording_stopped: Callable[[str], Awaitable[None]]
    on_recording_error: Callable[[str, str], Awaitable[None]]


def completion_callback(future):
    def _callback(*args):
        def set_result(future, *args):
            try:
                if len(args) > 1:
                    future.set_result(args)
                else:
                    future.set_result(*args)
            except asyncio.InvalidStateError:
                pass

        future.get_loop().call_soon_threadsafe(set_result, future, *args)

    return _callback


class DailyTransportClient(EventHandler):
    _daily_initialized: bool = False

    # This is necessary to override EventHandler's __new__ method.
    def __new__(cls, *args, **kwargs):
        return super().__new__(cls)

    def __init__(
        self,
        room_url: str,
        token: str | None,
        bot_name: str,
        params: DailyParams,
        callbacks: DailyCallbacks,
        loop: asyncio.AbstractEventLoop,
    ):
        super().__init__()

        if not DailyTransportClient._daily_initialized:
            DailyTransportClient._daily_initialized = True
            Daily.init()

        self._room_url: str = room_url
        self._token: str | None = token
        self._bot_name: str = bot_name
        self._params: DailyParams = params
        self._callbacks = callbacks
        self._loop = loop

        self._participant_id: str = ""
        self._video_renderers = {}
        self._transcription_ids = []
        self._transcription_status = None
        self._other_participant_has_joined = False

        self._joined = False
        self._leave_counter = 0

        self._executor = ThreadPoolExecutor(max_workers=5)

        self._client: CallClient = CallClient(event_handler=self)

        # We use a separate task to execute the callbacks, otherwise if we call
        # a `CallClient` function and wait for its completion this will
        # currently result in a deadlock. This is because `_call_async_callback`
        # can be used inside `CallClient` event handlers which are holding the
        # GIL in `daily-python`. So if the `callback` passed here makes a
        # `CallClient` call and waits for it to finish using completions (and a
        # future) we will deadlock because completions use event handlers (which
        # are holding the GIL).
        self._callback_queue = asyncio.Queue()
        self._callback_task = self._loop.create_task(self._callback_task_handler())

        self._camera: VirtualCameraDevice | None = None
        if self._params.camera_out_enabled:
            self._camera = Daily.create_camera_device(
                self._camera_name(),
                width=self._params.camera_out_width,
                height=self._params.camera_out_height,
                color_format=self._params.camera_out_color_format,
            )

        self._mic: VirtualMicrophoneDevice | None = None
        if self._params.audio_out_enabled:
            self._mic = Daily.create_microphone_device(
                self._mic_name(),
                sample_rate=self._params.audio_out_sample_rate,
                channels=self._params.audio_out_channels,
                non_blocking=True,
            )

        self._speaker: VirtualSpeakerDevice | None = None
        if self._params.audio_in_enabled or self._params.vad_enabled:
            self._speaker = Daily.create_speaker_device(
                self._speaker_name(),
                sample_rate=self._params.audio_in_sample_rate,
                channels=self._params.audio_in_channels,
                non_blocking=True,
            )
            Daily.select_speaker_device(self._speaker_name())

    def _camera_name(self):
        return f"camera-{self}"

    def _mic_name(self):
        return f"mic-{self}"

    def _speaker_name(self):
        return f"speaker-{self}"

    @property
    def participant_id(self) -> str:
        return self._participant_id

    def set_callbacks(self, callbacks: DailyCallbacks):
        self._callbacks = callbacks

    async def send_message(self, frame: TransportMessageFrame | TransportMessageUrgentFrame):
        if not self._joined:
            return

        participant_id = None
        if isinstance(frame, (DailyTransportMessageFrame, DailyTransportMessageUrgentFrame)):
            participant_id = frame.participant_id

        future = self._loop.create_future()
        self._client.send_app_message(
            frame.message, participant_id, completion=completion_callback(future)
        )
        await future

    async def read_next_audio_frame(self) -> InputAudioRawFrame | None:
        if not self._speaker:
            return None

        sample_rate = self._params.audio_in_sample_rate
        num_channels = self._params.audio_in_channels
        num_frames = int(sample_rate / 100) * 2  # 20ms of audio

        future = self._loop.create_future()
        self._speaker.read_frames(num_frames, completion=completion_callback(future))
        audio = await future

        if len(audio) > 0:
            return InputAudioRawFrame(
                audio=audio, sample_rate=sample_rate, num_channels=num_channels
            )
        else:
            # If we don't read any audio it could be there's no participant
            # connected. daily-python will return immediately if that's the
            # case, so let's sleep for a little bit (i.e. busy wait).
            await asyncio.sleep(0.01)
            return None

    async def write_raw_audio_frames(self, frames: bytes):
        if not self._mic:
            return None

        future = self._loop.create_future()
        self._mic.write_frames(frames, completion=completion_callback(future))
        await future

    async def write_frame_to_camera(self, frame: OutputImageRawFrame):
        if not self._camera:
            return None

        self._camera.write_frame(frame.image)

    async def join(self):
        # Transport already joined, ignore.
        if self._joined:
            # Increment leave counter if we already joined.
            self._leave_counter += 1
            return

        logger.info(f"Joining {self._room_url}")

        # For performance reasons, never subscribe to video streams (unless a
        # video renderer is registered).
        self._client.update_subscription_profiles(
            {"base": {"camera": "unsubscribed", "screenVideo": "unsubscribed"}}
        )

        self._client.set_user_name(self._bot_name)

        try:
            (data, error) = await self._join()

            if not error:
                self._joined = True
                # Increment leave counter if we successfully joined.
                self._leave_counter += 1

                logger.info(f"Joined {self._room_url}")

                if self._params.transcription_enabled:
                    await self._start_transcription()

                await self._callbacks.on_joined(data)
            else:
                error_msg = f"Error joining {self._room_url}: {error}"
                logger.error(error_msg)
                await self._callbacks.on_error(error_msg)
        except asyncio.TimeoutError:
            error_msg = f"Time out joining {self._room_url}"
            logger.error(error_msg)
            await self._callbacks.on_error(error_msg)

    async def _start_transcription(self):
        if not self._token:
            logger.warning("Transcription can't be started without a room token")
            return

        logger.info(f"Enabling transcription with settings {self._params.transcription_settings}")

        future = self._loop.create_future()
        self._client.start_transcription(
            settings=self._params.transcription_settings.model_dump(exclude_none=True),
            completion=completion_callback(future),
        )
        error = await future
        if error:
            logger.error(f"Unable to start transcription: {error}")
            return

    async def _join(self):
        future = self._loop.create_future()

        self._client.join(
            self._room_url,
            self._token,
            completion=completion_callback(future),
            client_settings={
                "inputs": {
                    "camera": {
                        "isEnabled": self._params.camera_out_enabled,
                        "settings": {
                            "deviceId": self._camera_name(),
                        },
                    },
                    "microphone": {
                        "isEnabled": self._params.audio_out_enabled,
                        "settings": {
                            "deviceId": self._mic_name(),
                            "customConstraints": {
                                "autoGainControl": {"exact": False},
                                "echoCancellation": {"exact": False},
                                "noiseSuppression": {"exact": False},
                            },
                        },
                    },
                },
                "publishing": {
                    "camera": {
                        "sendSettings": {
                            "maxQuality": "low",
                            "encodings": {
                                "low": {
                                    "maxBitrate": self._params.camera_out_bitrate,
                                    "maxFramerate": self._params.camera_out_framerate,
                                }
                            },
                        }
                    },
                    "microphone": {
                        "sendSettings": {
                            "channelConfig": "stereo"
                            if self._params.audio_out_channels == 2
                            else "mono",
                            "bitrate": self._params.audio_out_bitrate,
                        }
                    },
                },
            },
        )

        return await asyncio.wait_for(future, timeout=10)

    async def leave(self):
        # Decrement leave counter when leaving.
        self._leave_counter -= 1

        # Transport not joined, ignore.
        if not self._joined or self._leave_counter > 0:
            return

        self._joined = False

        logger.info(f"Leaving {self._room_url}")

        if self._params.transcription_enabled:
            await self._stop_transcription()

        try:
            error = await self._leave()
            if not error:
                logger.info(f"Left {self._room_url}")
                await self._callbacks.on_left()
            else:
                error_msg = f"Error leaving {self._room_url}: {error}"
                logger.error(error_msg)
                await self._callbacks.on_error(error_msg)
        except asyncio.TimeoutError:
            error_msg = f"Time out leaving {self._room_url}"
            logger.error(error_msg)
            await self._callbacks.on_error(error_msg)

    async def _stop_transcription(self):
        if not self._token:
            return
        future = self._loop.create_future()
        self._client.stop_transcription(completion=completion_callback(future))
        error = await future
        if error:
            logger.error(f"Unable to stop transcription: {error}")

    async def _leave(self):
        future = self._loop.create_future()
        self._client.leave(completion=completion_callback(future))
        return await asyncio.wait_for(future, timeout=10)

    async def cleanup(self):
        await self._loop.run_in_executor(self._executor, self._cleanup)
        self._callback_task.cancel()
        await self._callback_task

    def _cleanup(self):
        if self._client:
            self._client.release()
            self._client = None

    def participants(self):
        return self._client.participants()

    def participant_counts(self):
        return self._client.participant_counts()

    async def start_dialout(self, settings):
        future = self._loop.create_future()
        self._client.start_dialout(settings, completion=completion_callback(future))
        await future

    async def stop_dialout(self, participant_id):
        future = self._loop.create_future()
        self._client.stop_dialout(participant_id, completion=completion_callback(future))
        await future

    async def send_dtmf(self, settings):
        future = self._loop.create_future()
        self._client.send_dtmf(settings, completion=completion_callback(future))
        await future

    async def sip_call_transfer(self, settings):
        future = self._loop.create_future()
        self._client.sip_call_transfer(settings, completion=completion_callback(future))
        await future

    async def sip_refer(self, settings):
        future = self._loop.create_future()
        self._client.sip_refer(settings, completion=completion_callback(future))
        await future

    async def start_recording(self, streaming_settings, stream_id, force_new):
        future = self._loop.create_future()
        self._client.start_recording(
            streaming_settings, stream_id, force_new, completion=completion_callback(future)
        )
        await future

    async def stop_recording(self, stream_id):
        future = self._loop.create_future()
        self._client.stop_recording(stream_id, completion=completion_callback(future))
        await future

    async def send_prebuilt_chat_message(self, message: str, user_name: str | None = None):
        if not self._joined:
            return

        future = self._loop.create_future()
        self._client.send_prebuilt_chat_message(
            message, user_name=user_name, completion=completion_callback(future)
        )
        await future

    async def capture_participant_transcription(self, participant_id: str):
        if not self._params.transcription_enabled:
            return

        self._transcription_ids.append(participant_id)
        if self._joined and self._transcription_status:
            await self.update_transcription(self._transcription_ids)

    async def capture_participant_video(
        self,
        participant_id: str,
        callback: Callable,
        framerate: int = 30,
        video_source: str = "camera",
        color_format: str = "RGB",
    ):
        # Only enable the desired video source subscription on this participant.
        await self.update_subscriptions(
            participant_settings={participant_id: {"media": {video_source: "subscribed"}}}
        )

        self._video_renderers[participant_id] = callback

        self._client.set_video_renderer(
            participant_id,
            self._video_frame_received,
            video_source=video_source,
            color_format=color_format,
        )

    async def update_transcription(self, participants=None, instance_id=None):
        future = self._loop.create_future()
        self._client.update_transcription(
            participants, instance_id, completion=completion_callback(future)
        )
        await future

    async def update_subscriptions(self, participant_settings=None, profile_settings=None):
        future = self._loop.create_future()
        self._client.update_subscriptions(
            participant_settings=participant_settings,
            profile_settings=profile_settings,
            completion=completion_callback(future),
        )
        await future

    #
    #
    # Daily (EventHandler)
    #

    def on_app_message(self, message: Any, sender: str):
        self._call_async_callback(self._callbacks.on_app_message, message, sender)

    def on_call_state_updated(self, state: str):
        self._call_async_callback(self._callbacks.on_call_state_updated, state)

    def on_dialin_connected(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialin_connected, data)

    def on_dialin_ready(self, sip_endpoint: str):
        self._call_async_callback(self._callbacks.on_dialin_ready, sip_endpoint)

    def on_dialin_stopped(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialin_stopped, data)

    def on_dialin_error(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialin_error, data)

    def on_dialin_warning(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialin_warning, data)

    def on_dialout_answered(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialout_answered, data)

    def on_dialout_connected(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialout_connected, data)

    def on_dialout_stopped(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialout_stopped, data)

    def on_dialout_error(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialout_error, data)

    def on_dialout_warning(self, data: Any):
        self._call_async_callback(self._callbacks.on_dialout_warning, data)

    def on_participant_joined(self, participant):
        id = participant["id"]
        logger.info(f"Participant joined {id}")

        if not self._other_participant_has_joined:
            self._other_participant_has_joined = True
            self._call_async_callback(self._callbacks.on_first_participant_joined, participant)

        self._call_async_callback(self._callbacks.on_participant_joined, participant)

    def on_participant_left(self, participant, reason):
        id = participant["id"]
        logger.info(f"Participant left {id}")

        self._call_async_callback(self._callbacks.on_participant_left, participant, reason)

    def on_participant_updated(self, participant):
        self._call_async_callback(self._callbacks.on_participant_updated, participant)

    def on_transcription_started(self, status):
        logger.debug(f"Transcription started: {status}")
        self._transcription_status = status
        self._call_async_callback(self.update_transcription, self._transcription_ids)

    def on_transcription_stopped(self, stopped_by, stopped_by_error):
        logger.debug("Transcription stopped")

    def on_transcription_error(self, message):
        logger.error(f"Transcription error: {message}")

    def on_transcription_message(self, message):
        self._call_async_callback(self._callbacks.on_transcription_message, message)

    def on_recording_started(self, status):
        logger.debug(f"Recording started: {status}")
        self._call_async_callback(self._callbacks.on_recording_started, status)

    def on_recording_stopped(self, stream_id):
        logger.debug(f"Recording stopped: {stream_id}")
        self._call_async_callback(self._callbacks.on_recording_stopped, stream_id)

    def on_recording_error(self, stream_id, message):
        logger.error(f"Recording error for {stream_id}: {message}")
        self._call_async_callback(self._callbacks.on_recording_error, stream_id, message)

    #
    # Daily (CallClient callbacks)
    #

    def _video_frame_received(self, participant_id, video_frame):
        callback = self._video_renderers[participant_id]
        self._call_async_callback(
            callback,
            participant_id,
            video_frame.buffer,
            (video_frame.width, video_frame.height),
            video_frame.color_format,
        )

    def _call_async_callback(self, callback, *args):
        future = asyncio.run_coroutine_threadsafe(
            self._callback_queue.put((callback, *args)), self._loop
        )
        future.result()

    async def _callback_task_handler(self):
        while True:
            try:
                (callback, *args) = await self._callback_queue.get()
                await callback(*args)
            except asyncio.CancelledError:
                break


class DailyInputTransport(BaseInputTransport):
    def __init__(self, client: DailyTransportClient, params: DailyParams, **kwargs):
        super().__init__(params, **kwargs)

        self._client = client

        self._video_renderers = {}

        # Task that gets audio data from a device or the network and queues it
        # internally to be processed.
        self._audio_in_task = None

        self._vad_analyzer: VADAnalyzer | None = params.vad_analyzer
        if params.vad_enabled and not params.vad_analyzer:
            self._vad_analyzer = WebRTCVADAnalyzer(
                sample_rate=self._params.audio_in_sample_rate,
                num_channels=self._params.audio_in_channels,
            )

    async def start(self, frame: StartFrame):
        # Parent start.
        await super().start(frame)
        # Join the room.
        await self._client.join()
        # Create audio task. It reads audio frames from Daily and push them
        # internally for VAD processing.
        if self._params.audio_in_enabled or self._params.vad_enabled:
            self._audio_in_task = self.get_event_loop().create_task(self._audio_in_task_handler())

    async def stop(self, frame: EndFrame):
        # Parent stop.
        await super().stop(frame)
        # Leave the room.
        await self._client.leave()
        # Stop audio thread.
        if self._audio_in_task and (self._params.audio_in_enabled or self._params.vad_enabled):
            self._audio_in_task.cancel()
            await self._audio_in_task
            self._audio_in_task = None

    async def cancel(self, frame: CancelFrame):
        # Parent stop.
        await super().cancel(frame)
        # Leave the room.
        await self._client.leave()
        # Stop audio thread.
        if self._audio_in_task and (self._params.audio_in_enabled or self._params.vad_enabled):
            self._audio_in_task.cancel()
            await self._audio_in_task
            self._audio_in_task = None

    async def cleanup(self):
        await super().cleanup()
        await self._client.cleanup()

    def vad_analyzer(self) -> VADAnalyzer | None:
        return self._vad_analyzer

    #
    # FrameProcessor
    #

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, UserImageRequestFrame):
            await self.request_participant_image(frame.user_id)

    #
    # Frames
    #

    async def push_transcription_frame(self, frame: TranscriptionFrame | InterimTranscriptionFrame):
        await self.push_frame(frame)

    async def push_app_message(self, message: Any, sender: str):
        frame = DailyTransportMessageUrgentFrame(message=message, participant_id=sender)
        await self.push_frame(frame)

    #
    # Audio in
    #

    async def _audio_in_task_handler(self):
        while True:
            try:
                frame = await self._client.read_next_audio_frame()
                if frame:
                    await self.push_audio_frame(frame)
            except asyncio.CancelledError:
                break

    #
    # Camera in
    #

    async def capture_participant_video(
        self,
        participant_id: str,
        framerate: int = 30,
        video_source: str = "camera",
        color_format: str = "RGB",
    ):
        self._video_renderers[participant_id] = {
            "framerate": framerate,
            "timestamp": 0,
            "render_next_frame": False,
        }

        await self._client.capture_participant_video(
            participant_id, self._on_participant_video_frame, framerate, video_source, color_format
        )

    async def request_participant_image(self, participant_id: str):
        if participant_id in self._video_renderers:
            self._video_renderers[participant_id]["render_next_frame"] = True

    async def _on_participant_video_frame(self, participant_id: str, buffer, size, format):
        render_frame = False

        curr_time = time.time()
        prev_time = self._video_renderers[participant_id]["timestamp"]
        framerate = self._video_renderers[participant_id]["framerate"]

        if framerate > 0:
            next_time = prev_time + 1 / framerate
            render_frame = (next_time - curr_time) < 0.1

        elif self._video_renderers[participant_id]["render_next_frame"]:
            self._video_renderers[participant_id]["render_next_frame"] = False
            render_frame = True

        if render_frame:
            frame = UserImageRawFrame(
                user_id=participant_id, image=buffer, size=size, format=format
            )
            await self.push_frame(frame)
            self._video_renderers[participant_id]["timestamp"] = curr_time


class DailyOutputTransport(BaseOutputTransport):
    def __init__(self, client: DailyTransportClient, params: DailyParams, **kwargs):
        super().__init__(params, **kwargs)

        self._client = client

    async def start(self, frame: StartFrame):
        # Parent start.
        await super().start(frame)
        # Join the room.
        await self._client.join()

    async def stop(self, frame: EndFrame):
        # Parent stop.
        await super().stop(frame)
        # Leave the room.
        await self._client.leave()

    async def cancel(self, frame: CancelFrame):
        # Parent stop.
        await super().cancel(frame)
        # Leave the room.
        await self._client.leave()

    async def cleanup(self):
        await super().cleanup()
        await self._client.cleanup()

    async def send_message(self, frame: TransportMessageFrame | TransportMessageUrgentFrame):
        await self._client.send_message(frame)

    async def write_raw_audio_frames(self, frames: bytes):
        await self._client.write_raw_audio_frames(frames)

    async def write_frame_to_camera(self, frame: OutputImageRawFrame):
        await self._client.write_frame_to_camera(frame)


class DailyTransport(BaseTransport):
    def __init__(
        self,
        room_url: str,
        token: str | None,
        bot_name: str,
        params: DailyParams = DailyParams(),
        input_name: str | None = None,
        output_name: str | None = None,
        loop: asyncio.AbstractEventLoop | None = None,
    ):
        super().__init__(input_name=input_name, output_name=output_name, loop=loop)

        callbacks = DailyCallbacks(
            on_joined=self._on_joined,
            on_left=self._on_left,
            on_error=self._on_error,
            on_app_message=self._on_app_message,
            on_call_state_updated=self._on_call_state_updated,
            on_dialin_connected=self._on_dialin_connected,
            on_dialin_ready=self._on_dialin_ready,
            on_dialin_stopped=self._on_dialin_stopped,
            on_dialin_error=self._on_dialin_error,
            on_dialin_warning=self._on_dialin_warning,
            on_dialout_answered=self._on_dialout_answered,
            on_dialout_connected=self._on_dialout_connected,
            on_dialout_stopped=self._on_dialout_stopped,
            on_dialout_error=self._on_dialout_error,
            on_dialout_warning=self._on_dialout_warning,
            on_first_participant_joined=self._on_first_participant_joined,
            on_participant_joined=self._on_participant_joined,
            on_participant_left=self._on_participant_left,
            on_participant_updated=self._on_participant_updated,
            on_transcription_message=self._on_transcription_message,
            on_recording_started=self._on_recording_started,
            on_recording_stopped=self._on_recording_stopped,
            on_recording_error=self._on_recording_error,
        )
        self._params = params

        self._client = DailyTransportClient(
            room_url, token, bot_name, params, callbacks, self._loop
        )
        self._input: DailyInputTransport | None = None
        self._output: DailyOutputTransport | None = None

        # Register supported handlers. The user will only be able to register
        # these handlers.
        self._register_event_handler("on_joined")
        self._register_event_handler("on_left")
        self._register_event_handler("on_app_message")
        self._register_event_handler("on_call_state_updated")
        self._register_event_handler("on_dialin_connected")
        self._register_event_handler("on_dialin_ready")
        self._register_event_handler("on_dialin_stopped")
        self._register_event_handler("on_dialin_error")
        self._register_event_handler("on_dialin_warning")
        self._register_event_handler("on_dialout_answered")
        self._register_event_handler("on_dialout_connected")
        self._register_event_handler("on_dialout_stopped")
        self._register_event_handler("on_dialout_error")
        self._register_event_handler("on_dialout_warning")
        self._register_event_handler("on_first_participant_joined")
        self._register_event_handler("on_participant_joined")
        self._register_event_handler("on_participant_left")
        self._register_event_handler("on_participant_updated")
        self._register_event_handler("on_transcription_message")
        self._register_event_handler("on_recording_started")
        self._register_event_handler("on_recording_stopped")
        self._register_event_handler("on_recording_error")

    #
    # BaseTransport
    #

    def input(self) -> DailyInputTransport:
        if not self._input:
            self._input = DailyInputTransport(self._client, self._params, name=self._input_name)
        return self._input

    def output(self) -> DailyOutputTransport:
        if not self._output:
            self._output = DailyOutputTransport(self._client, self._params, name=self._output_name)
        return self._output

    #
    # DailyTransport
    #

    @property
    def participant_id(self) -> str:
        return self._client.participant_id

    async def send_image(self, frame: OutputImageRawFrame | SpriteFrame):
        if self._output:
            await self._output.queue_frame(frame, FrameDirection.DOWNSTREAM)

    async def send_audio(self, frame: OutputAudioRawFrame):
        if self._output:
            await self._output.queue_frame(frame, FrameDirection.DOWNSTREAM)

    def participants(self):
        return self._client.participants()

    def participant_counts(self):
        return self._client.participant_counts()

    async def start_dialout(self, settings=None):
        await self._client.start_dialout(settings)

    async def stop_dialout(self, participant_id):
        await self._client.stop_dialout(participant_id)

    async def send_dtmf(self, settings):
        await self._client.send_dtmf(settings)

    async def sip_call_transfer(self, settings):
        await self._client.sip_call_transfer(settings)

    async def sip_refer(self, settings):
        await self._client.sip_refer(settings)

    async def start_recording(self, streaming_settings=None, stream_id=None, force_new=None):
        await self._client.start_recording(streaming_settings, stream_id, force_new)

    async def stop_recording(self, stream_id=None):
        await self._client.stop_recording(stream_id)

    async def send_prebuilt_chat_message(self, message: str, user_name: str | None = None):
        """Sends a chat message to Daily's Prebuilt main room.

        Args:
        message: The chat message to send
        user_name: Optional user name that will appear as sender of the message
        """
        await self._client.send_prebuilt_chat_message(message, user_name)

    async def capture_participant_transcription(self, participant_id: str):
        await self._client.capture_participant_transcription(participant_id)

    async def capture_participant_video(
        self,
        participant_id: str,
        framerate: int = 30,
        video_source: str = "camera",
        color_format: str = "RGB",
    ):
        if self._input:
            await self._input.capture_participant_video(
                participant_id, framerate, video_source, color_format
            )

    async def update_subscriptions(self, participant_settings=None, profile_settings=None):
        await self._client.update_subscriptions(
            participant_settings=participant_settings, profile_settings=profile_settings
        )

    async def _on_joined(self, data):
        await self._call_event_handler("on_joined", data)

    async def _on_left(self):
        await self._call_event_handler("on_left")

    async def _on_error(self, error):
        # TODO(aleix): Report error to input/output transports. The one managing
        # the client should report the error.
        pass

    async def _on_app_message(self, message: Any, sender: str):
        if self._input:
            await self._input.push_app_message(message, sender)
        await self._call_event_handler("on_app_message", message, sender)

    async def _on_call_state_updated(self, state: str):
        await self._call_event_handler("on_call_state_updated", state)

    async def _handle_dialin_ready(self, sip_endpoint: str):
        if not self._params.dialin_settings:
            return

        async with aiohttp.ClientSession() as session:
            headers = {
                "Authorization": f"Bearer {self._params.api_key}",
                "Content-Type": "application/json",
            }
            data = {
                "callId": self._params.dialin_settings.call_id,
                "callDomain": self._params.dialin_settings.call_domain,
                "sipUri": sip_endpoint,
            }

            url = f"{self._params.api_url}/dialin/pinlessCallUpdate"

            try:
                async with session.post(
                    url, headers=headers, json=data, timeout=aiohttp.ClientTimeout(total=10)
                ) as r:
                    if r.status != 200:
                        text = await r.text()
                        logger.error(
                            f"Unable to handle dialin-ready event (status: {r.status}, error: {text})"
                        )
                        return

                    logger.debug("Event dialin-ready was handled successfully")
            except asyncio.TimeoutError:
                logger.error(f"Timeout handling dialin-ready event ({url})")
            except Exception as e:
                logger.exception(f"Error handling dialin-ready event ({url}): {e}")

    async def _on_dialin_connected(self, data):
        await self._call_event_handler("on_dialin_connected", data)

    async def _on_dialin_ready(self, sip_endpoint):
        if self._params.dialin_settings:
            await self._handle_dialin_ready(sip_endpoint)
        await self._call_event_handler("on_dialin_ready", sip_endpoint)

    async def _on_dialin_stopped(self, data):
        await self._call_event_handler("on_dialin_stopped", data)

    async def _on_dialin_error(self, data):
        await self._call_event_handler("on_dialin_error", data)

    async def _on_dialin_warning(self, data):
        await self._call_event_handler("on_dialin_warning", data)

    async def _on_dialout_answered(self, data):
        await self._call_event_handler("on_dialout_answered", data)

    async def _on_dialout_connected(self, data):
        await self._call_event_handler("on_dialout_connected", data)

    async def _on_dialout_stopped(self, data):
        await self._call_event_handler("on_dialout_stopped", data)

    async def _on_dialout_error(self, data):
        await self._call_event_handler("on_dialout_error", data)

    async def _on_dialout_warning(self, data):
        await self._call_event_handler("on_dialout_warning", data)

    async def _on_participant_joined(self, participant):
        await self._call_event_handler("on_participant_joined", participant)

    async def _on_participant_left(self, participant, reason):
        await self._call_event_handler("on_participant_left", participant, reason)

    async def _on_participant_updated(self, participant):
        await self._call_event_handler("on_participant_updated", participant)

    async def _on_first_participant_joined(self, participant):
        await self._call_event_handler("on_first_participant_joined", participant)

    async def _on_transcription_message(self, message):
        await self._call_event_handler("on_transcription_message", message)

        participant_id = ""
        if "participantId" in message:
            participant_id = message["participantId"]
        if not participant_id:
            return

        text = message["text"]
        timestamp = message["timestamp"]
        is_final = message["rawResponse"]["is_final"]
        try:
            language = message["rawResponse"]["channel"]["alternatives"][0]["languages"][0]
            language = Language(language)
        except KeyError:
            language = None
        if is_final:
            frame = TranscriptionFrame(text, participant_id, timestamp, language)
            logger.debug(f"Transcription (from: {participant_id}): [{text}]")
        else:
            frame = InterimTranscriptionFrame(text, participant_id, timestamp, language)

        if self._input:
            await self._input.push_transcription_frame(frame)

    async def _on_recording_started(self, status):
        await self._call_event_handler("on_recording_started", status)

    async def _on_recording_stopped(self, stream_id):
        await self._call_event_handler("on_recording_stopped", stream_id)

    async def _on_recording_error(self, stream_id, message):
        await self._call_event_handler("on_recording_error", stream_id, message)


Looking at the code, there isn't an explicit mechanism to know when the user has heard the playback from the TTS service. The Daily transport handles audio output through its =write_raw_audio_frames= method, but it doesn't provide feedback about playback completion.

Here's how audio output currently works in the pipeline:

1. The TTS service generates audio frames (=OutputAudioRawFrame=)
2. These frames are sent to the Daily transport's output component
3. The transport calls =write_raw_audio_frames= to send the audio to the WebRTC connection
4. The audio is played to the user through their browser/client

   Ok. When is the audio sent to the user? Is it sent once it is generated? or is it buffered? I'm asking because this is important for context building (only keep in context what the user actually heard and not what was interrupted) and this helps with clearing the audio queue of what the user didn't yet hear

Looking at the =BaseOutputTransport= class in pipecat, it buffers audio frames before sending them. Here's how it works:

1. When a TTS service generates audio, it creates =OutputAudioRawFrame=s that are queued in the transport's buffer
2. The transport has an internal buffer (audio queue) that collects these frames
3. The frames are then sent in chunks to maintain smooth playback

The key point about interruption is in the =BaseOutputTransport= class:
- When a =BotInterruptionFrame= is received, it clears this audio queue
- Any audio frames that haven't been sent to the WebRTC connection yet are discarded
- This means only the audio that was actually sent to the user (and presumably heard) should be considered for context

Here's the relevant flow:

1. User starts speaking while AI is talking
2. STT service detects speech and sends =UserStartedSpeakingFrame= and =BotInterruptionFrame=
3. =BotInterruptionFrame= causes:
   - Output transport to clear its audio queue (unsent audio)
   - TTS service to stop generating new audio
   - LLM service to stop generating new text
4. The context aggregator then only includes what was actually sent/heard by the user

This buffering and clearing mechanism is important because:
- It prevents audio fragments from playing after an interruption
- It helps maintain conversation coherence by only including what the user actually heard
- It allows for clean interruptions without audio artifacts

The actual sending of audio happens as the buffer is processed, not immediately when generated, which is why the interruption mechanism needs to clear the buffer.

*** How big are the chunks that are sent to the transport in milliseconds?

Looking at the Daily transport code, it uses WebRTC's standard audio frame size which is 10ms per frame. This can be seen in the audio frame calculation in the =read_next_audio_frame= method of =DailyTransportClient=:

#+begin_src python

def read_next_audio_frame(self) -> InputAudioRawFrame | None:
    if not self._speaker:
        return None

    sample_rate = self._params.audio_in_sample_rate
    num_channels = self._params.audio_in_channels
    num_frames = int(sample_rate / 100) * 2  # 20ms of audio

    future = self._loop.create_future()
    self._speaker.read_frames(num_frames, completion=completion_callback(future))
    audio = await future
#+end_src

Note that in this case it's reading 20ms chunks (2 frames) at a time, as indicated by the comment and the multiplication by 2 in the =num_frames= calculation.

For output (when sending audio to be played), WebRTC also typically uses 10ms frame sizes. This is a standard in WebRTC to maintain low latency and real-time performance. The frame size calculation is:

#+begin_src
frame_size = sample_rate / 100  # For 10ms chunks
#+end_src

So for example:
- At 16kHz sample rate: 160 samples per 10ms frame
- At 48kHz sample rate: 480 samples per 10ms frame

This small chunk size helps maintain low latency which is important for:
1. Quick interruptions when the user starts speaking
2. Natural conversation flow
3. Minimal audio buffering
4. Real-time adjustments to audio playback

** Conversation end


Based on this analysis, I can help you implement a similar audio chunking and interruption system in voice-fn. Here's how we could structure it:

1. First, we need to define our audio frame size constants:

#+begin_src clojure

(ns voice-fn.transport.constants)

(def webrtc-frame-ms 10)
(def default-frame-chunk-ms 20) ;; Two frames for efficiency

(defn calculate-frame-size
  "Calculate number of samples for a given duration in ms"
  [sample-rate ms]
  (int (* (/ sample-rate 1000) ms)))
#+end_src

2. For the transport output, we need a buffering system that:
   - Collects audio frames in 10ms chunks
   - Tracks what has been sent vs buffered
   - Can be cleared on interruption

Here's a proposed structure:

#+begin_src clojure

(ns voice-fn.transport.output-buffer
  (:require [voice-fn.frames :as frames]))

(defrecord AudioBuffer [frames sent-frames chunk-size])

(defn create-audio-buffer [chunk-size-ms sample-rate]
  (->AudioBuffer [] [] (calculate-frame-size sample-rate chunk-size-ms)))

(defprotocol AudioBufferManager
  (add-frame [this frame])
  (get-next-chunk [this])
  (clear-buffer [this])
  (buffer-size [this]))
#+end_src

3. Then modify the transport to use this buffer:

#+begin_src clojure

(defmethod pipeline/process-frame :transport/async-output
  [type pipeline {:keys [config]} frame]
  (case (:frame/type frame)
    :system/start
    (swap! pipeline assoc-in [type :audio-buffer]
           (create-audio-buffer webrtc-frame-ms
                              (get-in config [:audio-out/sample-rate])))

    :system/bot-interruption
    (do
      (clear-audio-buffer! pipeline type)
      (when-let [on-interrupt (:on-interrupt config)]
        (on-interrupt (get-sent-frames pipeline type))))

    :audio/output
    (when-let [buffer (get-in @pipeline [type :audio-buffer])]
      (add-frame buffer frame)
      (when-let [chunk (get-next-chunk buffer)]
        (send-audio! pipeline chunk)))

    nil))
#+end_src

4. For tracking what the user has heard:

#+begin_src clojure
(defn get-sent-frames
  "Get frames that were actually sent to the output"
  [pipeline type]
  (get-in @pipeline [type :audio-buffer :sent-frames]))

(defn mark-chunk-as-sent!
  [pipeline type chunk]
  (swap! pipeline update-in [type :audio-buffer :sent-frames] conj chunk))
#+end_src

This design:
1. Uses standard WebRTC 10ms frame sizes
2. Tracks what audio has actually been sent
3. Allows clean interruption by clearing unsent audio
4. Provides hooks for context management based on what was actually heard

Would you like me to elaborate on any of these aspects?

*** Let's get a more concrete example where our transport is twilio over websocket. Things missing:
1. User speaking state
   Based on this state, the context aggregator sends frames to the llm or not and the TTS engine generates frames or not to send to audio.
   This is going to be handled by UserStartedSpeaking and UserEndedSpeaking frames
2. Buffering audio output. Currently we just send chunks as they come from the TTS but that is not enough. We receive audio-output-frame from the :elevenlabs/audio-assembler but they are base64. We need a buffer that receives those frames, converts to binary and adds to a buffer and then we send them 10ms at a time to the user
3. THe audio output is also dependent on the user speaking state


Excellent points! Let's design these components:

1. First, let's add user speaking state to the pipeline:

#+begin_src clojure
;; src/voice_fn/frames.clj
(defframe user-started-speaking-frame :user/started-speaking
  "Sent when user starts speaking, used to pause AI output")

(defframe user-stopped-speaking-frame :user/stopped-speaking
  "Sent when user stops speaking, used to resume AI output")
#+end_src

2. Let's create an audio output buffer for Twilio:

#+begin_src clojure
;; src/voice_fn/transport/twilio/buffer.clj
(ns voice-fn.transport.twilio.buffer
  (:require [voice-fn.utils.core :as u]
            [clojure.core.async :as a]))

(def ^:private webrtc-frame-ms 10)

(defprotocol AudioBuffer
  (add-frame! [this frame])
  (clear! [this])
  (get-next-chunk! [this])
  (get-sent-frames [this]))

(defrecord TwilioAudioBuffer [buffer sent-frames chunk-size]
  AudioBuffer
  (add-frame! [this frame]
    (let [binary-data (u/decode-base64 (:data frame))]
      (swap! buffer conj binary-data)))

  (clear! [this]
    (reset! buffer []))

  (get-next-chunk! [this]
    (when-let [next-chunk (first @buffer)]
      (swap! buffer rest)
      (swap! sent-frames conj next-chunk)
      next-chunk))

  (get-sent-frames [this]
    @sent-frames))

(defn create-buffer [sample-rate]
  (->TwilioAudioBuffer
    (atom [])  ;; buffer
    (atom [])  ;; sent-frames
    (int (* (/ sample-rate 1000) webrtc-frame-ms)))) ;; chunk-size
#+end_src

3. Now let's modify the Twilio transport to handle user speaking state and buffered output:

#+begin_src clojure
;; src/voice_fn/transport/twilio.clj
(ns voice-fn.transport.twilio
  (:require
   [clojure.core.async :as a]
   [taoensso.telemere :as t]
   [voice-fn.frames :as f]
   [voice-fn.pipeline :as pipeline]
   [voice-fn.transport.twilio.buffer :as buffer]
   [voice-fn.transport.serializers :refer [make-twilio-serializer]]
   [voice-fn.utils.core :as u]))

(defn handle-media-event!
  [pipeline event]
  (let [audio-data (u/decode-base64 (get-in event [:media :payload]))
        ;; Check if volume exceeds threshold to detect speech
        speaking? (speech-detected? audio-data)]
    ;; Update speaking state and send appropriate frames
    (when (not= speaking? (get-in @pipeline [:transport/twilio :user-speaking?]))
      (swap! pipeline assoc-in [:transport/twilio :user-speaking?] speaking?)
      (a/put! (:pipeline/main-ch @pipeline)
              (if speaking?
                (f/user-started-speaking-frame true)
                (f/user-stopped-speaking-frame true))))
    ;; Send the audio frame
    (a/put! (:pipeline/main-ch @pipeline)
            (f/audio-input-frame audio-data))))

(defmethod pipeline/process-frame :transport/twilio-input
  [processor-type pipeline _ frame]
  (let [{:transport/keys [in-ch]} (:pipeline/config @pipeline)
        running? (atom false)]
    (case (:frame/type frame)
      :system/start
      (do
        (t/log! {:level :info :id processor-type} "Starting transport input")
        (reset! running? true)
        (a/go-loop []
          (when running?
            (when-let [input (a/<! in-ch

*** What do you think of this idea that system frames take priority in the pipecat system. Could we implement something similar in voice-fn?

Yes, that's an excellent observation! In pipecat, system frames like =BotInterruptionFrame= skip the regular input queues and are processed immediately. We can implement a similar priority system in voice-fn using core.async's features.

Here's how we could modify the pipeline to handle priority frames:

#+begin_src clojure
;; src/voice_fn/frames.clj
(ns voice-fn.frames)

(defn system-frame?
  "Returns true if the frame is a system frame that should be processed immediately"
  [frame]
  (let [frame-type (:frame/type frame)]
    (or (= frame-type :system/start)
        (= frame-type :system/stop)
        (= frame-type :system/bot-interruption)
        (= frame-type :user/started-speaking)
        (= frame-type :user/stopped-speaking))))

;; src/voice_fn/pipeline.clj
(ns voice-fn.pipeline
  (:require [clojure.core.async :as a :refer [>! <! chan go-loop]]))

(defn create-pipeline [processors-config]
  (let [main-ch (chan 1024)
        system-ch (chan 1024) ;; High priority channel for system frames
        main-pub (a/pub main-ch :frame/type)
        pipeline (atom {:pipeline/main-ch main-ch
                       :pipeline/system-ch system-ch
                       :pipeline/main-pub main-pub})]

    ;; Start each processor
    (doseq [{:processor/keys [type accepted-frames]} processors-config]
      (let [processor-ch (chan 1024)]
        ;; Subscribe to accepted frame types
        (doseq [frame-type accepted-frames]
          (a/sub main-pub frame-type processor-ch))

        ;; Start processor loop that prioritizes system frames
        (go-loop []
          (let [[frame ch] (a/alts! [system-ch processor-ch])]
            (when frame
              (when (= ch system-ch)
                ;; System frames are processed immediately
                (process-frame type pipeline frame))
              (when (and (= ch processor-ch)
                        (not (system-frame? frame)))
                ;; Regular frames go through normal processing
                (when-let [result (process-frame type pipeline frame)]
                  (when (frames/frame? result)
                    (>! main-ch result))))
              (recur))))

        (swap! pipeline assoc-in [type :processor/in-ch] processor-ch)))
    pipeline))

(defn send-frame!
  "Sends a frame to the appropriate channel based on its type"
  [pipeline frame]
  (if (system-frame? frame)
    (a/put! (:pipeline/system-ch @pipeline) frame)
    (a/put! (:pipeline/main-ch @pipeline) frame)))
#+end_src

This design:

1. Uses =alts!= to prioritize the system channel over the regular processor channel
2. System frames are processed immediately, bypassing regular queues
3. Regular frames follow the normal processing path
4. Each processor can still decide how to handle system frames

Example usage in a processor:

#+begin_src clojure
(defmethod process-frame :tts/elevenlabs
  [type pipeline processor frame]
  (case (:frame/type frame)
    ;; System frames get immediate handling
    :user/started-speaking
    (do
      (stop-tts! pipeline)
      (clear-audio-buffer! pipeline))

    :user/stopped-speaking
    (resume-tts! pipeline)

    ;; Regular frames follow normal flow
    :llm/output-text-sentence
    (when-not (user-speaking? pipeline)
      (generate-audio! pipeline frame))

    nil))
#+end_src

And in the Twilio transport:

#+begin_src clojure
(defmethod process-frame :transport/twilio-output
  [type pipeline processor frame]
  (case (:frame/type frame)
    ;; System frames get immediate handling
    :user/started-speaking
    (clear-audio-buffer! pipeline)

    :user/stopped-speaking

*** I think there's an error in your code because if a frame comes on the system channel, it will be processed by the first processor within the go-loop and not by all the subscribers that would normally processes it if it came from the main-ch

Ah, excellent catch! You're absolutely right. When a frame comes on the system channel, it's only processed by one processor rather than being distributed to all processors that should handle it.

Here's the corrected version:

#+begin_src clojure

(defn create-pipeline [processors-config]
  (let [main-ch (chan 1024)
        system-ch (chan 1024) ;; High priority channel for system frames
        main-pub (a/pub main-ch :frame/type)
        pipeline (atom {:pipeline/main-ch main-ch
                       :pipeline/system-ch system-ch
                       :pipeline/main-pub main-pub})]

    ;; Create a mult for the system channel so all processors get system frames
    (let [system-mult (a/mult system-ch)]

      ;; Start each processor
      (doseq [{:processor/keys [type accepted-frames]} processors-config]
        (let [processor-ch (chan 1024)
              processor-system-ch (chan 1024)]

          ;; Subscribe to regular frames
          (doseq [frame-type accepted-frames]
            (a/sub main-pub frame-type processor-ch))

          ;; Subscribe to system frames
          (a/tap system-mult processor-system-ch)

          ;; Start processor loop that prioritizes system frames
          (go-loop []
            (let [[frame ch] (a/alts! [processor-system-ch processor-ch])]
              (when frame
                (when-let [result (process-frame type pipeline processor frame)]
                  (when (frames/frame? result)
                    ;; System frames go back to system channel
                    (if (system-frame? result)
                      (>! system-ch result)
                      (>! main-ch result))))
                (recur))))

          (swap! pipeline assoc-in [type :processor/in-ch] processor-ch))))
    pipeline))
#+end_src

Key changes:
1. Created a mult for the system channel so all processors can receive system frames
2. Each processor gets its own tap into the system channel
3. Results from processing system frames go back to the system channel
4. All processors that accept a given system frame type will now process it

This ensures that system frames like =:user/started-speaking= will be properly handled by all relevant processors (TTS, LLM, audio buffer, etc.) while maintaining their priority nature.

Would you like me to show an example of how this would work in practice with multiple processors handling the same system frame?
